[["index.html", "Quantitative Environmental Methods: LRES 546 Chapter 1 Part 1: Foundations of Environmental Modeling 1.1 Introduction 1.2 Course overview and objectives 1.3 Course Description 1.4 Structure 1.5 Philosophical approach and resources 1.6 Tentative schedule, subject to change", " Quantitative Environmental Methods: LRES 546 Tim Covino &amp; Lauren Kremer Chapter 1 Part 1: Foundations of Environmental Modeling 1.1 Introduction This book provides the materials that we will use in Quantitative Environmental Methods (LRES 546). In this class we will be learning the fundamentals of environmental data analysis and simulation in R. Instructor: Dr. Tim Covino Time and location: Asynchronous, online Office hours: By appointment Email: timothy.covino@montana.edu TA: Lauren Kremer Email: lauren.kremer@montana.edu 1.2 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology/environmental science. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.3 Course Description This course will focus on development of quantitative analysis and modeling skills in watershed and environmental science. Students will develop skills necessary to perform quantitative analyses, describe and evaluate model structure, evaluate the merit of different models of varying type and complexity, and use quantitative analyses to address problems in environmental/watershed science. Students will apply computer programming in R to analyze and simulate watershed and/or environmental dynamics spanning simple to complex processes, analyses, and simulations. Technical skills and conceptual understanding will be built through lectures, readings, and hands-on quantitative projects. Note: If you aren’t familiar with R, or don’t have coding experience, don’t worry. We will walk you through all of the coding. I hope that by the end of this class each student will be a strong quantitative scientist with equally strong coding skills. 1.4 Structure This class will utilize hands-on/active learning, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class/work session. Programming is best learned by doing it often. Each week there will be recorded videos and/or readings, where we will talk about and work through various types of hydrological analyses. We will then put the content from the recorded lectures to work in a lab where students will complete a variety of hydrological analyses in R. Students can work through material on their schedule, but Dr. Covino and Lauren Kremer (TA) will be available to help with technical coding problems or to answer other questions on weekly labs - just send us an email and we will meet with you. 1.5 Philosophical approach and resources This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilize open source R and RStudio software. Books/resources, we may not use all of these, but they are good references: - R for Data Science - Statistical Methods in Water Resources - Tidy modeling with R - ggplot2: Elegant Graphics for Data Analysis - Advanced R - R Packages - Environmental Data Science Additional readings will be made available on this bookdown page as needed. 1.6 Tentative schedule, subject to change Part 1: Foundations of Environmental Modeling Week 1 1/13 - 1/16 - Introduction, overview, and technical skills. - If you need a refresher for R please see Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Week 2 1/19 - 1/23 -R tutorial and refresher Part 2: Tabular Models and Time-Series Analysis Week 3 1/26 - 1/30 - Hydrograph separation. Week 4 2/2 - 2/6 - Frequency analysis Week 5 2/9 - 2/13 - Rational and curve number (CN) methods Part 3: Bucket Models Week 6 2/16 - 2/20 - Transfer function &amp; rainfall-runoff models Week 7 2/23 - 2/27 - Monte Carlo (estimating model parameters) and sensitivity analysis Part 4: Model Classification and Application Week 8 3/2 - 3/6 - Classifying Model Structure (reading only) - Term assignment 1 Week 9 3/9 - 3/13 - Term assignment 2 Week 10 3/16 - 3/20 SPRING BREAK Part 5: Physical Process models Week 11 3/23 – 3/27 TBD - choose from - Snowmelt models - Evapotranspiration - Precipitation Part 6: Spatial Models and GIS Integration Week 12 3/30 - 4/3 - Gridded and tabular data retrieval using API integration packages Week 13 4/6 - 4/10 - DEM processing and analysis in Whitebox Week 14 4/13 - 4/17 - Term project work Week 15 4/20 - 4/24 - Term project work - Final presentations due April 25 Week 16 4/27 - 5/1 - Last week of classes – Peer review final presentations "],["r-tutorial.html", "Chapter 2 R Tutorial", " Chapter 2 R Tutorial This week we will be doing a hands-on R tutorial. This should be useful whether you are new to R or if you have a large amount of experience. Everyone works differently in R, so if you have experience you will likely learn other ways to do things. I find that is true, and I enjoy learning from others that go about coding differently than I do. This book is your friend RDS Here is a link to another class of mine that also has lots of code and tutorials that you can utilize Watershed Analysis Some topics we will cover in this tutorial: - installing R and RStudio on your computer - basics of GitHub - projects and why you should use them - layout of RStudio - RMarkdown - the tidyverse - packages in the tidyverse - loading, visualizing, and wrangling data - functions and for loops Read this short article from Jenny Bryan on why you should always us packages. Link This tutorial is accompanied by a video where I walk through all of the coding and describe what is happening. You should start the video and we will open the link below together and walk through the tutorial. You can pause the tutorial at any point you need to and work through it at your own pace. Last, please reach out to me if you have any questions (now and throughout the semester) and we will hop on a Zoom and get you on track! See here for part 1 tutorial repo See here for tutorial part 2 Chapter on functions Chapter on for loops "],["fundamentals-of-hydrological-analysis---physical-hydrology-and-empirical-methods.html", "Chapter 3 Fundamentals of Hydrological Analysis - Physical Hydrology and Empirical Methods 3.1 Hydrograph separation 3.2 Return Intervals 3.3 Rational method and NRCS curve number (20 pts)", " Chapter 3 Fundamentals of Hydrological Analysis - Physical Hydrology and Empirical Methods 3.1 Hydrograph separation This module was developed by Ana Bergrstrom and Matt Ross. 3.1.1 Learning Module 1 3.1.1.1 Summary Streamflow can come from a range of water sources. When it is not raining, streams are fed by the slow drainage of groundwater. When a rainstorm occurs, streamflow increases and water enters the stream more quickly. The rate at which water reaches the stream and the partitioning between groundwater and faster flow pathways is variable across watersheds. It is important to understand how water is partitioned between fast and slow pathways (baseflow and stormflow) and what controls this partitioning in order to better predict susceptibility to flooding and if and how groundwater can sustain streamflow in long periods of drought. In this module we will introduce the components of streamflow during a rain event, and how event water moves through a hillslope to reach a stream. We will discuss methods for partitioning a hydrograph between baseflow (groundwater) and storm flow (event water). Finally, we will explore how characteristics of a watershed might lead to more or less water being partitioned into baseflow vs. stormflow. We will test understanding through evaluating data collected from watersheds in West Virginia to determine how mountaintop mining, which fundamentally changed the watershed structure, affects baseflow. 3.1.1.2 Reading for this lab Ladson, A. R., R. Brown, B. Neal and R. Nathan (2013) A standard approach to baseflow separation using the Lyne and Hollick filter. Australian Journal of Water Resources 17(1): 173-18 Ladson et al., 2013 Lynne, V., Hollick, M. (1979) Stochastic time-variable rainfall-runoff modelling. In: pp. 89-93 Institute of Engineers Australia National Conference. Perth. Lyne and Hollick, 1979 3.1.1.3 Overall Learning Objectives At the end of this module, students should be able to describe the components of streamflow, the basics of how water moves through a hillslope, and the watershed characteristics that affect partitioning between baseflow and stormflow. 3.1.1.4 Lecture 3.1.1.5 Components of streamflow during a rain event During a rainstorm, precipitation is falling across the watershed: close to the stream, on a hillslope, and up at the watershed divide. This water that falls across the watershed flows downslope toward the stream via a number of flow pathways. Here we define and describe the basic flow pathways during a rain event. The first component is channel interception. This is water that falls directly on the water surface of the stream. The amount of water that falls directly on the channel is a function of stream size, if we have a very small, narrow creek, this will be a very small quantity. However, you can imagine that in a very large, broad river such as the Amazon, this volume of water is much larger. Channel interception is the first component during a rain event that causes streamflow to increase because it is contributing directly to the stream and therefore has no travel time. The second is overland flow, which is water that flows on the land surface to the stream. Overland flow can occur via a number of mechanisms which we will not explore too deeply here, but encourage further study on your own (resources provided). Briefly, overland flow includes water that falls on an impermeable surface such as pavement, water that runs downslope due to rain falling faster than the rate at which it can infiltrate the ground surface, and water that flows over the land surface because the ground is completely saturated. Overland flow is typically faster than water that travels through soils and deeper flow pathways and therefore is the next major component that starts to contribute to the increase in streamflow during a rain event. The third component is subsurface flow. This is water that infiltrates the land surface and flows downslope through shallow groundwater flow pathways. This is the last component that increases streamflow during a storm event, is the slowest of the stormflow components, and can contribute to elevated streamflow for a while after precipitation ends. The final component is baseflow. Baseflow can also be described as groundwater. This component is what sustains streamflow between rain events, but also continues to contribute during a rain event. Of water that infiltrates the ground surface, some moves quickly to the stream as subsurface flow, but some moves deeper and becomes part of deeper groundwater and baseflow. Thus baseflow can increase in times of higher wetness in the watershed, particularly during and right after rainy seasons or spring snowmelt. We can simplify this partitioning into baseflow and stormflow (often called quickflow). Baseflow being groundwater that moves more slowly and sustains streamflow between rain events. Stormflow is water that contributes to streamflow as a result of a rain event. Under this definition we can lump channel interception, overland flow, and subsurface flow into stormflow. 3.1.1.6 Storm flow through a hillslope When rain falls on the land surface, much of it infiltrates into the soil. Water moves through the soil column until it meets a layer of lower permeability and runs down the hillslope as subsurface flow. This layer of lower permeability allows some water to move through it, contributing to groundwater. Frequently the layer of lower permeability is the interface between soil and rock. Therefore the depth of soil has a large effect on how much water moves through soil vs. how much moves deeper into groundwater, becoming baseflow. 3.1.1.7 How we quantify baseflow It is impossible to know the amount of water moving as overland, subsurface, and base flow in all parts of a watershed. So in order to quantify how much water in a stream at any given time is storm flow vs. baseflow, we need to use some simplified methods (i.e., modeling). These frequently involve using the hydrograph (plot of streamflow over time) drawing lines, and calculating the volume of water above and below the line. This can be somewhat arbitrary and there are a variety of methods for delineating the cutoff between baseflow and stormflow. Despite what method you use and how simplified it is, this technique still provides valuable information and allows us to make comparisons across watersheds in order to understand how they function and what their structural properties are. 3.1.1.8 Baseflow separation methods One of the most basic methods for calculating base flow vs. storm flow is the straight line method. First, find the value of discharge at the point that streamflow begins to rise due to a storm. A straight line is drawn at that value until it intersects with the hydrograph (i.e. streamflow recedes back to the discharge it was at before the rainfall event started. Anything below this line is base flow and anything above it is storm flow. We learned above that some rainfall can move deep into the soil profile and contribute to baseflow. We might expect baseflow to increase over time and thus would want to use a method that can account for this. An addition to the straight line method was posed by Hewlett and Hibbert, 1967. This method, which we’ll call the Hewlett and Hibbert method finds the discharge at the starting point of a storm. Then, rather than a straight line of 0 slope and in the straight line method, we assume a slope of 0.05 cubic feet per second per square mile. The line with this calculated slope is drawn until it intersects with the hydrograph receding at the end of the storm. There are myriad other methods for baseflow separation of a wide range of complexity. We will give an example of one more method: a recursive filter method established by Lyne and Hollick (1976). This method smooths the hydrograph and partitions part of that smoothed signal into baseflow. The equation for this method is: You can see from this equation that a filter parameter, a, must be chosen. This parameter can be decided by the user, takes a value between 0 and 1, and is typically close to 1. Additionally this filtering method must be constrained so that baseflow is not negative or greater than the total streamflow. Output from this method for the Harvey River in Australia is originally published in Lyne and Hollick (1976) below (notice in the caption that the a parameter was set to 0.8): 3.1.1.9 Watershed controls on baseflow and stormflow The way a watershed is structured has a strong control on how water is partitioned into baseflow and stormflow. Below is a list of key structural properties: Land Use and Land Cover: If a watershed is developed or natural can dictate how much water infiltrates the land surface and how quickly. For example, a watershed with lots of pavement means that much more water will be overland flow with fewer opportunities to recharge baseflow. Furthermore, how a watershed is developed will affect partitioning. For example a residential area with houses on large, grassy lots will allow for more infiltration than a shopping center with large parking lots. Land cover in natural areas will also affect partitioning. Some other variables to consider may be: Land cover in natural areas: a dense forest vs. a recently harvested hillside. Soil type: clayey soils vs. sandy soils Depth to impeding layer: could be the bedrock interface, but could also be a low permeability clay layer in the soil Permeability of the underlying rock: Highly fractured sandstone vs. solid granite Slope: steeper slopes vs. flatter areas The partitioning is a combination of all of these factors. A watershed may have a very low slope, suggesting that it might have less stormflow. But if the soils in this watershed have an impermeable clay layer near the soil surface, a lot more water may end up as stormflow than one would expect. 3.1.2 Labwork (20 pts) In this lab we will analyze stream flow (Q) and precipitation (P) data from Tenderfoot Creek Experimental Forest (TCEF). TCEF is located in central Montana, north of White Sulphur Springs. See here for information about TCEF. You will do some data analysis on flows, calculate annual runoff ratios, and perform a hydrograph separation. 3.1.3 Repo link Watch this video on the workflow for starting these projects. Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd” or “_partial.Rmd”. Always be sure to read the README.md files in the GitHub repo. Sometimes they are useful, sometimes they aren’t, but always have a look. As I mentioned above you will work through the “_blank.Rmd” or “_partial.Rmd”. If you want to learn how to code in R, I encourage you to work through the blank version as much as possible. Also, if you don’t have much R background this lab might seem kind of challenging. But don’t worry. I’m challenging you right now, but it will get easier, and we can video chat if a live demonstration is needed. So don’t get frustrated if this seems tough right now. Soon you will be rattling off code with ease. Conversely, if you are an experienced coder and have ideas for how to do this in ways other than what I’ve shown here, please share code with your colleagues and help them develop their coding skills! Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. See here for an overview of projects and why you should use them from Jenny Bryan. 3.2 Return Intervals In this learning module we will quantify the magnitude and frequency of the precipitation ‘forcing’ to move from observation to prediction. This week introduces statistical frequency analysis and return intervals to evaluate the rarity of the events driving our process-based models. 3.2.1 Learning Module 2 3.2.1.1 Background information In the previous module, we performed hydrograph separation across three watersheds and we looked at the final output of complex system (the streamflow). By comparing patterns across the three watersheds, we looked at the response (the outcome) and worked backward to identify the processes influencing baseflow and quickflow. Your process-based explanations likely touched on: Storage: Soil depth and groundwater capacity. Surface Characteristics: Land cover and urbanization. Losses: Evapotranspiration and infiltration rates. However, a watershed’s behavior is only half of the story. A high runoff ratio is interesting because it reflects watershed behavior, but it doesn’t tell us how frequently the system produces extreme flows or floods. Now that you can separate baseflow from quickflow, you have the tools to link storm inputs to peak discharge. The next step is to evaluate precipitation using the same quantitative mindset. This shift marks our first step from qualitative process reasoning toward predictive modeling. In other words, we’ve asked: “What did the watershed do with the water it received?” Now we’ll ask: “How rare or extreme was the water it received in the first place?” To answer this question, we’ll explore return intervals (or recurrence intervals). Return intervals are a probabilistic model that summarize historical precipitation records to estimate the likelihood of extreme events. They do not predict when a storm will occur; instead, they describe the probability that a given magnitude will be exceeded in any year. In professional practice, we rarely design for “average” days, we design for the extremes. In last week’s analysis, we assumed that the forcing was the same for all 3 watersheds. Now let’s imagine we are evaluating a single watershed and see a massive spike in a hydrograph. To interpret the event, we need to know: Was this a relatively common storm impacting a low-storage watershed? Or was it a once-in-one hundred-years storm hitting a watershed that usually buffers flow? By calculating return intervals using Precipitation Duration, Frequency, and Intensity (IDF) curve, we begin to quantify the hydrologic risk. 3.2.1.1.1 Connecting to our R Workflow Before we can build a process model that predicts future runoff, we must understand the statistical characteristics of rainfall like magnitude, timing, and variability. Just as we used R to automate the separation of baseflow, we will now use it to fit statistical distributions to historical rainfall and calculating exceedance probabilities. Check out this lecture from colleague Joel Sholtes on precipitation frequency analysis. You can ignore his references to ‘assignments’ and end the lecture around the 26 minute mark. Short lecture on Intensity-Duration-Frequency (IDF) curves Reading on frequency analysis of flow (e.g., floods). You should notice that the frequency analysis is the same whether we apply it to Q (flow) or P (precipitation). So as long as you understand the fundamental principles you will be able to do frequency analysis on either Q or P. USGS reading on flow frequency analysis If you would like a refresher, there are also probability lecture slides on Canvas titled “probability.pptx”. 3.2.2 Repo link The repo for this module can be found here 3.3 Rational method and NRCS curve number (20 pts) 3.3.1 Learning Module 3 3.3.1.1 Background information - Rational Method In the previous module, we worked with our first probability-based model: return intervals. A model is simply an equation, or set of equations, that represents environmental processes and their interactions to estimate an outcome. Models act as maps of environmental systems: they are useful for understanding and prediction, but they are not exact representations of reality. As the statistician George Box famously noted, “All models are wrong, but some are useful.” Return intervals are a model because they summarize past observations into a probabilistic description of extreme events. They do not predict when a storm will occur, nor do they describe the physical processes that generate rainfall. Instead, they provide a simplified, probability-based way of representing risk that can be used in hydrologic analysis and design. In this module, we build on that foundation by introducing parameters. Parameters are numerical values within a model that summarize key characteristics or processes of a system and strongly influence model outputs. In hydrology, parameters often represent watershed properties such as land cover, soil type, or infiltration capacity. The Rational Method and the Curve Number method are examples of very simple rainfall–runoff models that rely on one or a small number of parameters to represent complex watershed behavior. While these methods are somewhat antiquated given today’s computational efficiency, they provide a clear and accessible way to see how model structure and parameter choices affect runoff estimates. Through these models, we will focus less on precise prediction and more on understanding how assumptions and parameterization shape our representation of hydrologic systems. The Rational Method is a type of simple hydrological analysis used to estimate the peak runoff rate from a small watershed during a rainfall event. It focuses on the maximum flow expected at the watershed outlet, making it especially useful for designing infrastructure such as storm drains, culverts, and small channels. Despite the existence of more advanced methods, this approach remains widely used in practice for its simplicity and ability to provide quick approximations, making it invaluable for preliminary assessments and as a foundation for understanding more complex hydrological and environmental modeling techniques. Here is a 5-minute video to get started: 3.3.1.2 Reading - Rational Method. Read at least sections 2 and 3 to for the formula and an example The Rational Method Some helpful terminology: runoff coefficient - represents how much rainfall actually becomes runoff time of concentration - the time it takes for some mass of precipitation to travel from the most remote point in a watershed to the outlet or point of interest. e.g., how long it takes a drop of rain to reach a culvert after it falls to the ground. 3.3.1.3 Background information - Curve Number The NRCS (Natural Resources Conservation Science) curve number (CN) is a parameter used to estimate the total runoff volume of water that will run off an ungaged watershed during a storm event. You may also see ‘SCS CN’ in texts. NRCS was previously known as Soil Conservation Science, they are the same. The curve number is based on soil type, land use and antecedent moisture conditions. So it is a parameter that summarizes multiple characteristics into one value. It was designed as a simple tool to describe typical watershed response from infrequent rainfall anywhere in the US for watersheds with the same soil type, land use, and surface runoff conditions. The CN method is a single event model to estimate of runoff volume from rainfall events (not peak discharge or a hydrograph). To understand the function and derivation of the CN number, let’s start with the the NRCS runoff equation: \\[ Q = \\frac{{(P - I_{a})^2}}{{P - I_{a} + S}} \\] Where Q = runoff(in) P = rainfall (in) S = potential maximum retention after runoff begins Ia = initial abstraction (initial amount of rainfall that is intercepted by the watershed surface and does not immediately contribute to runoff) Ia is assumed to reduce to 0.2S based on empirical observations by NRCS. If: \\[ S = \\frac{{1000}}{{CN}} - 10 \\] the runoff equation therefore reduces to: \\[ Q = \\frac{{[P - 0.02\\left(\\frac{{1000}}{{CN}} - 10\\right)]^2}}{{P + 0.8\\left(\\frac{{1000}}{{CN}} - 10\\right)}} \\] 3.3.1.4 Reading - Curve numbers Curve Number selection tables are available from the US Army Corps. Slides on selecting curve number start around slide 8. - Link 3.3.1.5 Reading - Supporting material Time of concentration. Up to “other considerations”, pages 15-1 to 15-9. - Link We will use the Kirpich method to calculate the time of concentration. Here is the citation for your reference. Kirpich, Z.P. (1940). “Time of concentration of small agricultural watersheds”. Civil Engineering. 10 (6): 362. 3.3.2 Repo link The repo for this module can be found here You will also need this link to select a parameter value ‘C’ or the runoff coefficient for the script (~line 117): Look at this table to select C. "],["probabilistic-and-process-simulation.html", "Chapter 4 Probabilistic and Process Simulation 4.1 Transfer function rainfall-runoff models 4.2 Monte Carlo Simulation", " Chapter 4 Probabilistic and Process Simulation * Modules 4.1 and 4.2 are adapted from Fabian Nipggen (REWM.4500.500) 4.1 Transfer function rainfall-runoff models 4.1.1 Learning Module 4 4.1.1.1 Summary In previous modules, we focused on where water goes in a watershed and whether it runs off quickly as stormflow or moves slowly through storage and contributes to baseflow. In this module, we shift focus to when water arrives at the stream. Why does timing matter? Water that moves quickly tends to look chemically different than water that moves slowly Longer travel times allow for more uptake by vegetation or interaction with soils, minerals, and microbes The timing of runoff shapes the hydrograph and affects solute transport In this module, we will model the temporal aspects of runoff response to input using a transfer function. A transfer function is a tool that describes how rainfall input is translated into runoff output over time. 4.1.1.1.1 Overall Learning Objectives By the end of this module, you should be able to: Explain what a transfer function is in plain language Describe how water transit time influences runoff timing and composition Recognize different ways transit time can be represented in hydrological models Interpret (not derive) the basic equations used in transfer-function rainfall–runoff models 4.1.1.1.2 Key Terms In modeling a flow system, note that consideration of time may vary depending on the questions being asked. Transit time is the average time required for water to travel through the entire flow system, from input (e.g., rainfall on soil surface) to output (e.g., discharge). Residence time is a portion of transit time, describing the amount of time water spends within a specific component of the flow system, like storage (e.g., in soil, groundwater, a puddle or a lake). First, familiarize yourself with the transfer function we will code (TRANSEP) by reading this paper. If the terminology feels dense, you may use AI tools to help extract the key concepts and focus on the big-picture ideas: TRANSEP - a combined tracer and runoff transfer function hydrograph separation model Then this text will step through key concepts in the paper to facilitate hands-on exploration of the rainfall-runoff portion of the TRANSEP model in the assessment. Figure 6.3. Conceptual diagram of the lumped parameter transit time modeling approach (McGuire &amp; McDonnell, 2006) A transfer function (TF) is a mathematical representation of how a system responds to input signals. In a hydrological context, it describes the transformation of inputs (e.g. precipitation) to outputs (e.g. runoff). In other words, it is a mathematical way of answering the question: If water enters the watershed at time t, how much of it shows up at the stream later and when? 4.1.1.1.3 The Linear Time-Invariant TF We start with a linear, time-invariant (LTI) system. The name sounds extravagant, but the assumptions are simple: Linear: Double the rainfall → double the runoff response Time-invariant: The watershed responds the same way today as it did yesterday Additive: Responses from multiple rainfall pulses add together Under these assumptions, we can represent the watershed as a linear reservoir. Linear reservoirs are simple models designed to simulate the storage and discharge of water in a catchment. These models assume that the catchment can be represented as single storage compartments and that Water is stored temporarily the rate of outflow is porportional to how much water is stored there is no threshold behavior or hysteresis. external link Recall this isn’t how all watersheds work, these are useful approximations. 4.1.1.1.4 The Instantaneous Unit Hydrograph: The Instantaneous Unit Hydrograph (IUH) is the transfer function used in the TRANSEP model. It is an approach to hydrograph separation that is useful for analyzing the temporal distribution of runoff in response to a ‘unit’ pulse of rainfall (e.g. uniform one-mm depth over a unit area represented by a unit hydrograph). In other words, it answers the question: If one unit of effective rainfall fell instantly over the watershed, how would that water be released to the stream over time? Therefore, the following assumptions are made when the IUH is used as a transfer function: the watershed response reflects the ensemble of watershed characteristics the response shape does not change through time the output response is linearly proportional to the input (i.e. runoff of scales linearly with rainfall) The IUH is not a specific storm, rather it is a response kernel that we reuse for all storms. 4.1.1.2 From Concept to Math Don’t worry about deriving or memorizing the following equations; they are included only to help you understand the relationships among variables and reinforce the underlying concepts. 4.1.1.2.1 IUH In TRANSEP, the IUH is written as a function \\(g(\\tau)\\). This function describes how runoff is distributed through time after rainfall occurs. \\[ g(\\tau) = \\frac{\\tau^{\\alpha-1}}{\\mathrm{B}^{\\alpha}\\Gamma(\\alpha)}exp(-\\frac{\\tau}{\\alpha}) \\] What matters here is that: The shape of \\(g(\\tau)\\) controls how fast or slow water moves Parameters change whether runoff is flashy or delayed The function integrates to 1 (mass is conserved) 4.1.1.2.2 Convolution: Turning Rainfall into Runoff Runoff is calculated by combining rainfall with the transfer function: \\[ Q(t)= \\int_{0}^{t} g(\\tau)p_{\\text{eff}}(t-\\tau)d\\tau \\] Runoff at time t is the sum of past rainfall inputs, each delayed and weighted by the transfer function. OK, I realize your eyes probably glazed over somewhere around ‘convolution’… don’t worry about the details of the equations. Check out this video to have convolution described using dollars and cookies, then imagine each dollar as a rainfall unit and each cookie as a runoff unit. Review the equations again after the video. knitr::include_url(&quot;https://www.youtube.com/embed/aEGboJxmq-w&quot;) 4.1.1.2.3 The Loss Function: Not all rainfall contributes to runoff. Some is lost to infiltration, evaporation, or temporary storage. The loss function represents the linear rainfall-runoff model used in the TRANSEP model. In other words, it converts total preciptiation \\(p(t)\\) into effective precipitation \\(p_{\\text{eff}}(t)\\). \\[ s(t) = b_{1} p(t + 1 - b_{2}^{-1}) s(t - \\triangle t) \\] \\[ s(t = 0) = b_{3} \\] \\[ p_{\\text{eff}}(t) = p(t) s(t) \\] where \\(p_{\\text{eff}}(t)\\) is the effective precipitation. \\(s(t)\\) is the antecedent precipitation index which is determined by giving more importance to recent precipitation and gradually reducing that importance as we go back in time (i.e. recent rainfall matters more than old rainfall). The rate at which this importance decreases is controlled by the parameter \\(b_{2}\\). The parameter \\(b_{3}\\) sets the initial antecedent precipitation index at the beginning of the simulated time series. Again: the goal is behavior, not physical realism. Transfer-function models are powerful because they: Explicitly represent timing, not just volume Allow separation of rainfall input and watershed response Provide a bridge between hydrology and tracer studies They are also limited: They assume linearity They hide spatial heterogeneity They trade physical detail for interpretability Understanding these tradeoffs is more important than memorizing equations. 4.1.1.2.4 How do we code this? We will use a skeletal version of TRANSEP, focusing only on the rainfall-runoff piece which includes the loss-function and the gamma transfer function. We will use rainfall and runoff data from the Tenderfoot Creek Experimental Forest to model annual streamflow at a daily time step. Then we can use this model as a jump-off point to start talking about model calibration and validation in future modules. 4.1.1.2.5 Final thoughts: If during your modeling experience, you find yourself wading through a bog of complex physics and multiple layers of transfer functions to account for every drop of input into a system, it is time to revisit your objectives. Remember that a model is always ‘wrong’, but it can help us understand complex systems, make predictions, and gain insights even if they are not an exact replica of the real world. Check out this paper for more: https://agupubs.onlinelibrary.wiley.com/doi/10.1029/93WR00877 4.1.2 Repo link Download the repo for this lab HERE 4.2 Monte Carlo Simulation 4.2.1 Learning Module 5 4.2.1.1 Background: In the previous unit, we used transfer functions to simulate watershed behavior. We saw that model parameters (e.g., recession constants, storage coefficients, routing terms) strongly control the shape and magnitude of simulated discharge. But here is the critical question: How certain are we about the parameter values we used? In most hydrological systems, parameters are not directly measurable. Instead, they are estimated, calibrated, or inferred. Different parameter combinations can often produce similar hydrographs. This means that model output is not determined by a single “correct” parameter set but by a range of plausible parameter values. So rather than asking: What is the model output? We begin asking: What range of outputs is possible given parameter uncertainty? Which parameters most strongly influence the results? How sensitive is the model to our assumptions? So, this shift from single deterministic simulations to probabilistic exploration leads us to Monte Carlo simulation. Monte Carlo Simulation is a method to estimate the probability of the outcomes of an uncertain event. It is based on a law of probability theory that says if we repeat an experiment many times, the average of the results will get closer to the true probability of those outcomes. First check out this video: 4.2.1.2 Reading Then read this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2924739/ for a understanding of the fundamentals. 4.2.1.3 How does this apply to hydrological modeling? When modeling watershed hydrological processes, we often attempting to quantify watershed inputs e.g., precipitation watershed outputs e.g., evapotranspiration, sublimation, runoff/discharge watershed storage precipitation that is not immediately converted to runoff or ET rather stored as snow or subsurface water. Imagine we are trying to predict the percentage of precipitation stored in a watershed after a storm event. We have learned that there are may factors that affect this prediction, like antecedent conditions, that may be difficult to measure directly. Monte Carlo Simulation can offer a valuable approach to estimate the probability of obtaining certain measurements when those factors can not be directly observed or measured. We can approximate the likelihood of specific measurement by simulating a range of possible scenarios. Monte Carlo Simulation is not only useful for estimating probabilities, but for conducting sensitivity analysis. In any model, there are usually several input parameters. Sensitivity analysis helps us understand how changes in these parameters affect the predicted values. To perform a sensitivity analysis using a Monte Carlo Simulation we can: Define the realistic ranges for each parameter we want to analyze Using Monte Carlo Simulation, randomly sample values from the defined ranges for each parameter Analyze output to understand how different input sample values affect the predicted output 4.2.1.4 Example Let’s consider all of this in an example model called WECOH - Watershed ECOHydrology. In this study, researchers (Nippgen et al.) were interested in the change in subsurface water storage through time and space on a daily and seasonal basis. Evolution of watershed connectivity The authors directly measured runoff/discharge, collected precipitation data at several points within the watershed, used remote sensing to estimate how much water was lost to evapotranspiration, and used digital elevation models to characterize the watershed topography. As we learned in the hydrograph separation module, topographic characteristics can have a significant impact on storage. Though resources like USDA’s Web Soil Survey can provide a basic understanding underlying geology across a large region, characterizing the heterogeneous nature of soils within a watershed can be logistically unfeasible. To estimate the soil characteristics like storage capacity (how much water the soil can hold) and hydraulic conductivity (how easily water can move through soil) in the study watershed, the authors used available resources to determine the possible range of values for each of their unknown parameters. They then tested thousands of model simulations using randomly selected values with the predetermined range for each of the soil characteristics. They compared the simulated discharge from these simulations to the actual discharge measurements. The simulations that predicted discharge patterns that closely matched reality helped them to estimate the unknown soil properties. Additionally, from the results of these simulations, they could identify which model inputs had the most significant impact on the discharge predictions, and how sensitive the output was to changes in each parameter. In this case, they determined that the model and system were most sensitive to precipitation. This type of sensitivity analysis can help us interpret the relative importance of different parameters and understand the overall sensitivity of the model or system. The study is linked for your reference but a thorough reading is not required. 4.2.1.5 Optional Reading This methods paper by Knighton et al. is an example of how Monte Carlo Simulation was used to estimate hydraulic conductivity in an urban system with varied land-cover.. 4.2.1.6 Generate a simulation with code For a 12-minute example in RStudio, check this out. If you are still learning the basics of R functionality, it may be helpful to code along with this video, pausing as needed. Note that this instruction is coding in an Rscript (after opening RStudio &gt; File &gt; New File &gt; R Script), rather than an Rmarkdown that we use in this class. 4.2.2 Repo link Download the repo for this lab HERE "],["data-retrieval-and-application.html", "Chapter 5 Data Retrieval and Application 5.1 Classifying model structure 5.2 Gridded and tabular data retrieval 5.3 Term Project Assignment 1 - 10 pnts", " Chapter 5 Data Retrieval and Application 5.1 Classifying model structure 5.1.1 Learning Module 6 This chapter marks the beginning of your term project. The first project assignment will be a brainstorming and data retrieval exercise designed to guide you through identifying a research question and locating the data needed to answer it using a model written in R. The goal is to move you from structured labs toward independent inquiry. Based on your collective interests, we will gather datasets and build models to explore how measurable environmental factors (such as snowmelt, land cover, and topography), influence runoff. The term project is your opportunity to design and explore a question that is specific, testable, and feasible within the scope of this semester. To prepare for that transition, we will begin with a review of modeling terminology, particularly concepts related to model type and complexity. Environmental models, including hydrological models, are built around simplifying assumptions of natural systems. The complexity of the model may depend on its application. Effective hydrological models share key traits: they are simple, parsimonious, and robust across various watersheds. In other words, they are easy to understand and streamlined and consistently perform well across different basins or even geographical areas. Therefore, more complex is only sometimes better. 5.1.1.1 Spatial Complexity There are general terms that classify the spatial complexity of hydrological models: A lumped system is one in which the dependent variables of interest are a function of time alone, and the study basin is spatially ‘lumped’ or assumed to be spatially homogeneous across the basin. So far in this course, we have focused mainly on lumped models. You may remember the figure below from the transfer functions module. It represents the lumped watershed as a bucket with a single input, outlet output, and storage volume for each timestep. A distributed system is one in which all dependent variables are functions of time and one or more spatial variables. Modeling a distributed system means partitioning our basins into raster cells (grids) and assigning inputs, outputs, and the spatial variables that affect inputs and outputs across these cells. We then calculate the processes at the cell level and route them downstream. These models allow us to represent the spatial complexity of physically based processes. They can simulate or forecast parameters other than streamflow, such as soil moisture, evapotranspiration, and groundwater recharge. A semi-distributed system is an intermediate approach that combines elements of both lumped and distributed systems. Certain variables may be spatially distributed, while others are treated as lumped. Alternatively, we can divide the watershed into sub-basins and treat each sub-basin as a lumped basin. Outputs from each sub-basin are then linked together and routed downstream. Semi-distribution allows for a more nuanced representation of the basin’s characteristics, acknowledging spatial variability where needed while maintaining some simplifications for computation efficiency. In small-scale studies, we can design a model structure that fits the specific situation well. However, when we are dealing with larger areas, model design may be challenging. Our data might differ across regions with variable climate and landscape features. Sometimes, it is best to use a complex model to capture all the different processes happening over a big area. However, it could be better to stick with a simpler model because we might have limited data or the number of calculations is very computationally expensive. It is up to the modeler to determine the simplest model that meets the desired application. For this determination, it is important to understand the advantages of different modeling approaches. 5.1.1.2 Modeling Approaches Empirical Models are based on empirical analysis of observed inputs (e.g., rainfall) or outputs (ET, discharge). These are most useful if you have extensive historical data so models can capture trends effectively. For example, if your goal is to predict the amount of dissolved organic carbon (DOC) transported out of a certain watershed, an empirical model will likely suffice. However, simple models may not be transferable to other watersheds. Also, they may not reveal much about the physical processes influencing runoff. Therefore, these types of models may not be valid after the study area experiences land use or climate change. Conceptual Models describe processes with simple mathematical equations. For example, we might use a simple linear equation to interpolate precipitation inputs over a watershed with a high elevation gradient using precipitation measurements from two points (high and low). This represents the basic relationship between precipitation and elevation, but does not capture all features that affect precipitation patterns (e.g. aspect, prevailing winds). The combined impact of these factors is probably negligible compared to the substantial amount of data required to accurately model them, so a conceptual model is sufficient. These can models can be especially useful when we have limited data, but theoretical knowledge to help ‘fill in the blanks’. Physically Based Models These models offer deep insights into the processes governing runoff generation by relying on fundamental physical equations like mass conservation. However, they come with drawbacks. Their implementation often demands complex numerical solving methods and a significant volume of input data. For example, if we want to understand how DOC transport changes in a watershed after a wildfire, we would want to understand many physical system properties pre- and post-fire like soil infiltration rates, quantification of forest canopy, stream flow data, carbon export, etc.. Without empirical data to validate these techniques, there is a risk of introducing substantial uncertainty into our models, reducing their reliability and effectiveness. An example of a spatial distributed and physically based watershed model from Huning and Marguilis, 2015: When modeling watersheds, we often use a mix of empirical, conceptual, and physically based models. The choice of model type depends on factors like the data we have, the time or computing resources we can allocate, and how we plan to use the model. These categorizations provide a philosophical foundation of how we understand and simulate systems. However we can also consider classifications that focus on the quantitative tools and techniques we use to implement these approaches. Consider that we have already applied each of these tools: Probability Models Many environmental processes can be thought of or modeled as stochastic, meaning a variable may take on any value within a specified range or set of values with a certain probability. Probability can be thought of in terms of the relative frequency of an event. We utilized probability models in the return intervals module where we observed precipitation data, and used that data to develop probability distributions to estimate likely outcomes for runoff. Probability models allow us to quantify risk and variability in systems. Regression Models Often we are interested in modeling processes with limited data, or processes that aren’t well understood. Regression assumes that there is a relationship between dependent and independent variables (you may also see modelers call these explanatory and response variables). We utilized regression methods in the hydrograph separation module to consider process-based mechanisms that differed among watersheds. Simulation Models Simulation models can simulate time series of hydrologic variables (as in the following snow melt module), or they can simulate characteristics of the modeled system, as we saw in the Monte Carlo module. These types of models are based on an assumption of what the significant variables are, an understanding of the important processes are, and/or a derivation of these physical processes from first principles (mass, energy balance). 5.1.1.3 A priori model selection: By understanding the different frameworks of environmental modeling, we can choose the right tools for the right context, depending on our data, goals and resources. In reality, the final model selection is a fluid process requiring multiple iterations at each step. In Keith Beven’s Rainfall-Runoff Modelling Primer, they illustrate the process as: While we aim to give some hands on experience across multiple model types, there is a wide range of possible models! Why would the most complex model, or one that represents the most elements in a system be best? Why even consider a simple bucket model? Many modelers have observed that the number of parameters required to describe a key behavior in a watershed is often quite low, meaning increasing the number of parameters does not result in a significantly improved model. This idea that simple models are often sufficient for representing a system have led to the investigation of parsimonious model structures (less complex). Consider though, that the model must sufficiently represent processes or it will be too unreliable outside of the range of conditions on which it was calibrated. Now that we have reviewed some concepts, our next step will be to develop a term project question. As you brainstorm and gather data, be sure to consider and use the modeling concepts and terminology we’ve covered to frame and structure your project design. 5.2 Gridded and tabular data retrieval 15 pnts 5.2.1 Learning Module 7 5.2.1.1 Objectives This module prepares you for your term project by introducing the spatial datasets that support watershed-scale modeling. Before you design your own modeling question, we’ll explore what types of data are available, how they are structured, and how to retrieve them reproducibly in R. In this module you will: –Retrieve streamflow data using the dataRetrieval package –Download and manage Digital Elevation Models (DEMs) –Retrieve and import remotely sensed raster data for a defined watershed extent Integrate tabular, vector, and raster datasets within a single workflow 5.2.1.2 Background So far, we have focused on conceptual models and parameter behavior. We now shift toward spatially distributed modeling, where environmental processes vary across space. Spatial modeling requires us to integrate hydrological concepts with geospatial data structures. Before doing that, we need a shared understanding of how spatial data are represented. A refresher in GIS concepts: This module assumes familiarity with basic Geographic Information System (GIS) such as coordinate systems (latitude and lognitude, UTM). Let’s review some of the data structures we will use: Gridded data refers to spatial data in raster format. If you have performed any GIS work, you are likely using gridded data. Rasters organize geographical areas into a grid where each regularly spaced cell represents a specific location and contains some attribute representing that location. For example, digital elevation models (DEM) are a form of gridded data. Imagine that we are working in a watershed and we can drop a grid or square celled net over the entire catchment. We can then model the elevation data of that catchment by assigning each cell in the net a mean elevation value for the area represented by the cell. Similarly, we can assign any value we wish to model to the cells. If we were to drop another net and assign a maximum tree height to every cell, we would be adding another ‘layer’ to our model. The spatial resolution of gridded data refers to the length and width of each cell represents. If we want to observe gridded data over time, we can generate a raster stack, where each raster layer represents a point in time. With a raster stack, we can increase the temporal resolution of our gridded data. Vector data describe spatial features using points, lines, and polygons. These spatial features are represented a geometrical objects with associated attributes, like lines to represent linear features such as rivers, or points to represent single locations. We will work with a .shp file in this module that is a collection of points representing the boundary of a watershed. Tabular data refers to data organized in a row-column format. In this class, each of our .csv datasets are a form of tabular data. 5.2.1.3 A refresher in remote sensing: Remote sensing is the science of obtaining information about objects or areas from a distance, typically from aircraft or satellites. It involves the collection and interpretation of data gathered by sensors that detect electromagnetic radiation reflected or emitted from the Earth’s surface and atmosphere. As ecologists, many of us utilize remote sensing to observe and analyze the Earth’s surface and its dynamics, providing valuable insights into environmental processes, land use and land cover changes, natural disasters, and more. Some helpful terms to recall: spatial resolution: Remotely sensed data can vary in its spatial resolution, which refers to the scale of the smallest unit, or level of visual detail. temporal resolution: When used to describe remotely sensed data, we are often talking about how frequently a remote sensing system revisits the same area on the Earth surface. Systems with high temporal resolution revisit the same location frequently, allowing for monitoring rapid changes in phenomena, where a lower resolution may limit a system’s ability to capture short-term events. We often find that data collection requires a balance of spatial and temporal resolution to select the most appropriate data for the coverage and detail that ecological work requires. 5.2.1.4 Ethical conduct When utilizing publicly available datasets, it is important for us as users to: -Be aware of any copyright or licensing agreements. There may be restrictions or usage terms imposed by data providers. Read provided agreements or terms to be sure we are in compliance with these when using datasets in our research. This may also include requirements for registration or subscription established by providers or agencies like USGS or NASA. -Cite and acknowledge the contributions of data providers and researchers whose datasets we utilize. This also facilitates transparency and reproducibility in our research. - Assess the quality and accuracy of geospatial data before using it for decision making purposes. It is important that we consider spatial resolution, temporal resolution and metadata documentation to critically evaluate the reliability of the datasets we use. -Promote Open Data Principles: Try to explore and use publicly available datasets that support transparency and collaboration within our scientific and academic communities. 5.2.2 Repo link The repo for this module can be found here 5.3 Term Project Assignment 1 - 10 pnts As part of your graduate program, whether you are a GA or part of the Professional Masters’ Program, you are required to develop a professional paper. It is intended to give you an in-depth experience in the design, implementation, and completion of an original project. The paper will also be a way to showcase your research interests and accomplishments to potential employers or admissions committees. (See LRES 575 for more). Your term project for this class is not meant to encompass your entire thesis or dissertation. Instead, it should focus on a single, well-defined component that contributes to your broader research. This could be an in-depth exploration of a specific feature of your research question, a feasibility study, or a clear demonstration of a cause-effect relationship within your work. In the upcoming weeks, you will design a repeatable workflow that is executable within the scope of this semester, and at the end of the term, you will present a concise (~8-minute) synopsis of your question and workflow. This assignment is focused on data retrieval; you will develop the main goal and objectives of your term project and explore, evaluate, and select data sources. A key component of this process is formulating a research question that clearly defines both an explanatory variable and a response variable; one is not sufficient without the other. The workflow for this assignment will help you refine your question and identify the datasets needed to address it. Your assignment is considered complete when you have identified at least one explanatory variable and one response variable, along with the corresponding data sources. Repo Link "],["physical-process-models.html", "Chapter 6 Physical Process Models 6.1 Snowmelt Models 6.2 Evapotranspiration", " Chapter 6 Physical Process Models 6.1 Snowmelt Models 6.1.1 Learning Module 9 6.1.1.1 Background: Understanding snowmelt runoff is crucial for managing water resources and assessing flood risks, as it plays a significant role in the hydrologic cycle. Annual runoff and peak flow are influenced by snowmelt, rainfall, or a combination of both. In regions with a snowmelt-driven hydrologic cycle, such as the Rocky Mountains, snowpack acts as a natural reservoir, storing water during the winter months and releasing it gradually during the spring and early summer, thereby playing a vital role in maintaining water availability for various uses downstream. By examining how snowmelt interacts with other factors like precipitation, land cover, and temperature, we can better anticipate water supply fluctuations and design effective flood management strategies. Learning objectives: In this module, our primary focus will be modeling snowmelt as runoff, enabling us to predict when it will impact streamflow timing. We will consider some factors that may influence runoff timing. However, the term ‘snowmelt modeling’ is a field in itself and can represent a lifetime worth of work. There are many uses for snowmelt modeling (e.g., climate science and avalanche forecasting). If you are interested in exploring more on this subject, there is an excellent Snow Hydrology: Focus on Modeling series offered by CUAHSI’s Virtual University on YouTube. Helpful terms: The most common way to measure the water content of the snowpack is by the Snow Water Equivalent or SWE. The SWE is the water depth resulting from melting a unit column of the snowpack. 6.1.1.2 Model Approaches Similar to the model development structure we discussed in the last module, snowmelt models are generally classified into three different types of abalation algorithms Empirical and Data-Driven Models: These models use historical data and statistical techniques to predict runoff based on the relationship between snow characteristics (like snow area) and runoff. They use past patterns to make predictions about the future. The emergence of data-driven models has benefited from the growth of massive data and the rapid increase in computational power. These models simulate the changes in snowmelt runoff using machine learning algorithms to select appropriate parameters (e.g., daily rainfall, temperature, solar radiation, snow area, and snow water equivalent) from different data sources. Conceptual Models: These models simplify the snowmelt process by establishing a simple, rule-based relationship between snowmelt and temperature. These models use a basic formula based on temperature to estimate how much snow will melt. Physical Models: The physical snowmelt models calculate snowmelt based on the energy balance of snow cover. If all the heat fluxes toward the snowpack are considered positive and those away are considered negative, the sum of these fluxes is equal to the change in heat content of the snowpack for a given time period. Fluxes considered may be net solar radiation (solar incoming minus albedo), thermal radiation, sensible heat transfer of air (e.g., when air is a different temperature than snowpack), latent heat of vaporization from condensation or evaporation/sublimation, heat conducted from the ground, advected heat from precipitation examples: layered snow thermal model (SNTHERM) and physical snowpack model (SNOWPACK), Many effective models may incorporate elements from some or all of these modeling approaches. 6.1.1.3 Spatial complexity We may also identify models based on the model architecture or spatial complexity. The architecture can be designed based on assumptions about the physical processes that may affect the snowmelt to runoff volume and timing. Homogenous basin modeling: You may also hear these types of models referred to as ‘black box’ models. Black-box models do not provide a detailed description of the underlying hydrological processes. Instead, they are typically expressed as empirical models that rely on statistical relationships between input and output variables. While these models can predict specific outcomes effectively, they may not be ideal for understanding the physical mechanisms that drive hydrological processes. In terms of snow cover, this is a simplistic case model where we assume: the snow is consistent from top to bottom of the snow column and across the watershed melt appears at the top of the snowpack water immediately flows out the bottom This type of modeling may work well if the snowpack is isothermal, if we are interested in runoff over large timesteps, or if we are modeling annual water budgets in lumped models. Vertical layered modeling: Depending on the desired application of the model, snowmelt may be modeled in multiple layers in the snow column (air-snow surface to ground). Climate models, for example, may estimate phase changes or heat flux and consider the snowpack in 5 or more layers. Avalanche forecasters may need to consider grain evolution, density, water content, and more over hundreds of layers! Hydrologists may also choose variable layers, but many will choose single- or two-layer models for basin-wide studies, as simple models can be effective when estimating basin runoff. Here is a study by Dutra et al. (2012) that looked at the effect of the number of snow layers, liquid water representation, snow density, and snow albedo parameterizations within their tested models. Table 1 and figures 1-3 will be sufficient to understand the effects of changes to these parameters on modeled runoff and SWE. In this case, the three-layer model performed best when predicting the timing of the SWE and runoff, but density improved more by changing other parameters rather than layers (Figure 1). Lateral spatial heterogeneity: The spatial extent of the snow cover determines the area contributing to runoff at any given time during the melt period. The more snow there is, the more water there will be when it melts. Therefore, snow cover tells us which areas will contribute water to rivers and streams as the snow melts. In areas with a lot of accumulated snow, the amount of snow covering the ground gradually decreases as the weather warms up. This melting process can take several months. How quickly the snow disappears depends on the landscape. For example, in mountainous ecosystems, factors like elevation, slope aspect, slope gradient, and forest structure affect how the snow can accumulate, evaporate or sublimate and how quickly the snow melts. For mountain areas, similar patterns of depletion occur from year to year and can be related to the snow water equivalent (SWE) at a site, accumulated ablation, accumulated degree-days, or to runoff from the watershed using depletion curves from historical data. Here is an example of snow depletion curves developed using statistical modeling and remotely sensed data. The use of remotely sensed data can be incredibly helpful to providing estimates in remote areas with infrequent measurements. Observing depletion patterns may not be effective in ecosystems where patterns are more variable (e.g., prairies). However, stratified sampling with snow surveys, snow telemetry networks (SNOTEL) or continuous precipitation measurements can be used with techniques like cluster analyses or interpolation, to determine variables that influence SWE and estimate SWE depth or runoff over heterogeneous systems. You can further explore all readings linked in the above section. These readings may assist in developing the workflow for your term project, though they are optional for completing this assignment. However, it is recommended that you review the figures to grasp the concepts and retain them for future reference if necessary. 6.1.1.4 Model choices: My snow is different from your snow When determining the architecture of your snow model, your model choices will reflect the application of your model and the processes you are trying to represent. Recall that parsimony and simplicity often make for the most effective models. So, how do we know if we have a good model? Here are a few things we can check to validate our model choices: Model Variability: A good model should produce consistent results when given the same inputs and conditions. Variability between model runs should be minimal if the watershed or environment is not changing. Versatility: Check the model under a range of scenarios different from the conditions under which it was developed. The model should apply to similar systems or scenarios beyond the initial scope of development Sensitivity Analysis: We reviewed this a bit in the Monte Carlo module. How do changes in model inputs impact outputs? A good model will show reasonable sensitivity changes in input parameters, with outputs responding as expected. Validation with empirical data: Comparison with real-world data checks whether the model accurately represents the actual system Applicability and simplicity: A good model should provide valuable insights or aid in decision-making processes relevant to the problem it addresses. It strikes a balance between complexity and simplicity, avoiding unnecessary intricacies that can lead to overfitting or computational inefficiency while sufficiently capturing the system’s complexities. 6.1.2 Labwork (20 pnts) 6.1.2.1 Download the repo for this lab HERE In this module, we will simulate snowmelt in a montane watershed in central Colorado with a simple temperature model. The Fool Creek watershed is located in the Fraser Experimental Forest (FEF), 137 km west of Denver, Colorado. The FEF contains several headwater streams (Strahler order 1-3) within the Colorado Headwaters watershed which supplies much of the water to the populated Colorado Front Range. This Forest has been the site of many studies to evaluate the effects of land cover change on watershed hydrology. The Fool Creek is a small, forested headwater stream, with an approximate watershed area of 2.75 km^2. The watershed elevation ranges from approximately 2,930m (9,600ft) at the USFS monitoring station to 3,475m (11,400ft) at the highest point. There is a SNOTEL station located at 3,400m. We will refer to data from the SNOTEL station as ‘high elevation’ or ‘upper watershed’ data. Data collected from the outlet may be referred to as ‘low elevation’, ‘lower watershed’ or ‘outlet’ data. Fool Creek delineated watershed Let’s look at some data for Fool Creek: This script collects SNOTEL input data using snotelr. The SNOTEL automated collection site in Fool Creek supplies SWE data that can simplify our runoff modeling. Lets explore the sites available through snotelr and select the site of interest. 6.1.2.2 Import data: First, we will generate a conceptual runoff model using the date, daily mean temperature, SWE depth in mm, and daily precipitation (mm) data from the SNOTEL station. We will use select() to keep the desired columns only. Then we will use mutate() to add a water year, and group_by() to add a cumulative precipitation column for each water year. We will also download flow data collected by USFS at the Fool Creek outlet at 2,930m to compare to simulated runoff. Let’s look at the data for the SNOTEL station at 3400m in elevation. 6.1.2.3 Calculate liquid input to the ground by analyzing the daily changes in Snow Water Equivalent (SWE) and precipitation. This script incorporates temperature conditions to determine when to add changes to liquid input. Let’s visualize the timing disparities between precipitation and melt input in relation to discharge. 6.1.2.4 Modeling SWE Now we are going to shift focus. Instead of modeling runoff, let’s assume we have precipitation data from 3400m (SNOTEL, upper watershed) in the Fool Creek watershed, but no SWE data. How could we model SWE using temperature data? Next, we will create a model to estimate SWE and then compare the results to the actual SWE data from the SNOTEL station in the upper Fool watershed. The model below is a variation of the degree-day method. In this simulation, temperature determines whether precipitation contributes to snowpack or becomes rain. When temperatures fall below a certain threshold and precipitation occurs, that amount is added to the snow water equivalent (SWE) accumulation. To find the optimal parameters for the upper watershed, we will run 100 Monte Carlo simulations and analyze the results. The key parameters we are testing include: Pack threshold – The SWE level at which melting begins. Degree-day factor – The rate at which snow melts per degree above freezing. Threshold temperature – The temperature that separates snow from rain. By adjusting these parameters, we aim to refine our model and improve its accuracy in simulating snowpack dynamics. This simple model seems to simulate SWE well if assessed with the NSE. Let’s go through the procedure again, but this time we’ll model SWE at the Fool Creek outlet (lower elevation), where we (really) don’t have daily SWE data. First, we’ll import the precipitation data collected from a USFS meteorological station near the Fool Creek outlet. We also need temperature data. This is also collected at the USFS Lower Fool meteorological station. Even though this meteorological station is fairly close to the SNOTEL station, the sites differ in elevation. Lets compare the observed cumulative precipitation between the upper watershed and the lower. Now let’s estimate SWE for the Lower Fool Creek. Again, we’ll just simulate a single year. Run another set of simulations and compare the values of the best performing parameters across sites. Since we don’t have SWE measurement for Lower Fool Creek, let’s see how the simulated values for the lower watershed compare to the observed SWE from the high elevation SNOTEL station. 6.2 Evapotranspiration 6.2.1 Repo here 6.2.2 Learning Module 10 20pnts 6.2.2.1 Background Suggested reading: Forest Evapotranspiration: Measurement and Modeling at Multiple Scales Evapotranspiration (ET) encompasses all processes through which water moves from the Earth’s surface to the atmosphere, comprising both evaporation and transpiration. This includes water vaporizing into the air from soil surfaces, the capillary fringe of groundwater, and water bodies on land. Much like snowmelt modeling, ET modeling and measurements are critical to many fields and could be a full course on its own. We will be focused on the basics of ET, modeling and data retrieval methods for water balance in hydrological modeling. Evapotranspiration is an important variable in hydrological models, as it accounts for much of the water loss in a system, outside of discharge. Transpiration, a significant component of ET, involves the movement of water from soil to atmosphere through plants. This occurs as plants absorb liquid water from the soil and release water vapor through their leaves. To gain a deeper understanding of ET, let’s review transpiration. 6.2.2.1.1 Transpiration Plant root systems to absorb water and nutrients from the soil, which they then distribute to their stems and leaves. As part of this process, plants regulate the loss of water vapor into the atmosphere through stomatal apertures, or transpiration. However, the volume of water transpired can vary widely due to factors like weather conditions and plant traits. Vegetation type: Plants transpire water at different rates. Some plants in arid regions have evolved mechanisms to conserve water by reducing transpiration. One mechanism involves regulating stomatal opening and closure. These plants can minimize water loss, especially during periods of high heat and low humidity. This closure of stomata can lead to diel and seasonal patterns in transpiration rates. Throughout the day, when environmental conditions are favorable for photosynthesis, stomata open to allow gas exchange, leading to increased transpiration. Conversely, during the night or under stressful conditions, stomata may close to conserve water, resulting in reduced transpiration rates. Humidity: As the relative humidity of the air surrounding the plant rises the transpiration rate falls. It is easier for water to evaporate into dryer air than into more saturated air. Soil type and saturation: Clay particles, being small, have a high capacity to retain water, while sand particles, being larger, readily release water. During dry periods, transpiration can contribute to the loss of moisture in the upper soil zone.When there is a shortage of moisture in the soil, plants may enter a state of senescence and reduce their rate of transpiration. Temperature: Transpiration rates go up as the temperature goes up, especially during the growing season, when the air is warmer due to stronger sunlight and warmer air masses. Higher temperatures cause the plant cells to open stomata, allowing for the exchange of CO2 and water with the atmosphere, whereas colder temperatures cause the openings to close. The availability and intensity of sunlight have a direct impact on transpiration rates. Likewise, the aspect of a location can influence transpiration since sunlight availability often depends on it. Wind &amp; air movement: Increased movement of the air around a plant will result in a higher transpiration rate. Wind will move the air around, with the result that the more saturated air close to the leaf is replaced by drier air. 6.2.2.2 Measurements In the realm of evapotranspiration (ET) modeling and data analysis, you’ll frequently encounter the terms potential ET and actual ET. These terms are important to consider when selecting data, as they offer very different insights into water loss processes from the land surface to the atmosphere. Potential Evapotranspiration (PET): Potential ET refers to the maximum possible rate at which water could evaporate and transpire under ideal conditions. These conditions typically assume an ample supply of water, unrestricted soil moisture availability, and sufficient energy to drive the evaporative processes. PET is often estimated based on meteorological variables such as temperature, humidity, wind speed, and solar radiation using empirical equations like the Penman-Monteith equation. Actual Evapotranspiration (AET): Actual ET, on the other hand, represents the observed or estimated rate at which water is actually evaporating and transpiring from the land surface under existing environmental conditions. Unlike PET, AET accounts for factors such as soil moisture availability, vegetation cover, stomatal conductance, and atmospheric demand. It reflects the true water loss from the ecosystem and is often of greater interest in hydrological modeling, as it provides a more realistic depiction of water balance dynamics. The formula for converting PET to AET is: AET = PET * Kc Where: AET is the actual evapotranspiration, PET is the potential evapotranspiration, and Kc is the crop coefficient. The crop coefficient accounts for factors such as crop type, soil moisture levels, climate conditions, and management practices. It can vary throughout the growing season as well. 6.2.2.2.1 Direct measurements: There are several methods to measure ET directly like lysimeters and gravimetric analysis, but this data rarely available to the public. There has been a concerted effort to enhance the accessibility of Eddy Covariance data, so the dataset mentioned in the video below may expand in the years to come. This video focuses on CO2 as an output of eddy covariance data, but among the ‘other gases’ mentioned, water vapor is included, offering means to estimate actual ET. The video also provides a resource where you might find eddy covariance data for your region of interest. knitr::include_url(&quot;https://www.youtube.com/embed/CR4Anc8Mkas&quot;) 6.2.2.2.2 Remote sensing: Remote sensing of evapotranspiration (ET) involves the use of satellite or airborne sensors to observe and quantify the exchange of water vapor between the Earth’s surface and the atmosphere over large spatial scales. This approach offers several advantages, including the ability to monitor ET across diverse landscapes, regardless of accessibility, and to capture variations in ET over time with high temporal resolution. Remote sensing data, coupled with energy balance models, can be used to estimate ET by quantifying the energy fluxes at the land surface. These models balance incoming solar radiation with outgoing energy fluxes, including sensible heat flux and latent heat flux (representing ET). Remote sensing-based ET estimates are often validated and calibrated using ground-based measurements, such as eddy covariance towers or lysimeters, to ensure accuracy and reliability. It can be helpful to validate these models yourself if you have a data source available in your ecoregion as a ‘sanity check’. Keep in mind that there are numerous models available, some of which may be tailored for specific ecoregions, resulting in significant variations in estimated evapotranspiration (ET) for your area among these models. If directly measured ET data is not available, you can check model output in a simple water balance. For example, inputs - outputs for your watershed (Ppt - Q - ET) should be approximately 0 (recall from our transfer function module that it is likely not exact). If the ET estimate matches precipitation, it’s likely that the selected model is overestimating ET for your region. Some resources for finding ET modeled from remote sensing data: ClimateEngine.org - This is a fun resource for all kinds of data. Actual evapotranspiration can be found in the TerraClimate dataset. OpenET - you need a Google account for this one. This site is great if you need timeseries data. You can select ‘gridded data’ and draw a polygon in your area of interest. You can select the year of interest at the top of the map, and once the timeseries generates, you can view and compare the output of seven different models. 6.2.2.3 Modeling: 6.2.3 Labwork (20 pts): For this assignment, we will work again in the Fraser Experimental Forest (same site as snowmelt module). Not only are there meteorological stations in our watershed of interest, but there is a Eddy Covariance tower nearby, in a forest with the same tree community as our watershed of interest. We can use this data to verify our model output for modeled data. 6.2.3.1 The Evapotranspiration package We will test a couple of simple methods that require few data inputs. However, it may be helpful to know that this package will allow calculations of PET, AET and Reference Crop Evapotranspiration from many different equations. Your selection of models may depend on the study region and the data available. 6.2.3.2 Import libraries 6.2.3.3 Import data 6.2.3.3.1 Meteorological data We’ll import three datasets in this workflow, one containing actual data from the eddy covariance tower that has been converted to AET. This dataset ranges from 2018 to 2023 Also imported is the discharge (Q) data from our watershed. Note that data is not collected during periods of deep snow. We will use this data to estimate the total discharge for 2022. Additionally, we’ll import the meteorological data from our watershed. We are going to estimate ET for our watershed during the 2022 water year, though met data is provided in calendar years, so we will need to extract the desired data from the meteorological dataframe. 6.2.3.4 Data formatting We will model ET at daily timesteps. Note that all imported data is for different timesteps, units and measurements than what we need for analysis. It is common practice to manipulate the imported data and adjust it to align with our model functions. Ensuring the correct structure of the data can prevent potential issues later in the code Now we have the RHmax and RHmin values that we will need for our ET models 6.2.3.4.1 Precipitation To add a layer of checks we will also check precipitation totals from the watershed from meteorological stations. This dataframe provides cumulative precipitation, though the accumulation begins on January 1. Therefore we will need to calculate daily precipitation for each day of the water year and generate a new cumulative value that accumulates from October 1 - Sept 30 (water year cumulative total). This was a lot of work to save one value. 6.2.3.5 Pseudocode Often when writing code, it is necessary to start with pseudocode. Pseudocode allows us to plan and structure the logic of the script before implementing it in a specific programming language. It is typically a mix of natural language and code that serves as a blueprint for the script(e.g., “I’ll used ‘merge’ to combine my data frames by a shared date column”). Once the workflow is outlined, then we can translate it into actual code using appropriate syntax. Q1. (2 pnts) In you own words, what did we do at each step in the above chunk and what is cumulative_ppt_2022. Why do we want to know this? Hint: if the third step in the above chunk is confusing, look at the new daily_ppt_mm column after running the first two steps to see what happens without it. ANSWER: 6.2.3.5.1 Discharge data Our next step will be to import and format discharge data collected from the weir at Fool Creek. Again, data collection starts on April 20th this year. To estimate discharge between Sept 31 of the previous year and April 20th of 2022, we will assume daily discharge between these dates is the mean of the discharge values from each of these dates. Q2 (3 pnts) Write pseudocode in a stepwise fashion to describe the workflow that will need to happen to estimate the total volume of water in mm/watershed area for 2022 lost to stream discharge. Consider that the units provided by USFS are in cubic feet/second (cfs). ANSWER: 1. Import dataframe 2. … 6.2.3.5.2 Evapotranspiration data Import and transform ET data. Note that ET data is in mmol/m2/s. We want to convert this to mm/day. Eddy covariance data collected from towers represents the exchange of gases, including water vapor, between the atmosphere and the land surface within a certain area known as the “footprint.” This footprint area is not fixed; it changes in size and shape depending on factors like wind direction, thermal stability, and measurement height. Typically, it has a gradual border, meaning that the influence of the tower measurements extends beyond its immediate vicinity. For our specific tower data, we’ve estimated the mean flux area, or footprint area, to be approximately 0.4 square kilometers. However, when estimating the total evapotranspiration (ET) for our entire watershed, we need to extrapolate the ET measured by the tower to cover the entire watershed area. This extrapolation involves scaling up the measurements from the tower footprint to the larger area of the watershed to get a more comprehensive understanding of water vapor exchange over the entire region. This appears to be a fairly well balanced water budget, especially considering that we have made some estimates along the way. Let’s see how this ET data compares to modeled data. 6.2.3.6 The Priestly Taylor method The Priestley-Taylor method is a semi-empirical model that estimates potential evapotranspiration as a function of net radiation. This method requires a list which contains: (climate variables) required by Priestley-Taylor formulation: Tmax, Tmin (degree Celcius), RHmax, RHmin (percent), Rs (Megajoules per sqm) or n (hour). We have measurements of relative humidity (RH) data from within our watershed meteorological station. However, RH data can be found through many sources providing local weather data. n refers to the number of sunshine hours. Evapotranspiration comes with a list physical ‘constants’, but we want to replace important constants with data specific to our cite. We can change the function name to specify the desired model. Now we’ll try thethe Hamon method. Recall that each of these are estimates of potential ET. Crop coefficients (Kc values) for specific tree species like those found in the Fraser Experimental Forest (lodgepole pine or Englemann spruce) may not be as readily available as they are for agricultural crops. However, you may be able to find some guidance in scientific literature or forestry publications. From what we have found, Kc estimates for lodgepole pine forests can be between 0.4 and 0.8. These values may vary depending on factors such as climate, soil type, elevation, and other site-specific factors. Our own estimates using water balance data from dates that correspond with the eddy flux tower data suggest seasonal fluctuations, with a mean of 0.55. AET = PET * Kc Let’s plot the two modeled ET timeseries with the eddy covaraince tower data. Q3. (4 pnts) Consider the two evapotranspiration (ET) models we have generated, one consistently underestimates ET, while the other consistently overestimates summer values and underestimates winter values. Consider ecosystem specificity in modeling. Why do you think these two methods generate such different estimates? What ecosystem-specific factors might contribute to the discrepancies between models? ANSWER: Q4 (3 pnts) Let’s assume we do not have eddy covariance tower data for this time period, but you have the provided discharge and precipitation measurements. Which model would you choose to estimate an annual ET sum and why? ANSWER: Q5 (3 pnts) In our modeling script, while our main objective was to estimate evapotranspiration (ET), which specific aspect required the most labor and attention to detail? Reflect on the tasks involved in data preparation, cleaning, and formatting. How did these preliminary steps impact the overall modeling process? ANSWER: Q6 (5 pnts) Review the sub-watershed figure and analyze how vegetation characteristics (e.g., height, age), slope, and aspect vary across high-elevation watersheds in the Fraser Experimental Forest. Utilize Google Maps’ high-resolution imagery as an additional resource to observe these landscape features. How might these factors interact to influence actual evapotranspiration (AET) in this region? Provide a hypothetical comparison of two sub-watersheds, describing how differences in these variables could lead to variations in AET. ANSWER: Fool Creek delineated watershed "],["spatial-models-and-gis-integration.html", "Chapter 7 Spatial Models and GIS Integration 7.1 DEM processing 7.2 HBV Hydrology Model", " Chapter 7 Spatial Models and GIS Integration 7.1 DEM processing 7.1.1 Learning Module 11 7.1.1.1 Objective: Users will explore basic hydrological tools through the process of DEM preprocessing and defining stream networks for a watershed in the Fraser Experimental Forest in Colorado using Whitebox Tools for R. This exercise will also demonstrate methods for writing functions and efficiently handling multiple files simultaneously, including importing, processing, and exporting data within the context of Whitebox Tools for R. Hydrological analysis preprocessing involves the use of digital elevation model (DEM) raster data to establish a watershed model and a simulation of surface hydrological processes. These steps enable us to quantify key parameters such as flow accumulation, stream network characteristics, and hydrological connectivity, which are essential for studying the dynamics of water movement within a landscape. Overall, preprocessing is the set of foundational steps in hydrological modeling and analysis. Whitebox Tools is an advanced geospatial data analysis platform that can be used to perform common geographical information systems (GIS) analysis operations. This platform was developed with the Center for Hydrogeomatics in Guelph University so it is focused on hydrological analysis. With just a DEM, it allows us to produce a multitude of outputs that can be used for future analysis (Lindsay, 2016) doi:10.1016/j.cageo.2016.07.003). While we are demonstrating its use in R, these tools are also available in QGIS and Python platforms. 7.1.2 THE LINK TO THE REPO IS HERE 7.1.3 Labwork (20 pts) 7.1.3.1 Installing libraries We are going to try installing the whitebox R package from CRAN as it should be the simplest method. However, if this does not work for you, you can install the development version from GitHub by putting this inside a code chunk: if (!require(“remotes”)) install.packages(‘remotes’) remotes::install_github(“opengeos/whiteboxR”, build = FALSE) More information on installation can be found at: https://cran.r-project.org/web/packages/whitebox/readme/README.html Helpful whitebox documentation can be found at https://jblindsay.github.io/wbt_book/preface.html. Essentially, we will be using input rasters via filepath and output filepaths as arguments for various whitebox functions. The script is designed to perform functions on all rasters in a given folder at once. When writing scripts, developers typically follow a standard workflow: 1. Import required libraries 2. Generate functions useful throughout the script 3. Establish working directories or paths to other directories if needed 4. Import data 5. Data Cleaning and Preprocessing - this may involve handling missing values, removing outliers, converting to preferred units. etc. 6. Exploratory Data Analysis - it is beneficial to explore data visually to help understand the characteristics of the data. 7. Apply functions, or models or other analytical techniques 8. Evaluate results - If modeling, this may involve comparing model predictions with observed data or conducting sensitivity analysis 9. Visualize and report - plots, maps and tables can be effective ways to communicate findings. While you might find slight variations among collaborators, following this general workflow ensures that our scripts are structured in a way that facilitates easy sharing and reproducibility of results. 7.1.3.1.1 Generate functions Since we have imported the required libraries, let’s generate some functions. Q1.(2 pnt) What does date: “19 February, 2026” in the header do? ANSWER: Q2.(3 pnt) What does ‘recursive = TRUE’ do? What would the ‘recursive = FALSE’ return? ANSWER: EXTRA (1 pnt) : Rewrite this function to generate ‘splitbase’ with fewer lines. For example, what happens when you replace ‘basename’ in ‘splitbase &lt;- strsplit(basename,’_‘)[[1]][1]’ with ‘splitpath[[1]][2]’? Q3. (3 pnt) What is the point of writing a function? Why do you think it is advantageous to write functions early in your script? ANSWER: 7.1.3.1.2 Establish new directories(folders) in our working directory. We will use these to store the outputs of the Whitebox functions. Check your working directory, you should see the new folders there. 7.1.3.2 Resample DEMs Here we will start with LiDAR data with 0.5m resolution. While this resolution has useful applications, a high resolution DEM can make hydrological models very computationally expensive with little or no improvement to the output. If you have the option of high resolution data in your work, you can test model outputs at different resolutions to determine what is the most efficient and effective resolution for your work. Here, we will resample our LiDAR data to a 10m resolution. Q4. (3 pnt) Did we use the function extractsitenames in the above chunk? How? What did it do? ANSWER: Let’s quickly check our work by importing a resampled DEM and checking the resolution. Note: This can also be done without importing the raster to the workspace by installing the library gdalUtils. Q5.(2 pnt) What is the resolution of the resampled DEM? Where and how could we change the resolution to 30m if desired? ANSWER: 7.1.3.3 Filling and breaching When performing hydrological analysis on a DEM, the DEM usually needs to be pre-processed by ‘filling’ or ‘breaching’ any depressions or sinks to create a hydraulically connected and filled DEM. There are several depression or pit filling options available in whitebox. Breach depressions can be a better option that just pit filling according to whitebox documentation, however, some users argue that this can smooth too much, resulting in an altered watershed delineation. It is prudent to investigate different DEM pre-processing methods and their resulting DEM. You can fill depressions directly, breach depressions and then fill them, applying breach or fill single cell pit before breach/fill depressions, and use the one that generates the most reasonable watershed delineation results. Here we are going to make extra sure our flowpaths are uninhibited by first filling in single cell pits, and then breaching any larger depressions. Q6 (2 pnt) What is breach1? How is lapply using breach1? ANSWER: 7.1.3.4 Flow direction and accumulation rasters Q7 (2 pnt) Check out the WhiteboxTools User Manual. What does a d8pointer raster tell us and what might we use it for? ANSWER: Let’s visualize some of our work so far: Q8. (3 pnt) What are the units for the values that you see in each of these legends? It may be helpful to check out the Manual again. ANSWER: 7.1.3.5 Streams Define streams using the flow accumulation raster. Use of the wbt_extract_streams function will return a raster with stream cells indicated only. Sometimes we would prefer to see the stream within the watershed boundary. Here we are using the extent of the flow accumulation raster to generate a raster with ‘0’ to indicate watershed cells, along with stream cells indicated by the streams.tif. This is a demonstration for one watershed. 7.1.3.6 Final thoughts: There are many more hydrological preprocessing and analysis tools available through Whitebox for R. If you are interested in watershed delineation in R, there is a tutorial here that is fairly easy to follow. However, if you find that you use these tools frequently and do not use R much in other work, you may also consider these options for hydrological analysis: 1. SAGA SAGA tools offer a versatile suite of geospatial processing capabilities accessible through both QGIS and ArcGIS plugins as well their standalone GUI interface. Often I find the GUI easiest for preprocessing, then I will import SAGA’s output rasters to QGIS for formatting or map making, or into model scripts. SAGA has a robust online support community, so it can be a valuable resource for hydrological work. 2. Similarly, Whitebox GAT tools can be used as plugin to QGIS and ArcGIS, providing Whitebox functionality directly with in a GIS environment. When using these tools, the order of operations is similar to our work above: fill the DEM, generate a flow direction and flow accumulation raster, identify channels, delineate watersheds, then you can move forward according to the specificity of your project. Ultimately, the choice of workflow is yours, but I suggest documenting your process as you proceed, including links or file paths to projects and scripts within a written or diagrammed workflow (or workflow software). It’s also important to carefully consider the organization and storage of your projects and files. For instance, files generated by a GIS project should be readily accessible to any associated scripts. Returning to a preprocessing step can be (sometimes painfully) challenging if there’s no clear way to trace back your workflow and regenerate a crucial layer. Take another look at the model selection process from Bevin (above), and think about how you can make your workflow easier to recall and repeat. A little planning upfront can save so much time later. (Seriously, so. much. time.) To document and track iterations efficiently, consider using a tool that fits your workflow: - Bookdown in R - this is useful if your work involves R scripts or Markdown. This is the tool we used to develop the tutorial ‘book’ for this course and I frequently use it to develop and share my own research scripts, figures and workflows. - Obsidian or Notion - Better options if R scripts aren’t a major part of your work, allowing for flexible organization and easy cross-referencing. - You can even make a slide in Power Point with a work flow chart, inserting links, file paths and written steps into the chart as you go. As you work on your term projects over the next couple of weeks, keep these tips in mind. If you haven’t started the thesis course yet (LRES 575), you’ll likely want to revisit the workflow you develop now and continue refining it for your thesis. Set yourself up for success by making it easy to track, recall and build on your work. Think about how you can make this process smoother for your future self. Your future self will thank you. Good Luck! 7.2 HBV Hydrology Model 7.2.1 Learning Module 12 Adapted from Fabian Nippgen (REWM 4500 5500) 7.2.1.1 Background: In this exercise, we will pull together many aspects of what we have learned so far to calibrate the HBV model to weighted data from the Fraser Experimental Forest in Colorado using precipitation/runoff and potential evapotranspiration (PET) data derived from the methods explored in the snowmelt and evapotranspiration modules. HBV is: In the previous labs, we have worked with single-process models. HBV is more complex process-based model using many parameters to describe the physical processes of the hydrologic cycle https://rpubs.com/sdhakal/848236 Reproduced from: Durga Lal Shrestha &amp; Dimitri P. Solomatine (2008) Data‐driven approaches for estimating uncertainty in rainfall‐runoff modelling, International Journal of River Basin Management, 6:2, 109-122, DOI: 10.1080/15715124.2008.9635341 HBV versions with a GUI exists, but the data manipulation is more convenient in a coding environment, and we want you to be able to see ‘under the hood’. Instead of dealing with the inner workings of the model too much, we will put the focus on importing and organizing the data, getting the model running and manual optimization. The HBV realization we will be using is contained in the R package TUWmodel. Please search that package in your browser and familiarize yourself with it (that will be the first step). TUW simply means “Technische Universitaet Wien”, or Vienna University of Technology. The model is slightly different from the original HBV but still similar enough to call it HBV. We also need to understand the arguments and parameters required for the model. Type ‘?TUWmodel’ in your console. Note that the package comes with an example dataset, and you can run examples either by copying and pasting the data(), TUWmodel() and plots into your console, or you can find and click ‘Run examples’ under the Examples header and view the script and plot outputs in the Help window. The first example shows how the ‘TUWmodel’ can be run given specifications for a long set of parameters (param). You can provide the model with precip, air temperature, area and ET along with the parameter set and it will simulate SWE, snow melt and discharge. We can obtain measurements for each of these variables, so we will import all of this data and test the fit of the simulated data. https://cran.r-project.org/web/packages/TUWmodel/TUWmodel.pdf 7.2.1.2 Import data: We will generate a conceptual runoff model starting with: RainMeltWeighted_mm –&gt; liquid input to the ground, combination of rain and melt PWeighted_mm –&gt; Precipitation measured at the SNOTEL sites, both liquid and frozen. This is NOT water that went into the ground at that point. This is, however, the P input time series for the HBV model. SweWeighted_mm –&gt; Snow water equivalent at the SNOTEL sites. We can use this to evaluate our snow routine. TempWeighted –&gt; Either daily mean, min, or max. We will use the mean. Discharge_mm –&gt; Q observed at the outlet. ET –&gt; Potential Evapotranspiration calculated using relative humidity and temperature. 7.2.2 Repo link Download the repo for this lab HERE 7.2.3 Labwork (20 pnts) Our first steps will be to import and format the data. As you might recall from the previous modules, this step can require the most script and ‘work’, but is critical to a valid model output. In this assignment, we have simplified much of the data collection for you, as the steps are ones you have already completed in previous modules. In the .csv imported below, we started by collecting the same SNOTEL data that you used in the snow melt module, but downloaded for a greater date range (water years 2018-2022) (date, SWE depth in mm, and daily precipitation (mm)) However, you will notice that the column names of the provided table contain ‘weighted_’. # Import model data # indata &lt;- read.csv(&quot;weighted_data_fool_HBV.csv&quot;)%&gt;% # mutate(date = ymd(date)) %&gt;% # rename(Q_m3_s = m3_s, Q_mm_day = mm_day, runoff_input.mm = input.mm ) # # str(indata) As we noted in the Snowmelt module, factors such as elevation and vegetation will affect the of snow-to-runoff rate. Similarly, temperature and precipitation differ with elevation, meaning that SNOTEL data from a single location may not fully represent conditions across the entire watershed. To account for these variations, the “weighted” values for this exercise have been adjusted using linear scaling relationships that estimate average conditions across the watershed. The adjustments use elevation-precipitation and elevation-temperature relationships derived from inverse distance weighting to better approximate spatially distributed hydrological inputs rather than relying on a single point measurement. If you are interested, you can start exploring spatial interpolation with: 1. Thiessen polygons 2. Inverse distance weighting 3. Kringing methods to determine if any of these are applicable to your study area. Other columns in our imported data set include: Q_m3_s and Q_mm_day - Discharge (Q) collected at the Fool Creek outlet by USFS, in units of cubic meters per second and mm per day from April until October. Tmax_c, Tmin_c and Tmean_c - Typically, we could find daily mean, max and minimum temperatures in the SNOTEL datasets, however, this particular station is missing temperature data (due to restrictions on technical access in 2020), so we retrieved temperature data from GridMET through Climate Engine. This highlights the importance of evaluating each variable for completeness. It will save you the headache of running the entire workflow, only to find that model outputs cannot be simulated for the later half of 2020 due to missing input data. RHmin, RHmax - Relative humidity daily min and max, also from GridMET, accessed through Climate Engine SWEdiff.mm, Pdiff.mm and runoff_input.mm - all daily outputs of a temperature based snowmelt model (the same as in the snow melt module). runoff_input is the estimated daily input to the stream from melted snow and liquid precipitation combined. Keep in mind that you can plot more than two variables in a single plot, but they will be most helpful if you group variables with a similar y-scale. For example, cumulative precipitation or SWE values will have a different range than variables that represent daily measurements like ‘input_mm’. Alternatively, you can add a secondary y-axis to your plot. # ggplot(data = indata) + # geom_point(aes(x = date, y = runoff_input.mm, color = &quot;runoff_input.mm&quot;), size = 0.5) + # Assign a label for legend # geom_point(aes(x = date, y = Q_mm_day, color = &quot;mm/day&quot;), size = 0.5) + # Assign a different label # geom_point(aes(x = date, y = weighted_precip.mm, color = &quot;weighted_precip.mm&quot;), size = 0.5) + # Assign a different label # geom_point(aes(x = date, y = Pdiff.mm, color = &quot;Pdiff.mm&quot;), size = 0.5) + # scale_color_manual(values = c(&quot;runoff_input.mm&quot; = &quot;blue&quot;, &quot;mm/day&quot; = &quot;red&quot;, &quot;weighted_precip.mm&quot; = &quot;green&quot;, &quot;Pdiff.mm&quot; = &#39;purple&#39;)) + # Customize colors # labs(color = &quot;Discharge Units&quot;) + # Legend title # theme_minimal() 2,930m. watershed_area_m2 &lt;- 2640000 Note that while the stream is snow-covered, there are no stage/flow measurements being made at this site. For many of our calculations to work, we will not want NA in our data frames. In this data set, if we view the tabular data, the end discharge reading (Q_mm_day), is very similar to the first in the following calendar year. For this example, we will then fill the NA values with the mean of the final and first readings for each winter (Oct - April) NA string. # Rather than scrolling through a dataframe, this gives us a sum of &#39;NA&#39; ros in this column # sum(is.na(indata$Q_mm_day)) # # Function to fill NA values in Q_mm_day during winter (Oct - Apr) # fill_na_winter &lt;- function(df) { # df &lt;- df %&gt;% # arrange(date) %&gt;% # Ensure data is sorted # mutate( # month = month(date), # is_winter = month %in% c(10, 11, 12, 1, 2, 3, 4) # Identify winter months # ) # # # Identify NA stretches in winter months # na_indices &lt;- which(is.na(df$Q_mm_day) &amp; df$is_winter) # # for (idx in na_indices) { # # Find the last non-NA before the current NA # prev_value &lt;- df$Q_mm_day[max(which(!is.na(df$Q_mm_day[1:(idx - 1)])))] # # # Find the next non-NA after the current NA # next_value &lt;- df$Q_mm_day[min(which(!is.na(df$Q_mm_day[(idx + 1):nrow(df)])) + idx)] # # # Replace the NA with the average of prev_value and next_value # if (!is.na(prev_value) &amp; !is.na(next_value)) { # df$Q_mm_day[idx] &lt;- (prev_value + next_value) / 2 # } # } # # return(df) # } # # # Apply the function # indata &lt;- fill_na_winter(indata) Let’s try our quick check again: # Now we should see zero NA in this column # sum(is.na(indata$Q_mm_day)) Next, we need to add daily potential evapotranspiration (PET) to the dataset. The evapotranspiration module covered some of the numerous pre-built functions available in various R packages designed for ET estimation. For instance, the ‘Evapotranspiration’ package provides ET estimates derived from approximately 20 distinct equations or variations. These functions require precise data formatting to ensure compatibility with the function arguments. To help you structure your function inputs similarly, many packages come with example datasets in their documentation. SPEI is another package that offers ET functions. The github repository for this package has been updated fairly recently, which can be important to verify. We chose to write our own function here so you could ‘see under the hood’. Not all packages are equally maintained, as they are often developed by researchers or modelers to improve the repeatably of their work, and contribute to the broader scientific community. Once funding ceases, or if the original developer moves on to new projects, a custom package may no longer receive updates or support. As R, RStudio and other supporting packages are updated, a package may become depreciated. If the package still functions correctly in your current R version and dependencies, there’s no immediate reason to stop using it. However, if you are concerned that future versions of R or dependencies might break the packages you use, you can check the development and maintenance history of custom or specialized packages (or write a package or function for yourself as exemplified below!). Here is our Hargreaves function, adapted from the ‘Evapotranspiration’ package to minimize re-formatting of our dataframe. # # Function to calculate ET # calculate_ET &lt;- function(data, constants, ts = &quot;daily&quot;, message = &quot;yes&quot;, save.csv = &quot;no&quot;, ...) { # # # Check for required data # if (is.null(data$Tmax) | is.null(data$Tmin)) { # stop(&quot;Required data missing for &#39;Tmax&#39; and &#39;Tmin&#39;, or &#39;Temp&#39;&quot;) # } # # # Hargreaves-Samani ET Calculation # Ta &lt;- (data$Tmax + data$Tmin) / 2 # P &lt;- 101.3 * ((293 - 0.0065 * constants$Elev) / 293) ^ 5.26 # delta &lt;- 4098 * (0.6108 * exp((17.27 * Ta) / (Ta + 237.3))) / ((Ta + 237.3) ^ 2) # gamma &lt;- 0.00163 * P / constants$lambda # d_r2 &lt;- 1 + 0.033 * cos(2 * pi / 365 * data$J) # delta2 &lt;- 0.409 * sin(2 * pi / 365 * data$J - 1.39) # w_s &lt;- acos(-tan(constants$lat_rad) * tan(delta2)) # N &lt;- 24 / pi * w_s # R_a &lt;- (1440 / pi) * d_r2 * constants$Gsc * (w_s * sin(constants$lat_rad) * sin(delta2) + # cos(constants$lat_rad) * cos(delta2) * sin(w_s)) # C_HS &lt;- 0.00185 * (data$Tmax - data$Tmin) ^ 2 - 0.0433 * (data$Tmax - data$Tmin) + 0.4023 # ET_HS.Daily &lt;- 0.0135 * C_HS * R_a / constants$lambda * (data$Tmax - # data$Tmin) ^ 0.5 * (Ta + 17.8) # ET.Daily &lt;- ET_HS.Daily # # # Create YearMonth Column # data$YearMonth &lt;- as.Date(paste(year(data$Date.daily), month(data$Date.daily), &quot;01&quot;, sep = &quot;-&quot;)) # # # Annual and Monthly Aggregations # ET.Annual &lt;- aggregate(ET.Daily ~ year(YearMonth), data = data, FUN = sum) # ET.Monthly &lt;- aggregate(ET.Daily ~ YearMonth, data = data, FUN = sum) # # # ET formulating # ET_formulation &lt;- &quot;Hargreaves-Samani&quot; # ET_type &lt;- &quot;Reference Crop ET&quot; # results &lt;- list(ET.Daily = ET.Daily, ET.Monthly = ET.Monthly, # ET.Annual = ET.Annual, ET_formulation = ET_formulation, # ET_type = ET_type) # # # Save to CSV if required # if (save.csv == &quot;yes&quot;) { # for (i in 1:length(results)) { # namer &lt;- names(results[i]) # write.table(as.character(namer), file = &quot;ET_HargreavesSamani.csv&quot;, # dec = &quot;.&quot;, quote = FALSE, col.names = FALSE, # row.names = F, append = TRUE, sep = &quot;,&quot;) # write.table(data.frame(get(namer, results)), file = &quot;ET_HargreavesSamani.csv&quot;, # col.names = F, append = TRUE, sep = &quot;,&quot;) # } # invisible(results) # } else { # return(results) # } # } Now we will format the inputs so the data is easily read by the function, and run the function. # # Format our data to fit the function # PET_data &lt;- list( # Tmax = indata$Tmax, # Tmin = indata$Tmin, # J = as.numeric(format(indata$date, &quot;%j&quot;)), # Date.daily = indata$date # ) # # # Define constants # constants &lt;- list( # Elev = 2900, # Elevation in meters # lambda = 2.45, # Latent heat of vaporization in MJ.kg^-1 # lat_rad = 39.88 * pi / 180, # Latitude in radians # Gsc = 0.0820 # Solar constant in MJ.m^-2.min^-1 # ) # # PET_Hargreaves &lt;- calculate_ET( # data = PET_data, # constants = constants, # ts = &quot;daily&quot;, # Optional; defaults to &quot;daily&quot; # message = &quot;yes&quot;, # Optional; prints summary # save.csv = &quot;no&quot; # Optional; do not save results to a CSV # ) Now we’ll add daily ET into our original dataframe: # PET_mm &lt;- PET_Hargreaves$ET.Daily # # put the approximated PET_mm into the larger indata df # indata &lt;- cbind(indata, PET_mm) #with cbind # # head(indata) # # Annual summary stats # indata_analysis &lt;- indata %&gt;% # select(-date) %&gt;% # group_by(wtr_yr) %&gt;% # summarise( # weighted_precip.mm = sum(weighted_precip.mm, na.rm = TRUE), # Tmean_c = mean(Tmean_c, na.rm = TRUE), # swe.mm_max = max(weighted_swe.mm, na.rm = TRUE), # pcumul.mm_max = max(weighted_pcumul.mm, na.rm = TRUE), # Tmax = max(Tmax_c, na.rm = TRUE), # Tmin = min(Tmin_c, na.rm = TRUE), # Q_mm_sum = sum(Q_mm_day, na.rm = TRUE), # PET_mm_sum = sum(PET_mm, na.rm = TRUE), # runoff_input.mm = sum(runoff_input.mm, na.rm = TRUE), # ) # # indata_analysis # # Your script below should fit your variables from your summary dataframe above. Here is an example of what a simple water balance might look like if I named my summary dataframe indata_analysis: # # # Calculate the residual water after accounting for PET and discharge # residual_water &lt;- indata_analysis$runoff_input.mm - (indata_analysis$PET_mm_sum + indata_analysis$Q_mm_sum) # # # View the residual for each year # print(residual_water) By subtracting discharge and PET from runoff_input, we’re essentially examining the residual water. This could represent the amount of water available to the system after accounting for the demand (PET) and the outflow (discharge). # # make long form with PETsum and Psum as the key-value pairs (exclude wtr_yr from ) # dat_sum_plot &lt;- indata_analysis %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -wtr_yr) # # # bar plot pf PET and P for each year # ggplot(dat_sum_plot, aes(x = wtr_yr, y = value, fill = key)) + # geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + # labs(x = &quot;Water Year&quot;, y = &quot;mm/year&quot;, fill = {}) # ### TIMING # # create weekly means or totals for both time series and plot them together to determine the timing of each # dat_weekly &lt;- indata %&gt;% # group_by(Week = week(date)) %&gt;% # summarise( # PET = sum(PET_mm), # P = sum(runoff_input.mm) # ) %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Week) # # # line plot of P and PET on a weekly basis. Use dat_weekly as the data source. # ggplot(dat_weekly, aes(x = Week, y = value, color = key)) + # geom_line() + # create line plot # labs(x = &quot;Week of Year&quot;, y = &quot;mm/year&quot;, color = {}) 7.2.3.1 Single model run Look at the parameters to make sure you know what each parameter does. The way the model is set up, is that everything is hidden inside a function. The user (–&gt; you) only formats the input data and passes the parameters to the function. All model output is contained in “modelRun”. If you look at the Environment, you will notice that modelRun is a list. A list is even more flexible in terms of data storage than a dataframe (a dataframe is actually a special type of list…). While lists are super flexible, they can also be more cumbersome to deal with. I included some code that takes the output from the model run and saves all the important bits and pieces in a convenient dataframe called HBVRun. For this part (single model runs), you will really only need the data contained in the HBVRun dataframe. 7.2.3.2 single model execution # # set up the parameter vector # model_params &lt;- c( # 1.05, # SCF snow correction factor [-] (e.g., 0.9-1.5); # 1.80, # DDF degree day factor [mm/degC/timestep] (e.g., 0.0-5.0 mm/degC/day); # 2, # Tr threshold temperature above which precipitation is rain [degC] (e.g., 1.0-3.0 degC); # 0, # Ts threshold temperature below which precipitation is snow [degC] (e.g., -3.0-1.0 degC); # -0.336, # Tm threshold temperature above which melt starts [degC] (e.g., -2.0-2.0 degC); # 0.2, # LPrat parameter related to the limit for potential evaporation [-] (e.g., 0.0-1.0); # 121, # FC field capacity, i.e., max soil moisture storage [mm] (e.g., 0-600 mm); # 2.52, # BETA the non linear parameter for runoff production [-] (e.g., 0.0-20.0); # 0.473, # k0 storage coefficient for very fast response [timestep] (e.g., 0.0-2.0 days); # 9.06, # k1 storage coefficient for fast response [timestep] (e.g., 2.0-30.0 days); # 142, # k2 storage coefficient for slow response [timestep] (e.g., 30.0-250.0 days); # 50.1, # lsuz threshold storage state, i.e., the very fast response start if exceeded [mm] (e.g., 1.0-100.0 mm); # 2.38, # cperc constant percolation rate [mm/timestep] (e.g., 0.0-8.0 mm/day); # 10, # bmax maximum base at low flows [timestep] (e.g., 0.0-30.0 days); # 25 # croute free scaling parameter [timestep^2/mm] (e.g., 0.0-50.0 days^2/mm); # ) # # # # set time period # model_in &lt;- indata %&gt;% # filter(date &gt;= as_date(&quot;2017-10-01&quot;) &amp; date &lt;= as_date(&quot;2022-09-30&quot;)) # # # set up the model # ## THIS IS THE ACTUAL MODEL EXECUTION # modelRun &lt;- TUWmodel( # prec = model_in$weighted_precip.mm, # precip input # airt = model_in$Tmean_c, # air temp input # ep = model_in$PET_mm, # pet input # area = 1, # one zone for the entire watershed # param = model_params # input model parameters # ) # # # # get all outputs into a nice df # HBVRun &lt;- tibble( # Date = model_in$Date, # date # P_mm = modelRun$prec, # precip # Tair = modelRun$airt, # air temp # SWEobs = model_in$weighted_swe.mm, # observed swe # PET = modelRun$ep,# pet # Qobs = model_in$Q_mm_day, # observed discharge # Qsim = modelRun$q[1, ], # simulated discharge # Qsurf = modelRun$q0[1, ], # surface runoff # Qsubsurf = modelRun$q1[1, ], # subsurface flow # Qbase = modelRun$q2[1, ], # groundwater flow # Rain = modelRun$rain[1, ], # simulated rain # Snow = modelRun$snow[1, ], # simulated snowfall # Melt = modelRun$melt[1, ], # simulated melt # SWEsim = modelRun$swe[1, ], # simulated swe # Soilmoist = modelRun$moist[1, ], # simulated soil storage # AET = modelRun$eta[1, ], # simulated evapotranspiration # StorageUpper = modelRun$suz[1, ], # upper storage value # StorageLower = modelRun$slz[1, ] # lower storage value # ) 7.2.3.3 Objective functions 7.2.3.3.1 KGE Before we can start trying to tune our model to look more like the observed discharge record, it would be helpful to have some sort of quantified metric for how well our modeled data fits the measured data. There are many different ways to do this, but discussion of the pros and cons of those approaches is beyond this quick introduction to modeling. Here we will demonstrate the Kling-Gupta efficiency both for runoff as well as for swe. # # select the Qobs and Qsim timeseries # # DON&#39;T FORGET TO EXCLUDE THE FIRST YEAR # Qobs &lt;- HBVRun$Qobs[366:length(HBVRun$Qobs)] # observed runoff WITHOUT WARM-UP PERIOD # Qsim &lt;- HBVRun$Qsim[366:length(HBVRun$Qsim)] # simulated runoff WITHOUT WARM-UP PERIOD # # # KGE # kge_r_q &lt;- cor(Qobs, Qsim) # kge_beta_q &lt;- mean(Qsim) / mean(Qobs) # kge_gamma_q &lt;- (sd(Qsim) / mean(Qsim)) / (sd(Qobs) / mean(Qobs)) # kge_q &lt;- 1 - sqrt((kge_r_q - 1)^2 + (kge_beta_q - 1)^2 + (kge_gamma_q - 1)^2) # kge_q # ## Snow - water equivalent # SWEobs &lt;- HBVRun$SWEobs[366:length(HBVRun$SWEobs)] # observed swe WITHOUT WARM-UP PERIOD # SWEsim &lt;- HBVRun$SWEsim[366:length(HBVRun$SWEobs)] # simulated swe WITHOUT WARM-UP PERIOD # # # KGE # kge_r_swe &lt;- cor(SWEobs, SWEsim) # kge_beta_swe &lt;- mean(SWEsim) / mean(SWEobs) # kge_gamma_swe &lt;- (sd(SWEsim) / mean(SWEsim)) / (sd(SWEobs) / mean(SWEobs)) # kge_swe &lt;- 1 - sqrt((kge_r_swe - 1)^2 + (kge_beta_swe - 1)^2 + (kge_gamma_swe - 1)^2) # kge_swe 7.2.3.3.2 Nash-Sutcliffe Efficiency (NSE). Basically, the NSE looks at how much better your model run did that if you had just used the mean discharge for the data record as your “modelled results”. It does this by comparing how far off the observed values where from the mean discharge to how far off the modeled values were from the observed discharge. –&gt; Mathematically, NSE is the sum of the squared differences between the modeled and observed discharge divided by the sum of the squared differences between the observed and mean discharge, subtracted by 1. \\[ NSE = 1 - \\frac{\\sum_{t = 1}^{T}{(Q_m^t - Q_o^t)^2}}{\\sum_{t = 1}^{T}{(Q_o^t - \\bar{Q_o})^2}} \\] Where \\(Q_m^t\\) is modeled discharge at time t, \\(Q_o^t\\) is observed discharge at time t, and \\(\\bar{Q_o}\\) is mean observed discharge. # #Calculate NSE for snow, SWE is modeled, STA2 is measured # NSE_Q &lt;- 1 - ((sum((HBVRun$Qobs - HBVRun$Qsim) ^ 2)) / # sum((HBVRun$Qsim - mean(HBVRun$Qsim)) ^ 2)) # # NSE_Q # #Calculate NSE for snow, SWE is modeled, STA2 is measured # NSEsno &lt;- 1 - ((sum((HBVRun$SWEobs - HBVRun$SWEsim) ^ 2)) / # sum((HBVRun$SWEsim - mean(HBVRun$SWEsim)) ^ 2)) # # NSEsno 7.2.3.4 Calibrate HBV manually Woohoo! We can now run our model and assess how well it is working! Now, let’s see how well we can get it to work. The code below runs the model, produces a plot, and calculates the NSE based on discharge. # #when this term = 1, then triangular routing is invoked, or for no routing, routing = 0 # #if routing = 0 then MAXBAS doesn&#39;t do anything # routing &lt;- 0 # # #hard code parameters # params &lt;- c(40, #FCM ax soil moisture storage, field capacity # 1, #beta Shape coefficient governing fate of water input to soil moisture storage # 0.3, #LP Threshold for reduction of evap # 0.4, #SFCF Snowfall correction factor # -1.5, #TT Threshold temperature # 1, #CFMAX Degree-day factor # 0.05, #k0 Recession constant (upper storage, near surface) # 0.01, #k1 Recession constant (upper storage) # 0.001, #k2 Recession constant (lower storage) # 0, #UZL Threshold for shallow storage # 0, #PERC Percolation, max flow from upper to lower storage # 1 #MAXBAS base of the triangular routing function, days # ) # # #Run the model # Out &lt;- HBV(params, P, Temp, PET, routing) # # #Add observed output # Out &lt;- bind_cols(Out, Qobs1) # # #Trim out the warm up period # OutTrim &lt;- filter(Out, DATE &gt;= mdy(&quot;01-01-2011&quot;)) # # #Calculate NSE # NSE &lt;- 1 - ((sum((OutTrim$q - OutTrim$WS_3) ^ 2)) / # sum((OutTrim$WS_3 - mean(OutTrim$WS_3)) ^ 2)) # # #Create plot with NSE in title # OutTrim %&gt;% plot_ly(x = ~DATE) %&gt;% # add_trace(y = ~q, name = &#39;Modeled&#39;, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% # add_trace(y = ~WS_3, name = &#39;Measured&#39;, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% # layout(title=paste(&quot;NSE: &quot;, round(NSE,2))) 7.2.3.5 Plot observed and model simulations Generate plots that include and compare the different modeled fluxes from your best NSE. Some of those fluxes can be immediately compared to observed data (e.g., runoff or SWE), while others only exist in simulated form (e.g., storages or outflows of the various runoff components) and need to be assessed with the perceptual model in mind. Make sure that the axes are properly labeled when you create plots. The script below will get you started. # # Add date to the HBVRun result tibble # HBVRun &lt;- as.data.frame(HBVRun) # HBVRun$Date &lt;- model_in$date # str(HBVRun) # # # Overall runoff (Qsim and Qobs) # q_plot &lt;- ggplot(data = HBVRun) + # geom_line(aes(x = Date, y = Qobs, color = &quot;Qobs&quot;)) + # Qobs # geom_line(aes(x = Date, y = Qsim, color = &quot;Qsim&quot;)) + # Qsim # labs(x = {}, y = &quot;Q (mm/day)&quot;, color = {}, title = &quot;Qobs and Qsim&quot;) # ggplotly(q_plot) # Swe # swe_plot &lt;- ggplot(data = HBVRun) + # geom_line(aes(x = Date, y = SWEobs, color = &quot;SWEobs&quot;)) + # SWEobs # geom_line(aes(x = Date, y = SWEsim, color = &quot;SWEsim&quot;)) + # SWEsim # labs(x = {}, y = &quot;SWE (mm)&quot;, color = {}) # ggplotly(swe_plot) # q_bucket_plot &lt;- ggplot(data = HBVRun) + # geom_line(aes(x = Date, y = Qsurf, color = &quot;Qsurf&quot;)) + # Qsurf # geom_line(aes(x = Date, y = Qsubsurf, color = &quot;Qsubsurf&quot;)) + #Qsubsurf # geom_line(aes(x = Date, y = Qbase, color = &quot;Qbase&quot;)) + # Qbase # labs(x = {}, y = &quot;Q (mm)&quot;, color = {}, title = &quot;Q0, Q1, and Q2&quot;) # ggplotly(q_bucket_plot) # # PET and AET # pet_plot &lt;- ggplot(data = HBVRun) + # geom_line(aes(x = Date, y = PET, color = &quot;PET&quot;)) + # PET # geom_line(aes(x = Date, y = AET, color = &quot;AET&quot;)) + # AET # labs(x = {}, y = &quot;Flux (mm)&quot;, color = {}, title = &quot;PET and AET&quot;) # ggplotly(pet_plot) # # Storages # storage_plot &lt;- ggplot(data = HBVRun) + # geom_line(aes(x = Date, y = Soilmoist, color = &quot;Soil moisture storage&quot;)) + # soil moisture storage # geom_line(aes(x = Date, y = StorageUpper, color = &quot;Upper storage&quot;)) + # upper storage # geom_line(aes(x = Date, y = StorageLower, color = &quot;Lower storage&quot;)) + # lower storage # labs(x = {}, y = &quot;Storage (mm)&quot;, color = {}, title = &quot;Storages&quot;) # ggplotly(storage_plot) "]]
