[["index.html", "Quantitative Environmental Methods: LRES 546 Chapter 1 Part 1: Foundations of Environmental Modeling 1.1 Introduction 1.2 Course overview and objectives 1.3 Course Description 1.4 Structure 1.5 Philosophical approach and resources 1.6 Tentative schedule, subject to change", " Quantitative Environmental Methods: LRES 546 Tim Covino &amp; Lauren Kremer Chapter 1 Part 1: Foundations of Environmental Modeling 1.1 Introduction This book provides the materials that we will use in Quantitative Environmental Methods (LRES 546). In this class we will be learning the fundamentals of environmental data analysis and simulation in R. This class can be taken online or in-person. The online will be asynchronous. As a function of demand, in 2025, this class will be taught online. Instructor: Dr. Tim Covino For 2025 the class will be online, asynchronous Office hours: By appointment Email: timothy.covino@montana.edu TA: Lauren Kremer Email: lauren.kremer@montana.edu 1.2 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology/environmental science. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.3 Course Description This course will focus on development of quantitative analysis and modeling skills in watershed and environmental science. Students will develop skills necessary to perform quantitative analyses, describe and evaluate model structure, evaluate the merit of different models of varying type and complexity, and use quantitative analyses to address problems in environmental/watershed science. Students will apply computer programming in R to analyze and simulate watershed and/or environmental dynamics spanning simple to complex processes, analyses, and simulations. Technical skills and conceptual understanding will be built through lectures, readings, and hands-on quantitative projects. Note: If you aren’t familiar with R, or don’t have coding experience, don’t worry. We will walk you through all of the coding. I hope that by the end of this class each student will be a strong quantitative scientist with equally strong coding skills. 1.4 Structure This class will utilize hands-on/active learning, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class/work session. Programming is best learned by doing it often. Each week there will be recorded videos and/or readings, where we will talk about and work through various types of hydrological analyses. We will then put the content from the recorded lectures to work in a lab where students will complete a variety of hydrological analyses in R. Students can work through material on their schedule, but Dr. Covino and Lauren Kremer (TA) will be available during lab (Thursday 13:40 - 14:55) to help with technical coding problems or to answer other questions on weekly labs. 1.5 Philosophical approach and resources This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilize open source R and RStudio software. Books/resources, we may not use all of these, but they are good references: - R for Data Science - Statistical Methods in Water Resources - Tidy modeling with R - ggplot2: Elegant Graphics for Data Analysis - Advanced R - R Packages - Environmental Data Science Additional readings will be made available on this bookdown page as needed. 1.6 Tentative schedule, subject to change Part 1: Foundations of Environmental Modeling Week 1 1/14 - 1/17 - Introduction, overview, and technical skills. - If you need a refresher for R please see Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Part 2: Tabular Models and Time-Series Analysis Week 2 1/21 - 1/24 - Hydrograph separation. Week 3 1/27 - 1/31 - Frequency analysis Week 4 2/3 - 2/7 - Rational and curve number (CN) methods Week 5 2/10 - 2/14 - Monte Carlo and sensitivity analysis Part 3: Bucket Models (Process-Based Models) Week 6 2/17 - 2/21 - Classifying Model Structure - Snowmelt models Week 7 2/24 - 2/28 - Evapotranspiration Week 8 3/3 - 3/7 - Term assignment 1 Week 9 3/10 - 3/14 - Term assignment 2 Week 10 Spring Break 3/17 - 3/21 Week 11 3/24 - 3/28 lumped to semi-distributed modeling - HBV Model - OR Precipitation interpolation Part 4: Spatial Models and GIS Integration Week 12 3/31 - 4/4 - Gridded and tabular data retrieval using API integration packages Week 13 4/7 - 4/11 - DEM processing and analysis in Whitebox Week 14 4/14 - 4/18 - Transfer Functions Part 5: Application Week 15 4/21 - 4/25 - Term project work Week 16 4/28 - 5/2 - Final presentations due May 2 Week 17 5/5 - 5/7 - Finals week - Share final presentations "],["fundamentals-of-hydrological-analysis---physical-hydrology-and-empirical-methods.html", "Chapter 2 Fundamentals of Hydrological Analysis - Physical Hydrology and Empirical Methods 2.1 Hydrograph separation 2.2 Return Intervals 2.3 Rational method and NRCS curve number (20 pts)", " Chapter 2 Fundamentals of Hydrological Analysis - Physical Hydrology and Empirical Methods 2.1 Hydrograph separation This module was developed by Ana Bergrstrom and Matt Ross. 2.1.1 Learning Module 1 2.1.1.1 Summary Streamflow can come from a range of water sources. When it is not raining, streams are fed by the slow drainage of groundwater. When a rainstorm occurs, streamflow increases and water enters the stream more quickly. The rate at which water reaches the stream and the partitioning between groundwater and faster flow pathways is variable across watersheds. It is important to understand how water is partitioned between fast and slow pathways (baseflow and stormflow) and what controls this partitioning in order to better predict susceptibility to flooding and if and how groundwater can sustain streamflow in long periods of drought. In this module we will introduce the components of streamflow during a rain event, and how event water moves through a hillslope to reach a stream. We will discuss methods for partitioning a hydrograph between baseflow (groundwater) and storm flow (event water). Finally, we will explore how characteristics of a watershed might lead to more or less water being partitioned into baseflow vs. stormflow. We will test understanding through evaluating data collected from watersheds in West Virginia to determine how mountaintop mining, which fundamentally changed the watershed structure, affects baseflow. 2.1.1.2 Reading for this lab Ladson, A. R., R. Brown, B. Neal and R. Nathan (2013) A standard approach to baseflow separation using the Lyne and Hollick filter. Australian Journal of Water Resources 17(1): 173-18 Ladson et al., 2013 Lynne, V., Hollick, M. (1979) Stochastic time-variable rainfall-runoff modelling. In: pp. 89-93 Institute of Engineers Australia National Conference. Perth. Lyne and Hollick, 1979 2.1.1.3 Overall Learning Objectives At the end of this module, students should be able to describe the components of streamflow, the basics of how water moves through a hillslope, and the watershed characteristics that affect partitioning between baseflow and stormflow. 2.1.1.4 Lecture 2.1.1.5 Components of streamflow during a rain event During a rainstorm, precipitation is falling across the watershed: close to the stream, on a hillslope, and up at the watershed divide. This water that falls across the watershed flows downslope toward the stream via a number of flow pathways. Here we define and describe the basic flow pathways during a rain event. The first component is channel interception. This is water that falls directly on the water surface of the stream. The amount of water that falls directly on the channel is a function of stream size, if we have a very small, narrow creek, this will be a very small quantity. However, you can imagine that in a very large, broad river such as the Amazon, this volume of water is much larger. Channel interception is the first component during a rain event that causes streamflow to increase because it is contributing directly to the stream and therefore has no travel time. The second is overland flow, which is water that flows on the land surface to the stream. Overland flow can occur via a number of mechanisms which we will not explore too deeply here, but encourage further study on your own (resources provided). Briefly, overland flow includes water that falls on an impermeable surface such as pavement, water that runs downslope due to rain falling faster than the rate at which it can infiltrate the ground surface, and water that flows over the land surface because the ground is completely saturated. Overland flow is typically faster than water that travels through soils and deeper flow pathways and therefore is the next major component that starts to contribute to the increase in streamflow during a rain event. The third component is subsurface flow. This is water that infiltrates the land surface and flows downslope through shallow groundwater flow pathways. This is the last component that increases streamflow during a storm event, is the slowest of the stormflow components, and can contribute to elevated streamflow for a while after precipitation ends. The final component is baseflow. Baseflow can also be described as groundwater. This component is what sustains streamflow between rain events, but also continues to contribute during a rain event. Of water that infiltrates the ground surface, some moves quickly to the stream as subsurface flow, but some moves deeper and becomes part of deeper groundwater and baseflow. Thus baseflow can increase in times of higher wetness in the watershed, particularly during and right after rainy seasons or spring snowmelt. We can simplify this partitioning into baseflow and stormflow (often called quickflow). Baseflow being groundwater that moves more slowly and sustains streamflow between rain events. Stormflow is water that contributes to streamflow as a result of a rain event. Under this definition we can lump channel interception, overland flow, and subsurface flow into stormflow. 2.1.1.6 Storm flow through a hillslope When rain falls on the land surface, much of it infiltrates into the soil. Water moves through the soil column until it meets a layer of lower permeability and runs down the hillslope as subsurface flow. This layer of lower permeability allows some water to move through it, contributing to groundwater. Frequently the layer of lower permeability is the interface between soil and rock. Therefore the depth of soil has a large effect on how much water moves through soil vs. how much moves deeper into groundwater, becoming baseflow. 2.1.1.7 How we quantify baseflow It is impossible to know the amount of water moving as overland, subsurface, and base flow in all parts of a watershed. So in order to quantify how much water in a stream at any given time is storm flow vs. baseflow, we need to use some simplified methods (i.e., modeling). These frequently involve using the hydrograph (plot of streamflow over time) drawing lines, and calculating the volume of water above and below the line. This can be somewhat arbitrary and there are a variety of methods for delineating the cutoff between baseflow and stormflow. Despite what method you use and how simplified it is, this technique still provides valuable information and allows us to make comparisons across watersheds in order to understand how they function and what their structural properties are. 2.1.1.8 Baseflow separation methods One of the most basic methods for calculating base flow vs. storm flow is the straight line method. First, find the value of discharge at the point that streamflow begins to rise due to a storm. A straight line is drawn at that value until it intersects with the hydrograph (i.e. streamflow recedes back to the discharge it was at before the rainfall event started. Anything below this line is base flow and anything above it is storm flow. We learned above that some rainfall can move deep into the soil profile and contribute to baseflow. We might expect baseflow to increase over time and thus would want to use a method that can account for this. An addition to the straight line method was posed by Hewlett and Hibbert, 1967. This method, which we’ll call the Hewlett and Hibbert method finds the discharge at the starting point of a storm. Then, rather than a straight line of 0 slope and in the straight line method, we assume a slope of 0.05 cubic feet per second per square mile. The line with this calculated slope is drawn until it intersects with the hydrograph receding at the end of the storm. There are myriad other methods for baseflow separation of a wide range of complexity. We will give an example of one more method: a recursive filter method established by Lyne and Hollick (1976). This method smooths the hydrograph and partitions part of that smoothed signal into baseflow. The equation for this method is: You can see from this equation that a filter parameter, a, must be chosen. This parameter can be decided by the user, takes a value between 0 and 1, and is typically close to 1. Additionally this filtering method must be constrained so that baseflow is not negative or greater than the total streamflow. Output from this method for the Harvey River in Australia is originally published in Lyne and Hollick (1976) below (notice in the caption that the a parameter was set to 0.8): 2.1.1.9 Watershed controls on baseflow and stormflow The way a watershed is structured has a strong control on how water is partitioned into baseflow and stormflow. Below is a list of key structural properties: Land Use and Land Cover: If a watershed is developed or natural can dictate how much water infiltrates the land surface and how quickly. For example, a watershed with lots of pavement means that much more water will be overland flow with fewer opportunities to recharge baseflow. Furthermore, how a watershed is developed will affect partitioning. For example a residential area with houses on large, grassy lots will allow for more infiltration than a shopping center with large parking lots. Land cover in natural areas will also affect partitioning. Some other variables to consider may be: Land cover in natural areas: a dense forest vs. a recently harvested hillside. Soil type: clayey soils vs. sandy soils Depth to impeding layer: could be the bedrock interface, but could also be a low permeability clay layer in the soil Permeability of the underlying rock: Highly fractured sandstone vs. solid granite Slope: steeper slopes vs. flatter areas The partitioning is a combination of all of these factors. A watershed may have a very low slope, suggesting that it might have less stormflow. But if the soils in this watershed have an impermeable clay layer near the soil surface, a lot more water may end up as stormflow than one would expect. 2.1.2 Labwork (20 pts) In this lab we will analyze stream flow (Q) and precipitation (P) data from Tenderfoot Creek Experimental Forest (TCEF). TCEF is located in central Montana, north of White Sulphur Springs. See here for information about TCEF. You will do some data analysis on flows, calculate annual runoff ratios, and perform a hydrograph separation. 2.1.3 Repo link Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd” or “_partial.Rmd”. Always be sure to read the README.md files in the GitHub repo. Sometimes they are useful, sometimes they aren’t, but always have a look. As I mentioned above you will work through the “_blank.Rmd” or “_partial.Rmd”. If you want to learn how to code in R, I encourage you to work through the blank version as much as possible. Also, if you don’t have much R background this lab might seem kind of challenging. But don’t worry. I’m challenging you right now, but it will get easier, and we can video chat if a live demonstration is needed. So don’t get frustrated if this seems tough right now. Soon you will be rattling off code with ease. Conversely, if you are an experienced coder and have ideas for how to do this in ways other than what I’ve shown here, please share code with your colleagues and help them develop their coding skills! Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. See here for an overview of projects and why you should use them from Jenny Bryan. 2.2 Return Intervals In this learning module we will focus on return intervals. We first need to understand return intervals before we can move onto the rational method and curve numbers. The repo for this module can be found here 2.2.1 Learning Module 2 2.2.1.1 Background information Lecture from colleague Joel Sholtes on precipitation frequency analysis. Short lecture on Intensity-Duration-Frequency (IDF) curves Reading on frequency analysis of flow (e.g., floods). You should notice that the frequency analysis is the same whether we apply it to Q (flow) or P (precipitation). So as long as you understand the fundamental principles you will be able to do frequency analysis on either Q or P. USGS reading on flow frequency analysis There are also probability lecture slides on D2L. Titled “probability.pptx”. 2.2.2 Labwork (20 pts) In this lab we will look at some precipitation data to get an idea of return intervals for a given rain event. A return interval is the inverse of the probability. So if a certain rain even has a 10% probability of happening any year it has a 1/p return interval, so: R = 1/0.1 = 10 years. This means on average you can expect that size event about every ten years. From a probability perspective it is actually more correct to state that there is a 10% chance of that size rain event in any year. The reason this is better is that it communicates that you certainly can have a 10% probability event occur in back-to-back years. After computing some return intervals we will then use some of the simpler rainfall-runoff modeling approaches (the rational method and the curve number method) to simulate runoff for a hypothetical basin in our next unit. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 2.2.2.1 Packages We have a few new packages today. Those include rnoaa and leaflet. rnooa is a package used to download NOAA climate data. leaflet is a package for interactive mapping. Recall that if you have not installed these packages on your computer you will need to run: install.packages(“rnoaa”) and so on for the others. 2.2.2.2 Precipitation return intervals First, let’s start by getting some precipitation (P) data. rnoaa has a function called ghcnd_stations(). This function will download the site information for all stations in the GHCND network. GHCND network - Link Ok, so we want station USC00241044, which runs from 1892 to now. We use the meteo_pull_monitors() function to do that. Now we have some climate data. Take a minute to look at the climate_data df. First, just looking at the df we see that the data don’t actually start until 1894. It is also always a good idea to just plot some data. Below plot prcp, snow, tmax and tmin. You can just make 4 different plots. This is just for visual inspection. This part of the process is called exploratory data analysis (EDA). This should always be the first step when downloading data whether you download the data from the internet or from a sensor in the field. You might have noticed that the values seem to be much too large! Did you notice that? This is a great skill to develop. Have a look at the data and ask “are these numbers reasonable?”. In this case, the answer would be no! One thing to note is that NOAA data comes in tenths of degrees for temp and tenths of millimeters for precip. Type ?meteo_pull_monitors into the console and the help screen will tell you that. So, we need to clean up the df a bit. Let’s do that here. Now that we’ve converted units, it is a good idea to plot your data again for some EDA. Make plots of each of the variables (prcp, snow, tmax, and tmin) over time to inspect. How do the data look? Do they make sense? Do you see any strange values? There is a large snow event in 1951. We can assume that is “real”, so let’s keep it in the analysis. But you should think through how you could exclude it from the analysis. How could you use the filter function to do that? Next, we want to use some skills from the hydrograph sep lab to add a water year (WY) column. Try that here. I like to rearrange the order of columns. Using: Now, create a new df called climate_an where you calculate the total P (i.e., the sum) for each water year. Use group_by and summarize (or better yet, reframe). Also keep in mind that you will need to deal with NA values in the df. How do you do that in summarize? As a note, reframe can be used instead of summarize and is a more general purpose function. You can try each. What happens if you don’t deal with NA values by using something like na.rm = TRUE? Now, plot total anual P on the Y and water year on the x. What do you see? Now let’s calculate some probabilities. Look up the pnorm() function for this (either type it into the Help window, or type ?pnorm in the console. You only need x, the mean, and standard deviation (sd) for the calculations. Q1. (3 pts) What is the probability that the annual precipitation in a given year is less than 400 mm? This is the F(A) in the CDF in the probability lecture slides. Q1 ANSWER: Q2. (3 pts) What is the probability that the maximum annual precipitation in a given year is GREATER than 500 mm? Q2 ANSWER: Q3. (3 pts) What is the probability that the annual P is between 400 and 500 mm? Q4. (3 pts) What is the return period for a year with AT LEAST 550 mm of precip? The return period, Tr, is calculated as Tr = 1/p, with p being the probability for an event to occur. Q5. (8 pts) Explain why probability analysis of climate data assumes the data are normally distributed and stationary? Below provide a histogram and a density plot of the total annual P data and comment on the visual appearance in terms of normality. Next use google and the links below to test for normality and stationarity. Be quantitative in commenting on the normality and stationarity of the total P data. here here here 2.3 Rational method and NRCS curve number (20 pts) The repo for this module can be found here 2.3.1 Learning Module 3 2.3.1.1 Background information - Rational Method The Rational Method is a type of simple hydrological analysis used to estimate the peak runoff rate from a small watershed during a rainfall event. It is particularly useful for estimating the amount of water that will flow through a particular area during a storm, like a drainage system or culvert. Despite the existence of more advanced methods, this approach remains widely used in practice for its simplicity and ability to provide quick approximations, making it invaluable for preliminary assessments and as a foundation for understanding more complex hydrological and environmental modeling techniques. Here is a 5-minute video to get started: 2.3.1.2 Reading - Rational Method. Read at least sections 2 and 3 to for the formula and an example The Rational Method Some helpful terminology: runoff coefficient - represents how much rainfall actually becomes runoff time of concentration - the time it takes for some mass of precipitation to travel from the most remote point in a watershed to the outlet or point of interest. e.g., how long it takes a drop of rain to reach a culvert after it falls to the ground. 2.3.1.3 Background information - Curve Number The NRCS (Natural Resources Conservation Science) curve number (CN) is a tool used to estimate the total runoff volume of water that will run off an ungaged watershed during a storm event. The curve number is based on soil type, land use and antecedent moisture conditions. You may also see SCS CN in texts. NRCS was previously known as Soil Conservation Science, they are the same. It was designed as a simple tool to describe typical watershed response from infrequent rainfall anywhere in the US for watersheds with the same soil type, land use, and surface runoff conditions. The CN method is a single event model to estimate of runoff volume from rainfall events (not peak discharge or a hydrograph). To understand the function and derivation of the CN number, let’s start with the the NRCS runoff equation: \\[ Q = \\frac{{(P - I_{a})^2}}{{P - I_{a} + S}} \\] Where Q = runoff(in) P = rainfall (in) S = potential maximum retention after runoff begins Ia = initial abstraction (initial amount of rainfall that is intercepted by the watershed surface and does not immediately contribute to runoff) Ia is assumed to reduce to 0.2S based on empirical observations by NRCS. If: \\[ S = \\frac{{1000}}{{CN}} - 10 \\] the runoff equation therefore reduces to: \\[ Q = \\frac{{[P - 0.02\\left(\\frac{{1000}}{{CN}} - 10\\right)]^2}}{{P + 0.8\\left(\\frac{{1000}}{{CN}} - 10\\right)}} \\] 2.3.1.4 Reading - Curve numbers Curve Number selection tables are available from the US Army Corps. Slides on selecting curve number start around slide 8. - Link 2.3.1.5 Reading - Supporting material Time of concentration. Up to “other considerations”, pages 15-1 to 15-9. - Link We will use the Kirpich method to calculate the time of concentration. Here is the citation for your reference. Kirpich, Z.P. (1940). “Time of concentration of small agricultural watersheds”. Civil Engineering. 10 (6): 362. 2.3.2 Labwork (20 pts) In this module, you will apply the Rational Method and the SCS curve number (CN) method to estimate peak flows and effective rainfall/runoff volumes. This lab also introduces two coding techniques; for-loops and functions. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) Packages 2.3.2.1 Part I - Rational Method The goal is to calculate peak runoff (cfs) for a small 280-acre rangeland watershed near Bozeman for multiple events with different return periods. You will first have to calculate the time of concentration and then look up the the rainfall values for the different return intervals. The longest flowpath in the watershed is 6300 ft long, average watershed slope is 1.95%. Look at this table to select C. 2.3.2.1.1 Time of Concentration Time of concentration is the time it takes water to travel along the longest flowpath in the watershed and exit the watershed. tc in this example should be ~ 29.91. If your calculated value is very different, start by checking slope value (S). For the sake of simplicity in next steps, do not change the name of variables. 2.3.2.1.2 Storm Depths Now that you have the time of concentration, you need to find the corresponding 1, 2, 5, 10, 25, 50, and 100 year storm depths for a duration that works for the Rational Method in that particular watershed (that is, the duration that is closest to tc). Create a dataframe (or ‘tibble’ which is the tidyverse data frame) called “storms” that has a column for the return period, Tr, the storm depth, Pin, and the average storm intensity over ONE HOUR, Pin_hr. Typically hourly depths may be determined from a rainfall analysis. For the sake of the assignment, approximate daily depths corresponding to appropriate frequencies are provided here for Bozeman, MT. To obtain 1 hour depths by dividing daily depth by 24. 2.3.2.1.3 Example for-loop Now that we have the rainfall intensities, we need to set up a way that calculates Qp for each of those intensities, without us having to go back and manually enter them each time. We do this with a for-loop. Let’s first look at how for loops work in R. A for-loop will execute the code inside it for a specified number of iterations. The structure looks like: for(i in some number of iterations) { output &lt;- some code/function that needs to be executed } In the chunk below, we create the vector x, which contains 6 values (0, 2, 4, 6, 8, 10). Each loop then executes a calculation using each value in ‘x’ in sequence. In a for-loop, ‘i’ is called the loop index or iterator. It has a new value in each iteration of the loop. You can think of i as a ‘counter’ that helps the loop keep track of its progress. In the example below, we want to run the code in the loop for every value in the vector ‘x’. Since there are 6 values in vector ‘x’, then we will tell the loop to run 6 times. However, rather than for (i in 1:6), using 1:length(x) ensures the loop adapts to the size of x, no matter how many values it contains. This improves the flexibility of our code. We also need to store the output of loop during each iteration. If we don’t store the results of each iteration or loop, the loop will overwrite the output each time, leaving us with only the result from the final iteration. To store the output of each loop iteration, we have to preallocate a vector (like y) to store all of the outputs. In other words, we are creating an empty vector with the same length as ‘x’ for the for-loop to add values to. We have created a vector x from 0 to 10 in increments of 2. The for-loop takes each element of that vector and squares it. The “i” is called an index and runs from 1 through 6 (the length of x). During the first iteration i is 1, during the second iteration i is 2, and so on. We are then writing the results from the calculation into a new vector, y. When i is 1, we are squaring the first value in vector x, which is 0. The first value in y will be zero as well. When i is 2, the second value in x gets squared, which is 2^2. The second value in y is going to be 4. 2.3.2.1.4 Calculate Qp with for-loop Now let’s set up the for-loop for the Rational Method. We need to set up a for-loop that does the same calculation 7 times, the number of precip values in “storms”. However, ‘length()’ for a dataframe returns the number of columns, so you want to use ‘nrow()’ when referencing a dataframe. (As apposed to using length() for a vector as above). The calculated peak discharges should go into a new column of our “storms” df. However, indexing is slow for dataframes, so we will write the new values into a new vector, Qp, and then after the loop insert the vector as a column into the dataframe. If you are using the complete version of the assignment, do not assume that this C value is correct 2.3.2.1.5 Plot Tr and Qp Now plot the return interval against the storm peakflow. You only need three to four lines for this: 1st sets up the data, 2nd defines the theme that removes the gray background and sets the axes labels to a proper size, 3rd plots the data with geom_points, 4th makes the axes labels with labs. Note that axis labels have units. This is an important feature when communicating your findings. 1. (1 pt) What does the C in the Rational Method do? ANSWER: 2. (2 pt) What is the the time of concentration and why does it need to be taken into account for the Rational Method? What is a common issue among many tc methods? ANSWER: 2.3.2.2 Part II - NRCS CN In this exercise we will write a function that takes the necessary inputs for the NRCS CN method and returns a value based on the parameters. This is not fully automated, you will still have to look up the CN yourself for a given land use. The goal is to write a function that requires P, CN, and AMC (antecedent moisture condition) as an input in order to calculate Q. 2.3.2.2.1 AMC Table 2.3.2.3 Function example Let’s look at a simple example for a function. Assume we want a number with an exponent and we want to be able to choose both the base and the exponent. The function ‘function()’ defines the inputs in (), the actual calculation then follows in {}. Run the above function. You will notice that a function was added to the Global Environment all the way at the bottom. Now let’s test the function with a base of 2 and an exponent of 3. This should perform the calculation 2x2x2 = 8 2.3.2.3.1 NRCS CN test Before we write a function to estimate Q using CN, let’s make sure we can set up the correct steps WITHOUT the function. This is almost always a helpful step in developing functions. It can save hours of headaches later. Let’s try P = 2.5 inches, CN = 90, and AMC = 3. The biggest problem here is going to be creating a lookup table for the AMC. I will provide the basic structure of the code for this. Sanity check: Q should ~ 2.0 for the specified parameters. 3. (4 pt) What is one of the critical assumptions for the SCS CN method? (1-2 sentences). Why is it important to understand assumptions of models like this one? ANSWER: Now write a function called “scs_cn” that takes the inputs P, CN, and AMC (all numeric) and returns Q, Ia, Si, and the RR as a df called “scs_out”. Test the function for P = 2.5 in, CN = 90, AMC = 3. 4. (4 pts) Describe what factors into the curve number (and most notably what is missing). What does it mean when the CN is 100 or 0? ANSWER: 5. (3 pts) What is the difference between a deterministic and a stochastic model? Describe one example each. ANSWER: 6. (6 pts)(4-6 sentences) Let’s consider the cross-disciplinary relevance of hydrological modeling. You may not be studying hydrology directly, but environmental resource managers often need to estimate peak flows. Why? (1-2 sentences). In a sentence, what is your specific area of interest, or the topic of your professional paper, if known. Consider broader implications of the hydrologic cycle, like water quality, nutrient or pollution transport, or erosion. Does rainfall/runoff management affect your niche? If so, how? (2-4 sentences) ANSWER: "],["process-based-modeling---probabilistic-and-process-simulation.html", "Chapter 3 Process-Based Modeling - Probabilistic and Process Simulation 3.1 Transfer function rainfall-runoff models 3.2 Monte Carlo Simulation", " Chapter 3 Process-Based Modeling - Probabilistic and Process Simulation * Modules 3.1 and 3.2 are adapted from Fabian Nipggen (REWM.4500.500) 3.1 Transfer function rainfall-runoff models 3.1.1 Learning Module 4 3.1.1.1 Summary In previous modules, we explored how watershed characteristics influence the flow of input water through or over hillslopes to quickly contribute to stormflow or to be stored for later contribution to baseflow. Therefore, the partitioning of flow into baseflow or stormflow can be determined by the time it spends in the watershed. Furthermore, the residence time of water in various pathways may affect weathering and solute transport within watersheds. To improve our understanding of water movement within a watershed, it can be crucial to incorporate water transit time into hydrological models. This consideration allows for a more realistic representation of how water moves through various storage compartments, such as soil, groundwater, and surface water, accounting for the time it takes for water to traverse these pathways. In this module, we will model the temporal aspects of runoff response to input using a transfer function. First, please read: TRANSEP - a combined tracer and runoff transfer function hydrograph separation model Then this chapter will step through key concepts in the paper to facilitate hands-on exploration of the rainfall-runoff portion of the TRANSEP model in the assessment. Then, we will introduce examples of other transfer functions to demonstrate alternative ways of representing time-induced patterns in hydrological modeling, prompting you to consider response patterns in your study systems. 3.1.1.2 Overall Learning Objectives At the end of this module, students should be able to describe several ways to model and identify transit time within hydrological models. They should have a general understanding of how water transit time may influence the timing and composition of runoff. 3.1.1.3 Terminology In modeling a flow system, note that consideration of time may vary depending on the questions being asked. Transit time is the average time required for water to travel through the entire flow system, from input (e.g., rainfall on soil surface) to output (e.g., discharge). Residence time is a portion of transit time, describing the amount of time water spends within a specific component of the flow system, like storage (e.g., in soil, groundwater, or a lake). Figure 6.3. Conceptual diagram of the lumped parameter transit time modeling approach (McGuire &amp; McDonnell, 2006) A transfer function (TF) is a mathematical representation of how a system responds to input signals. In a hydrological context, it describes the transformation of inputs (e.g. precipitation) to outputs (e.g. runoff). These models can be valuable tools for understanding the time-varying dynamics of a hydrological system. 3.1.1.4 The Linear Time-Invariant TF We’ll begin the discussion in the context of a linear reservoir. Linear reservoirs are simple models designed to simulate the storage and discharge of water in a catchment. These models assume that the catchment can be represented as single storage compartments or as a series of interconnected storage compartments and that the change the amount of water stored in the reservoir (or reservoirs) is directly proportional to the inflows and outflows. In other words, the linear relationship between inflows and outflows means that the rate of water release is proportional to the amount of water stored in the reservoir. 3.1.1.4.1 The Instantaneous Unit Hydrograph: The Instantaneous Unit Hydrograph (IUH) represents the linear rainfall-runoff model used in the TRANSEP model. It is an approach to hydrograph separation that is useful for analyzing the temporal distribution of runoff in response to a ‘unit’ pulse of rainfall (e.g. uniform one-inch depth over a unit area represented by a unit hydrograph). In other words, it is a hydrograph that results from one unit (e.g. 1 mm) of effective rainfall uniformly distributed over the watershed and occurring in a short duration. Therefore, the following assumptions are made when the IUH is used as a transfer function: 1. the IUH reflects the ensemble of watershed characteristics 2. the shape characteristics of the unit hydrograph are independent of time 3. the output response is linearly proportional to the input By interpreting the IUH as a transfer function, we can model how the watershed translates rainfall into runoff. In the TRANSEP model, this transfer function is represented as \\(g(\\tau)\\) and thus the rainfall-induced response to runoff. \\[ g(\\tau) = \\frac{\\tau^{\\alpha-1}}{\\mathrm{B}^{\\alpha}\\Gamma(\\alpha)}exp(-\\frac{\\tau}{\\alpha}) \\] The linear portion of the TRANSEP model describes a convolution of the effective precipitation and a runoff transfer function. \\[ Q(t)= \\int_{0}^{t} g(\\tau)p_{\\text{eff}}(t-\\tau)d\\tau \\] Whoa, wait…what? Tau, integrals, and convolution? Don’t worry about the details of the equations. Check out this video to have convolution described using dollars and cookies, then imagine each dollar as a rainfall unit and each cookie as a runoff unit. Review the equations again after the video. knitr::include_url(&quot;https://www.youtube.com/embed/aEGboJxmq-w&quot;) 3.1.1.4.2 The Loss Function: The loss function represents the linear rainfall-runoff model used in the TRANSEP model. \\[ s(t) = b_{1} p(t + 1 - b_{2}^{-1}) s(t - \\triangle t) \\] \\[ s(t = 0) = b_{3} \\] \\[ p_{\\text{eff}}(t) = p(t) s(t) \\] where \\(p_{\\text{eff}}(t)\\) is the effective precipitation. \\(s(t)\\) is the antecedent precipitation index which is determined by giving more importance to recent precipitation and gradually reducing that importance as we go back in time. The rate at which this importance decreases is controlled by the parameter \\(b_{2}\\). The parameter \\(b_{3}\\) sets the initial antecedent precipitation index at the beginning of the simulated time series. In other words, these equations are used to simulate the flow of water in a hydrological system over time. The first equation represents the change in stored water at each time step, taking into account precipitation, loss to runoff, and the system’s past state. The second equation sets the initial condition for the storage at the beginning of the simulation. The third equation calculates the effective precipitation, considering both precipitation and the current storage state. 3.1.1.4.3 How do we code this? We will use a skeletal version of TRANSEP, focusing only on the rainfall-runoff piece which includes the loss-function and the gamma transfer function. We will use rainfall and runoff data from TCEF to model annual streamflow at a daily time step. Then we can use this model as a jump-off point to start talking about model calibration and validation in future modules. 3.1.1.4.4 Final thoughts: If during your modeling experience, you find yourself wading through a bog of complex physics and multiple layers of transfer functions to account for every drop of input into a system, it is time to revisit your objectives. Remember that a model is always ‘wrong’. Like a map, it provides a simplified representation of reality. It may not be entirely accurate, but it serves a valuable purpose. Models help us understand complex systems, make predictions, and gain insights even if they are not an exact replica of the real world. Check out this paper for more: https://agupubs.onlinelibrary.wiley.com/doi/10.1029/93WR00877 3.1.2 Repo link Download the repo for this lab HERE 3.1.3 Labwork (20 pts) In this homework/lab, you will write a simple, lumped rainfall-runoff model. The foundation for this model is the TRANSEP model from Weiler et al., 2003. Since TRANSEP (tracer transfer function hydrograph separation model) contains a tracer module that we don’t need, we will only use the loss function (Jakeman and Hornberger) and the gamma transfer function for the water routing. The data for the model is from the Tenderfoot Creek Experimental Forest in central Montana. Load packages with a function that checks and installs packages if needed: Define input year We could use every year in the time series, but for starters, we’ll use 2006. Use filter() to extract the 2006 water year. 1) QUESTION: What does the function cumsum() do?(1 pt) ANSWER: Define the initial inputs This chunk defines the initial inputs for measured precip and measured runoff. Parameterization We will use these parameters. You can change them if you want, but I’d suggest leaving them like this at least until you get the model to work. Even tiny changes can have a huge effect on the simulated runoff. Loss function This is the module for the Jakeman and Hornberger loss function where we turn our measured input precip into effective precipitation (p_eff). This part contains three steps. 1) preallocate a vector p_eff: Initiate an empty vector for effective precipitation that we will fill in with a loop using Peff(t) = p(t)s(t). Effective precipitation is the portion of precipitation that generates streamflow and event water contribution to the stream. It is separated to produce event water and displace pre-event water into the stream. 2) set the initial value for s: s(t) is an antecedent precipitation index. How much does antecedent precipitation affect effective precipitation? 3) generate p_eff inside of a for-loop Please note: The Weiler et al. (2003) paper states that one of the loss function parameters (vol_c) can be determined from the measured input. That is actually not the case. s(t) is the antecedent precipitation index that is calculated by exponentially weighting the precipitation backward in time according to the parameter b2 is a ‘dial’ that places weight on past precipitation events. 2) QUESTION (3 pts): Interpret the two figures. What is the meaning of “frac”? For this answer, think about what the effective precipitation represents. When is frac “high”, when is “frac” low? ANSWER: Extra Credit (2 pt): Combine the two plots into one, with a meaningfully scaled secondary y-axis Runoff transfer function Routing module - This short chunk sets up the TF used for the water routing. This part contains only two steps: 1) the calculation of the actual TF. 2) normalization of the TF so that the sum of the TF equals 1. tau(0) is the mean residence time 3) QUESTION (2 pt): Why is it important to normalize the transfer function? ANSWER: 4) QUESTION (4 pt): Describe the transfer function. What are the units and what does the shape of the transfer function mean for runoff generation? ANSWER: Convolution This is the heart of the model. Here, we convolute the input with the TF to generate runoff. There is another thing you need to pay attention to: We are only interested in one year (365 days), but since our TF itself is 365 timesteps long, small parts of all inputs except for the first one, will be turned into streamflow AFTER the water year of interest, that is, in the following year. In practice, this means you have two options to handle this. 1) You calculate q_all and then manually cut the matrix/vector to the correct length at the end, or 2) You only populate a vector at each time step and put the generated runoff per iteration in the correct locations within the vector. For this to work, you would need to trim the length of the generated runoff by one during each iteration. This approach is more difficult to code, but saves a lot of memory since you are only calculating/storing one vector of length 365 (or 366 during a leapyear). We will go with option 1). The code for option 2 is shown at the end for reference. Convolution summarized (recall dollars and cookies): Each loop iteration results in a row of the matrix representing the convolution at a specific time step. each time step of p_eff is an iteration, and for each timestep, it multiplies the effective precipitation at that timestep by the entire transfer function. Then each row is summed and stored in the vector q_all. q_all_loop is an intermediate step in the convolution process and can help visualize how the convolution evolves over time. As an example, if we have precip at time step 1, we are interested in how this contributes to runoff at time steps 1,2,3 etc, all the way to 365 (or the end of our period). so the first row of the matrix q_all_loop represents the contribution of precipitation at timestep 1 to runoff at each timestep. the second row represents the contribution of precipitation at time step 2 to runoff at each timestep. Then when we sum up the rows, we get q_all, where each element represents the total runoff at a specific time step. 5) QUESTION (5 pts): We set the TF length to 365. What is the physical meaning of this (i.e., how well does this represent a real system and why)? Could the transfer function be shorter or longer than that? ANSWER: Plots Plot the observed and simulated runoff. Include a legend and label the y-axis. 6) QUESTION (3 pt): Evaluate how good or bad the model performed (i.e., visually compare simulated and observed streamflow, e.g., low flows and peak flows). ANSWER: 7) QUESTION (2 pt): Compare the effective precipitation total with the simulated runoff total and the observed runoff total. What is p_eff and how is it related to q_all? Discuss why there is a (small) mismatch between the sums of p_eff and q_all. ANSWER: THIS IS THE CODE FOR CONVOLUTION METHOD 2 This method saves storage requirements since only a vector is generated and not a full matrix that contains all response pulses. The workflow here is to generate a a response vector for the first pulse. This will take up 365 time steps. On the second time step, we generate another response pulse with 364 steps that starts at t=2. We then add that vector to the first one. On the third time step, we generate a response pulse of length 363 that starts at t=3 and then add it to the existing one. And so on and so forth. 3.2 Monte Carlo Simulation 3.2.1 Learning Module 5 3.2.1.1 Background: Monte Carlo Simulation is a method to estimate the probability of the outcomes of an uncertain event. It is based on a law of probability theory that says if we repeat an experiment many times, the average of the results will get closer to the true probability of those outcomes. First check out this video: 3.2.1.2 Reading Then read this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2924739/ for a understanding of the fundamentals. 3.2.1.3 How does this apply to hydrological modeling? When modeling watershed hydrological processes, we often attempting to quantify watershed inputs e.g., precipitation watershed outputs e.g., evapotranspiration, sublimation, runoff/discharge watershed storage precipitation that is not immediately converted to runoff or ET rather stored as snow or subsurface water. Imagine we are trying to predict the percentage of precipitation stored in a watershed after a storm event. We have learned that there are may factors that affect this prediction, like antecedent conditions, that may be difficult to measure directly. Monte Carlo Simulation can offer a valuable approach to estimate the probability of obtaining certain measurements when those factors can not be directly observed or measured. We can approximate the likelihood of specific measurement by simulating a range of possible scenarios. Monte Carlo Simulation is not only useful for estimating probabilities, but for conducting sensitivity analysis. In any model, there are usually several input parameters. Sensitivity analysis helps us understand how changes in these parameters affect the predicted values. To perform a sensitivity analysis using a Monte Carlo Simulation we can: Define the realistic ranges for each parameter we want to analyze Using Monte Carlo Simulation, randomly sample values from the defined ranges for each parameter Analyze output to understand how different input sample values affect the predicted output 3.2.1.4 Example Let’s consider all of this in an example model called WECOH - Watershed ECOHydrology. In this study, researchers (Nippgen et al.) were interested in the change in subsurface water storage through time and space on a daily and seasonal basis. Evolution of watershed connectivity The authors directly measured runoff/discharge, collected precipitation data at several points within the watershed, used remote sensing to estimate how much water was lost to evapotranspiration, and used digital elevation models to characterize the watershed topography. As we learned in the hydrograph separation module, topographic characteristics can have a significant impact on storage. Though resources like USDA’s Web Soil Survey can provide a basic understanding underlying geology across a large region, characterizing the heterogeneous nature of soils within a watershed can be logistically unfeasible. To estimate the soil characteristics like storage capacity (how much water the soil can hold) and hydraulic conductivity (how easily water can move through soil) in the study watershed, the authors used available resources to determine the possible range of values for each of their unknown parameters. They then tested thousands of model simulations using randomly selected values with the predetermined range for each of the soil characteristics. They compared the simulated discharge from these simulations to the actual discharge measurements. The simulations that predicted discharge patterns that closely matched reality helped them to estimate the unknown soil properties. Additionally, from the results of these simulations, they could identify which model inputs had the most significant impact on the discharge predictions, and how sensitive the output was to changes in each parameter. In this case, they determined that the model and system were most sensitive to precipitation. This type of sensitivity analysis can help us interpret the relative importance of different parameters and understand the overall sensitivity of the model or system. The study is linked for your reference but a thorough reading is not required. 3.2.1.5 Optional Reading This methods paper by Knighton et al. is an example of how Monte Carlo Simulation was used to estimate hydraulic conductivity in an urban system with varied land-cover.. 3.2.1.6 Generate a simulation with code For a 12-minute example in RStudio, check this out. If you are still learning the basics of R functionality, it may be helpful to code along with this video, pausing as needed. Note that this instruction is coding in an Rscript (after opening RStudio &gt; File &gt; New File &gt; R Script), rather than an Rmarkdown that we use in this class. 3.2.2 Repo link Download the repo for this lab HERE 3.2.3 Labwork (20 pnts) In this lab/homework, you will use the transfer function model from the previous module for some sensitivity analysis using Monte Carlo simulations. The code is mostly the same as last module with some small adjustments to save the parameters from each Monte Carlo run. In this Monte Carlo simulation, we’re testing how different parameter values affect simulated streamflow (discharge). Since we don’t know the true values of certain model parameters (like b1, b2, b3, a, and b that you may recall from the transfer function), we randomly sample them within a reasonable range and run the model many times to see which parameter combinations best match observed streamflow data. Each iteration of the loop represents one possible “guess” at the true parameter values, and we evaluate how good that guess is using metrics like Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). After the completion of the MC runs, we will use the Generalized Likelihood Uncertainty estimation (GLUE) method to evaluate parameter sensitivity. In other words, we sort all the runs to find the best-fitting parameter combinations, which helps us understand the most likely values of these unknown parameters in our hydrological model. There are three main objectives for this homework: 1) Set up the Monte Carlo analysis 2) Run the MC simulation for ONE of the years in the study period and perform a GLUE sensitivity analysis 3) Compare the different objective functions. A lot of the code in this homework will be provided. 3.2.3.1 Setup Import packages, including the “progress” and “tictoc” packages. These will allow us to time our loops and functions Read data - this is the same PQ data we have worked with in previous modules. Define variables 3.2.3.2 Parameter initialization This chunk has two purposes. The first is to set up the number of iterations for the Monte Carlo simulation. The entire model code is essentially wrapped into the main MC for loop. Each iteration of that loop is one full model realization: loss function, TF, convolution, model fit assessment (objective function). For each model run (each MC iteration), we will save the model parameters and respective objective functions in a dataframe. This will be the main source for the GLUE sensitivity analysis at the end. The model parameters are sampled in the main model chunk, this is just the preallocation of the dataframe. You will both run your own MC simulation, to set up the code, but will also receive a .Rdata file with 100,000 runs to give you more behavioral runs for the sensitivity analysis and uncertainty bounds. As a tip, while setting up the code, I would recommend setting the number of MC iterations to something low, for example 10 or maybe even 1. Once you have confirmed that your code works, crank up the number of iterations. Set it to 1000 to see how many behavioral runs you get. After that, load the file with the provided runs. 3.2.3.3 MC Model run This is the main model chunk. The tic() and toc() statements measure the execution time for the whole chunk. There is also a progressbar in the chunk that3 will run in the console and inform you about the progress of the MC simulation. The loss function parameters are set in the loss function, the TF parameters in the loss function code. For each loop iteration, we will store the parameter values and the simulated discharge in the “param” dataframe. So, if we ran the MC simulation 100 times, we would end up with 100 parameter combinations and simulated discharges. Q1 (2 pt) How are the loss function and transfer function parameters being sampled? That is, what is the underlying distribution and why did we choose it? (2-3 sentences) ANSWER: Extra point: What does the while loop do in the transfer function calculation? And why is it in there? (1-2 sentences) ANSWER: Save or load data After setting up the MC simulation, we will actually use a pre-created dataset. Running the MC simulation tens of thousands of times will take multiple hours. For that reason, we have done this for you and saved it as an importable data set. Best run Now that we have the dataframe with all parameter combinations and all efficiencies, we can plot the best simulation and compare it to the observed discharge 3.2.4 Sensitivity analysis We will use the GLUE methodology to assess parameter sensitivity. We will use GLUE for two purposes: 1) Assessing parameter sensitivity – Understanding how different parameter values influence model performance 2) Creating an envelope of model simulations – Identifying a range of possible model outputs that fit the observed data To start, we need to format the data so we can create dotty plots and density plots using Nash-Sutcliffe Efficiency (NSE) as our performance metric. When plotting, each parameter should be displayed in its own panel. You can achieve this using facet_wrap() in ggplot2, and be sure to set the axis scaling to “free” so each parameter is properly visualized Q2 (4 pts) Describe the sensitivity analysis with the three different plots. Are the parameters sensitive? Which ones are, which ones are not? Does this affect your “trust” in the model? (5-8 sentences) ANSWER: Q3 (4 pts) What are the differences between the dotty plots and the density plots? What are the differences between the two density plots? (2-3 sentences) ANSWER: Uncertainty bounds In this section, we will generate uncertainty bounds for our simulation results. Instead of using all model runs, we will only include the behavioral runs—the ones that meet our performance criteria. This ensures that our uncertainty range reflects only the most realistic simulations. Q4 (3 pts) Describe what the envelope actually is. Could we say we are dealing with confidence or prediction intervals? (2-3 sentences) Hint: Think about what the envelope is capturing. Does it reflect uncertainty in the model structure and parameters or does it describe the variability in future observations? ANSWER: Q5 (3 pts) If you inspect the individual model runs (in the Qruns df), you will notice that they all perform somewhat poorly when it comes to the initial baseflow. Why is that and what could you do to change this? (Note: you don’t have to actually do this, just describe how you might approach that issue (2-3 sentences) Hint: Consider factors such as parameter initialization, storage effects, or missing processes in the model structure. What tuning or modifications could address this? ANSWER: Q6 (4 pts) Run the provided code to generate a plot comparing the best model run for each objective function. Then, analyze the differences in model performance by answering the following: Which objective function provides the best match to observed runoff overall? How do the different functions perform in capturing peak flow events? Which function best represents baseflow conditions? Which struggles the most? If you were to prioritize accuracy in low-flow conditions, which objective function might you choose and why?** ANSWER: The coding steps are: 1) get the simulated q vector with the best run for each objective function, 2) put them in a dataframe/tibble, 3) create the long form, 4) plot the long form dataframe/tibble. These steps are just ONE possible outline how the coding steps can be broken up. Response surface "],["model-classification-and-application.html", "Chapter 4 Model Classification and Application 4.1 Classifying model structure 4.2 Term Project Assignment 1 - 10 pnts", " Chapter 4 Model Classification and Application 4.1 Classifying model structure 4.1.1 Learning Module 6 Throughout the rest of the course, we will gather data and create models to explore how measurable environmental factors, such as snow melt, land cover, and topography, impact runoff. To discuss these methods, we should review some modeling terminology describing model complexity and type. Environmental models, including hydrological models, are built around simplifying assumptions of natural systems. The complexity of the model may depend on its application. Effective hydrological models share key traits: they are simple, parsimonious, and robust across various watersheds. In other words, they are easy to understand and streamlined and consistently perform well across different basins or even geographical areas. Therefore, more complex is only sometimes better. 4.1.1.1 Spatial Complexity There are general terms that classify the spatial complexity of hydrological models: A lumped system is one in which the dependent variables of interest are a function of time alone, and the study basin is spatially ‘lumped’ or assumed to be spatially homogeneous across the basin. So far in this course, we have focused mainly on lumped models. You may remember the figure below from the transfer functions module. It represents the lumped watershed as a bucket with a single input, outlet output, and storage volume for each timestep. A distributed system is one in which all dependent variables are functions of time and one or more spatial variables. Modeling a distributed system means partitioning our basins into raster cells (grids) and assigning inputs, outputs, and the spatial variables that affect inputs and outputs across these cells. We then calculate the processes at the cell level and route them downstream. These models allow us to represent the spatial complexity of physically based processes. They can simulate or forecast parameters other than streamflow, such as soil moisture, evapotranspiration, and groundwater recharge. A semi-distributed system is an intermediate approach that combines elements of both lumped and distributed systems. Certain variables may be spatially distributed, while others are treated as lumped. Alternatively, we can divide the watershed into sub-basins and treat each sub-basin as a lumped basin. Outputs from each sub-basin are then linked together and routed downstream. Semi-distribution allows for a more nuanced representation of the basin’s characteristics, acknowledging spatial variability where needed while maintaining some simplifications for computation efficiency. In small-scale studies, we can design a model structure that fits the specific situation well. However, when we are dealing with larger areas, model design may be challenging. Our data might differ across regions with variable climate and landscape features. Sometimes, it is best to use a complex model to capture all the different processes happening over a big area. However, it could be better to stick with a simpler model because we might have limited data or the number of calculations is very computationally expensive. It is up to the modeler to determine the simplest model that meets the desired application. For this determination, it is important to understand the advantages of different modeling approaches. 4.1.1.2 Modeling Approaches Empirical Models are based on empirical analysis of observed inputs (e.g., rainfall) or outputs (ET, discharge). These are most useful if you have extensive historical data so models can capture trends effectively. For example, if your goal is to predict the amount of dissolved organic carbon (DOC) transported out of a certain watershed, an empirical model will likely suffice. However, simple models may not be transferable to other watersheds. Also, they may not reveal much about the physical processes influencing runoff. Therefore, these types of models may not be valid after the study area experiences land use or climate change. Conceptual Models describe processes with simple mathematical equations. For example, we might use a simple linear equation to interpolate precipitation inputs over a watershed with a high elevation gradient using precipitation measurements from two points (high and low). This represents the basic relationship between precipitation and elevation, but does not capture all features that affect precipitation patterns (e.g. aspect, prevailing winds). The combined impact of these factors is probably negligible compared to the substantial amount of data required to accurately model them, so a conceptual model is sufficient. These can models can be especially useful when we have limited data, but theoretical knowledge to help ‘fill in the blanks’. Physically Based Models These models offer deep insights into the processes governing runoff generation by relying on fundamental physical equations like mass conservation. However, they come with drawbacks. Their implementation often demands complex numerical solving methods and a significant volume of input data. For example, if we want to understand how DOC transport changes in a watershed after a wildfire, we would want to understand many physical system properties pre- and post-fire like soil infiltration rates, quantification of forest canopy, stream flow data, carbon export, etc.. Without empirical data to validate these techniques, there is a risk of introducing substantial uncertainty into our models, reducing their reliability and effectiveness. An example of a spatial distributed and physically based watershed model from Huning and Marguilis, 2015: When modeling watersheds, we often use a mix of empirical, conceptual, and physically based models. The choice of model type depends on factors like the data we have, the time or computing resources we can allocate, and how we plan to use the model. These categorizations provide a philosophical foundation of how we understand and simulate systems. However we can also consider classifications that focus on the quantitative tools and techniques we use to implement these approaches. Consider that we have already applied each of these tools: Probability Models Many environmental processes can be thought of or modeled as stochastic, meaning a variable may take on any value within a specified range or set of values with a certain probability. Probability can be thought of in terms of the relative frequency of an event. We utilized probability models in the return intervals module where we observed precipitation data, and used that data to develop probability distributions to estimate likely outcomes for runoff. Probability models allow us to quantify risk and variability in systems. Regression Models Often we are interested in modeling processes with limited data, or processes that aren’t well understood. Regression assumes that there is a relationship between dependent and independent variables (you may also see modelers call these explanatory and response variables). We utilized regression methods in the hydrograph separation module to consider process-based mechanisms that differed among watersheds. Simulation Models Simulation models can simulate time series of hydrologic variables (as in the following snow melt module), or they can simulate characteristics of the modeled system, as we saw in the Monte Carlo module. These types of models are based on an assumption of what the significant variables are, an understanding of the important processes are, and/or a derivation of these physical processes from first principles (mass, energy balance). 4.1.1.3 A priori model selection: By understanding the different frameworks of environmental modeling, we can choose the right tools for the right context, depending on our data, goals and resources. In reality, the final model selection is a fluid process requiring multiple iterations at each step. In Keith Beven’s Rainfall-Runoff Modelling Primer, they illustrate the process as: While we aim to give some hands on experience across multiple model types, there is a wide range of possible models! Why would the most complex model, or one that represents the most elements in a system be best? Why even consider a simple bucket model? Many modelers have observed that the number of parameters required to describe a key behavior in a watershed is often quite low, meaning increasing the number of parameters does not result in a significantly improved model. This idea that simple models are often sufficient for representing a system have led to the investigation of parsimonious model structures (less complex). Consider though, that the model must sufficiently represent processes or it will be too unreliable outside of the range of conditions on which it was calibrated. Now that we have reviewed some concepts, our next step will be to develop a term project question. As you brainstorm and gather data, be sure to consider and use the modeling concepts and terminology we’ve covered to frame and structure your project design. 4.2 Term Project Assignment 1 - 10 pnts Repo Link As part of the Professional Masters’ Program, you are required to develop a professional paper. It is intended to give you an in-depth experience in the design, implementation, and completion of an original project. The paper will also be a way to showcase your research interests and accomplishments to potential employers or admissions committees. (See LRES 575 for more). Your term project for this class is not meant to encompass your entire thesis or dissertation. Instead, it should focus on a single, well-defined component that contributes to your broader research. This could be an in-depth exploration of a specific feature of your research question or a clear demonstration of a cause-effect relationship within your work. Next week, you will design a repeatable workflow that is executable within the scope of this semester, and at the end of the term, you will present a concise (~8-minute) synopsis of your question and workflow. This assignment is focused on data retrieval; you will develop the main goal and objectives of your term project and explore, evaluate, and select data sources. A key component of this process is formulating a research question that clearly defines both an explanatory variable and a response variable; one is not sufficient without the other. The workflow for this assignment will help you refine your question and identify the datasets needed to address it. Your assignment is considered complete when you have identified at least one explanatory variable and one response variable, along with the corresponding data sources. 4.2.1 Brainstorming Q1. What is the primary research question for your term project? (1-2 sentences)(1 pt) Think of this as your first approach to developing a research question. It should reflect a clear purpose and be specific enough to guide your initial search for data, but it likely will evolve throughout this assignment. That is okay, even expected. You will be asked for a refined response at the end of the assignment. Consider: - The problem or issue you want to address - What specific phenomenon within that issue are you interested in understanding - What measurable or observable outcomes do you want to analyze. For example, instead of ‘how does climate change affect forests?’, you might consider ‘How has seasonal precipitation variability impacted the timing of peak NDVI in the Pacific Northwest from 2000 to 2020.’ Note that this question is specific enough to guide your search for the necessary data, such as precipitation records and NDVI time series. ANSWER: Q2. What are 2-3 types of data your research question requires? Address each sub-question (2 pts) Be as specific as you can, what types of variables or indicators are you looking for? What format should the data be in (e.g., spatial datasets, time-series data, species counts)? What might be the origin of this data? (e.g., satellite imagery, weather station records, survey data?) What temporal length and resolution would be ideal to answer this question? ANSWER: 4.2.2 Data exploration and selection Now that you have an idea of what kind of data your question requires, the next step is to locate actual datasets that support your inquiry. There are numerous datasets available to public use, including public repositories, government agencies, academic institutions, even data uploaded to repositories like Zendodo and Figshare making data for specific studies discoverable and citable by other researchers. You may even have datasets from your organization that you can use. (Note: If you plan to use a dataset from your organization, you must have direct access to it now, not just a promise that you can obtain it later. Do not commit to a dataset unless you already have it and can work with it this semester). It is up to you to explore a variety of sources and identify datasets that you can retrieve and work with, but here are some ideas and suggestions to get you started. Many of these platforms allow R access to their datasets with packages that facilitate downloading, managing, and analyzing the data directly within R. However if you can not find an appropriate package, see below for a webscraping example. Climate or weather data: NOAA, SNOTEL, and Google’s climate engine are all potential sources of earth data. Hydrological data: USGS Soil data: Web Soil Survey, try R package soilDB Water quality and air pollution: EPA Envirofacts Global biodiversity: Global Biodiversity Information Facility with the R processing package rgbif eBird and R package auk Fire: Monitoring Trends in Burn Severity Remotely sensed spatial data: You may have already some experience with Google Earth Engine. This is a fast way to access an incredible library of resources. If you have used this in other courses but feel rusty, we can help! Also check out this e-book. However, if your interest is in a few images, USGS’ Earth Explorer GloVis is also a good resource. 4.2.2.1 Expand your search This is not an exhaustive list of possibilities. Consider searching MSU’s library databases. For example, the Web of Science database contains organized searchable datasets built by published researchers. Google searches using advanced search operators to include specific file types with your search terms can be helpful (e.g., .csv, .xlsx, .geojson). Engage with AI tools to explore potential datasets or repositories based your specific topic. An active approach to searching and learning will help you discover datasets that will support your research question. You can download and import data if the dataset is small, or for large datasets, you might try webscraping. Webscraping in R is a method of extracting data from web pages when the data isn’t available through an API or a direct download. Check out some of the methods in this 15 min video: webscraping example for R with ChatGPT. There are many similar tutorials, some tailored to specific data types. Take some time to explore the available resources. 4.2.2.2 Adapt your research question Once you spend a solid hour or two searching databases, you may find that you need to adapt your research question based on data availability. You will likely spend some time refining your initial question to fit the datasets you find and that is encouraged! Then resume the dataset search, and continue refining your question and searching until you can develop an executable methodology. Q3. Explore your data, this part requires 2 answers (3 pnts total) You are encouraged to explore many packages and write many more code chunks for your personal use. However, for the assignment submission, retrieve at least one dataset relevant to your question using an R package like dataRetrieval or by webscraping in R. Be sure that your methods are reproducible, meaning I should be able to run your code from my computer and see the same figures and plots that you do without needing a .txt or .csv in my working directory. Similarly, try writing required packages using a the function script below. This ensures that if another user does not have a package installed, it will be installed before it is loaded. Install needed packages # #Load packages # # # pkgTest is a helper function to load packages and install packages only when they are not installed yet. # pkgTest &lt;- function(x) # { # if (x %in% rownames(installed.packages()) == FALSE) { # install.packages(x, dependencies= TRUE) # } # library(x, character.only = TRUE) # } # neededPackages &lt;- c(&quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;Evapotranspiration&quot;) # for (package in neededPackages){pkgTest(package)} Explore the available data using vignettes or Cran documentation for your specific dataset and package like dataRetrieval’s documentation. Create a dataframe containing 3-6 columns and print the head of the dataframe. #head(dfname) Write a brief summary (3-4 sentences) of what your dataframe describes. Generate plots: Create at least one plot that summarizes the data and describe it’s use to you. Are there gaps in the data? Does the data cover the time or space you are interested in? Are there significant outliers that need consideration? Generate a histogram or density plot of at least one variable in your dataset. The script here will help start a density plot showing multiple variables. You may adapt or change this as needed. #plottable_vars &lt;- dfname %&gt;% # dplyr::select(variable1, variable2,...) #long &lt;- plottable_vars %&gt;% # pivot_longer(cols = everything(), names_to = &quot;Variable&quot;, values_to = &quot;Value&quot;) # Plot density plots for each variable #ggplot(long, aes(x = Value, fill = Variable)) + # geom_density(alpha = 0.5) + # Add transparency for overlapping densities # facet_wrap(~ Variable, scales = &quot;free&quot;, ncol = 2) + # Create separate panels per variable # theme_minimal() + # labs(title = &quot;Density Plots for Variables&quot;, # x = &quot;Value&quot;, y = &quot;Density&quot;) What does the above plot tell you about the distribution of your data? Is it as expected? Why do we need to consider the distribution of data when deciding on analysis methods? Q4. Putting it all together (3 pts) Create a table in R and export it as a .csv file to submit with this assignment .rmd. In this table, include all datasets (at least two; one explanatory variable and one response variable though you can have more of either) that you will use for your term project. The script below specifies all of the sections required in this table, though you will need to change names and information accordingly in c(). # Load necessary library library(dplyr) # Create a data frame data_table &lt;- data.frame( Dataset_Name = c(&quot;Dataset1&quot;, &#39;Dataset2&#39;), Data_Source = c(&#39;NOAA&#39;, &#39;NRCS&#39;), #Agency or institution name Data_Type = c(&quot;Climate&quot;, &quot;Hydrology&quot;), # what kind of data is it? Source_Link = c(&quot;https://www.link1/&quot;, &quot;https://link2/&quot;), # We will used these to access the sources Key_variables = c(&#39;precipitation&#39;, &#39;soil_conductivity&#39;), Temporal_range = c(&#39;2020-2021&#39;, &#39;1979-today&#39;), Spatial_Coverage = c(&#39;Montana&#39;, &#39;Juneau_AK&#39;), Data_Quality = c(&quot;High&quot;, &quot;Moderate&quot;), # An example of high quality might be a dataset that has already been cleaned by an agency is not missing data in the period or space of interest. Data_Quality_notes = c(&#39;QCd with no missing data&#39;, &#39;some unexpected values&#39;), Feasibility = c(&quot;High&quot;, &quot;Medium&quot;), # How useable is this to you? Do you need help figuring out how to download it? Is there an R package that you need to learn to access the data? Feasibility_notes = c(&#39;note1&#39;, &#39;note2&#39;), variable_type = c(&#39;explanatory&#39;, &#39;response&#39;) ) # Display the table print(data_table) # Export the data table as a .csv to a file path of your choice: #exportpath &lt;- file.path(getwd(), somefolder, term_assign_table.csv) #replace somefolde with an actual folder name that exists in your local environment #write.csv(data_able, exportpath, row.names=FALSE) Once this is exported, you can format if desired for readibility and submit the .csv with a completed .rmd. Q5. What is your updated research question? How, specifically, will the data sources listed above help you to answer that question? (1 pt) "],["physical-process-models.html", "Chapter 5 Physical Process Models 5.1 Evapotranspiration 5.2 Snowmelt Models", " Chapter 5 Physical Process Models 5.1 Evapotranspiration 5.1.1 Repo here 5.1.2 Learning Module 7 20pnts 5.1.2.1 Background Suggested reading: Forest Evapotranspiration: Measurement and Modeling at Multiple Scales Evapotranspiration (ET) encompasses all processes through which water moves from the Earth’s surface to the atmosphere, comprising both evaporation and transpiration. This includes water vaporizing into the air from soil surfaces, the capillary fringe of groundwater, and water bodies on land. Much like snowmelt modeling, ET modeling and measurements are critical to many fields and could be a full course on its own. We will be focused on the basics of ET, modeling and data retrieval methods for water balance in hydrological modeling. Evapotranspiration is an important variable in hydrological models, as it accounts for much of the water loss in a system, outside of discharge. Transpiration, a significant component of ET, involves the movement of water from soil to atmosphere through plants. This occurs as plants absorb liquid water from the soil and release water vapor through their leaves. To gain a deeper understanding of ET, let’s review transpiration. 5.1.2.1.1 Transpiration Plant root systems to absorb water and nutrients from the soil, which they then distribute to their stems and leaves. As part of this process, plants regulate the loss of water vapor into the atmosphere through stomatal apertures, or transpiration. However, the volume of water transpired can vary widely due to factors like weather conditions and plant traits. Vegetation type: Plants transpire water at different rates. Some plants in arid regions have evolved mechanisms to conserve water by reducing transpiration. One mechanism involves regulating stomatal opening and closure. These plants can minimize water loss, especially during periods of high heat and low humidity. This closure of stomata can lead to diel and seasonal patterns in transpiration rates. Throughout the day, when environmental conditions are favorable for photosynthesis, stomata open to allow gas exchange, leading to increased transpiration. Conversely, during the night or under stressful conditions, stomata may close to conserve water, resulting in reduced transpiration rates. Humidity: As the relative humidity of the air surrounding the plant rises the transpiration rate falls. It is easier for water to evaporate into dryer air than into more saturated air. Soil type and saturation: Clay particles, being small, have a high capacity to retain water, while sand particles, being larger, readily release water. During dry periods, transpiration can contribute to the loss of moisture in the upper soil zone.When there is a shortage of moisture in the soil, plants may enter a state of senescence and reduce their rate of transpiration. Temperature: Transpiration rates go up as the temperature goes up, especially during the growing season, when the air is warmer due to stronger sunlight and warmer air masses. Higher temperatures cause the plant cells to open stomata, allowing for the exchange of CO2 and water with the atmosphere, whereas colder temperatures cause the openings to close. The availability and intensity of sunlight have a direct impact on transpiration rates. Likewise, the aspect of a location can influence transpiration since sunlight availability often depends on it. Wind &amp; air movement: Increased movement of the air around a plant will result in a higher transpiration rate. Wind will move the air around, with the result that the more saturated air close to the leaf is replaced by drier air. 5.1.2.2 Measurements In the realm of evapotranspiration (ET) modeling and data analysis, you’ll frequently encounter the terms potential ET and actual ET. These terms are important to consider when selecting data, as they offer very different insights into water loss processes from the land surface to the atmosphere. Potential Evapotranspiration (PET): Potential ET refers to the maximum possible rate at which water could evaporate and transpire under ideal conditions. These conditions typically assume an ample supply of water, unrestricted soil moisture availability, and sufficient energy to drive the evaporative processes. PET is often estimated based on meteorological variables such as temperature, humidity, wind speed, and solar radiation using empirical equations like the Penman-Monteith equation. Actual Evapotranspiration (AET): Actual ET, on the other hand, represents the observed or estimated rate at which water is actually evaporating and transpiring from the land surface under existing environmental conditions. Unlike PET, AET accounts for factors such as soil moisture availability, vegetation cover, stomatal conductance, and atmospheric demand. It reflects the true water loss from the ecosystem and is of greater interest in hydrological modeling, as it provides a more realistic depiction of water balance dynamics. The formula for converting PET to AET is: AET = PET * Kc Where: AET is the actual evapotranspiration, PET is the potential evapotranspiration, and Kc is the crop coefficient. The crop coefficient accounts for factors such as crop type, soil moisture levels, climate conditions, and management practices. It can vary throughout the growing season as well. 5.1.2.2.1 Direct measurements: There are several methods to measure ET directly like lysimeters and gravimetric analysis, but this data rarely available to the public. There has been a concerted effort to enhance the accessibility of Eddy Covariance data, so this dataset may expand in the years to come. knitr::include_url(&quot;https://www.youtube.com/embed/CR4Anc8Mkas&quot;) This video focuses on CO2 as an output of eddy covariance data, but among the ‘other gases’ mentioned, water vapor is included, offering measurements of actual ET. The video also provides a resource where you might find eddy covariance data for your region of interest. 5.1.2.2.2 Remote sensing: Remote sensing of evapotranspiration (ET) involves the use of satellite or airborne sensors to observe and quantify the exchange of water vapor between the Earth’s surface and the atmosphere over large spatial scales. This approach offers several advantages, including the ability to monitor ET across diverse landscapes, regardless of accessibility, and to capture variations in ET over time with high temporal resolution. Remote sensing data, coupled with energy balance models, can be used to estimate ET by quantifying the energy fluxes at the land surface. These models balance incoming solar radiation with outgoing energy fluxes, including sensible heat flux and latent heat flux (representing ET). Remote sensing-based ET estimates are often validated and calibrated using ground-based measurements, such as eddy covariance towers or lysimeters, to ensure accuracy and reliability. It can be helpful to validate these models yourself if you have a data source available in your ecoregion as a ‘sanity check’. Keep in mind that there are numerous models available, some of which may be tailored for specific ecoregions, resulting in significant variations in estimated evapotranspiration (ET) for your area among these models. If directly measured ET data is not available, you can check model output in a simple water balance. For example, inputs - outputs for your watershed (Ppt - Q - ET) should be approximately 0 (recall from our transfer function module that it is likely not exact). If the ET estimate matches precipitation, it’s likely that the selected model is overestimating ET for your region. Some resources for finding ET modeled from remote sensing data: ClimateEngine.org - This is a fun resource for all kinds of data. Actual evapotranspiration can be found in the TerraClimate dataset. OpenET - you need a Google account for this one. This site is great if you need timeseries data. You can select ‘gridded data’ and draw a polygon in your area of interest. You can select the year of interest at the top of the map, and once the timeseries generates, you can view and compare the output of seven different models. 5.1.2.3 Modeling: 5.1.3 Labwork (20 pts): For this assignment, we will work again in the Fraser Experimental Forest (same site as snowmelt module). Not only are there meteorological stations in our watershed of interest, but there is a Eddy Covariance tower nearby, in a forest with the same tree community as our watershed of interest. We can use this data to verify our model output for modeled data. 5.1.3.1 The Evapotranspiration package We will test a couple of simple methods that require few data inputs. However, it may be helpful to know that this package will allow calculations of PET, AET and Reference Crop Evapotranspiration from many different equations. Your selection of models may depend on the study region and the data available. 5.1.3.2 Import libraries 5.1.3.3 Import data 5.1.3.3.1 Meteorological data We’ll import three datasets in this workflow, one containing actual data from the eddy covariance tower for the years 2017-2018. Also imported is the discharge (Q) data from our watershed. Note that data is not collected during periods of deep snow. We will use this data to estimate the total discharge for 2022. Additionally, we’ll import the meteorological data from our watershed for the years of interest. We are going to estimate ET for our watershed during the 2022 water year, though met data is provided in calendar years, so we will need to extract the desired data from the met dataframe. 5.1.3.4 Data formatting We will model ET at daily timesteps. Note that all imported data is for different timesteps, units and measurements than what we need for analysis. It is common practice to manipulate the imported data and adjust it to align with our model functions. Ensuring the correct structure of the data can prevent potential issues later in the code Now we have the RHmax and RHmin values that we will need for our ET models 5.1.3.4.1 Precipitation To add a layer of checks we will also check precipitation totals for the watershed from our met data. This dataframe also provides cumulative precipitation, though the accumulation begins on January 1. Therefore we will need to calculate daily precipitation for each day of the water year and generate a new cumulative value. This was a lot of work to save one value. 5.1.3.5 Pseudocode Often when writing code, it is necessary to first write ‘pseudocode’. This allows us to think about workflow before writing scripts involves planning and structuring the logic of a script. Psuedocode is usually mixture of natural language and code-like constructs (e.g., ‘I’ll use ’merge’ or ‘ggplot’ to…’) that serves as a blueprint for our script before we dive into specific programming language syntax. Then we can develop our code by filling in the programming language as needed. Q1. (2 pnts) In you own words, what did we do at each step in the above chunk and what is cumulative_ppt_2022. Why do we want to know this? ANSWER: 5.1.3.5.1 Discharge data Our next step will be to import and format discharge data collected from the weir at Fool Creek. Again, data collection starts on April 20th this year. To estimate discharge between Sept 31 of the previous year and April 20th of 2022, we will assume daily discharge between these dates is the mean of the discharge values from each of these dates. Q2 (3 pnts) Write pseudocode in a stepwise fashion to describe the workflow that will need to happen to estimate the total volume of water in mm/watershed area for 2022 lost to stream discharge. Consider that the units provided by USFS are in cubic feet/second (cfs). ANSWER: 1. Import dataframe 2. … 5.1.3.5.2 Evapotranspiration data Import and transform ET data. Note that ET data is in mmol/m2/s. We want to convert this to mm/day. Eddy covariance data collected from towers represents the exchange of gases, including water vapor, between the atmosphere and the land surface within a certain area known as the “footprint.” This footprint area is not fixed; it changes in size and shape depending on factors like wind direction, thermal stability, and measurement height. Typically, it has a gradual border, meaning that the influence of the tower measurements extends beyond its immediate vicinity. For our specific tower data, we’ve estimated the mean flux area, or footprint area, to be approximately 0.4 square kilometers. However, when estimating the total evapotranspiration (ET) for our entire watershed, we need to extrapolate the ET measured by the tower to cover the entire watershed area. This extrapolation involves scaling up the measurements from the tower footprint to the larger area of the watershed to get a more comprehensive understanding of water vapor exchange over the entire region. This appears to be a fairly well balanced water budget, especially consdering that we have made some estimates along the way. Let’s see how this ET data compares to modeled data. 5.1.3.6 The Priestly Taylor method (actual evapotranspiration) The Priestley-Taylor method is a semi-empirical model that estimates potential evapotranspiration as a function of net radiation. This method requires a list which contains: (climate variables) required by Priestley-Taylor formulation: Tmax, Tmin (degree Celcius), RHmax, RHmin (percent), Rs (Megajoules per sqm) or n (hour). We have measurements of relative humidity (RH) data from within our watershed meteorological station. However, RH data can be found through many sources providing local weather data. n refers to the number of sunshine hours. Evapotranspiration comes with a list physical ‘constants’, but we want to replace important constants with data specific to our cite. Recall that each of these are estimates of potential ET. Crop coefficients (Kc values) for specific tree species like those found in the Fraser Experimental Forest (lodgepole pine or Englemann spruce) may not be as readily available as they are for agricultural crops. However, you may be able to find some guidance in scientific literature or forestry publications. From what we have found, Kc estimates for lodgepole pine forests can be between 0.4 and 0.8. These values may vary depending on factors such as climate, soil type, elevation, and other site-specific factors. Our own estimates using water balance data from dates that correspond with the eddy flux tower data suggest seasonal fluctuations, with a mean of 0.55. AET = PET * Kc Let’s plot the two modeled ET timeseries with the eddy covaraince tower data. Q3 (4 pnts) Considering that models are often developed for specific ecosystems, discuss why these two methods might yield such different results when applied to the current system. What ecosystem-specific factors could influence the accuracy of these models in this context? ANSWER: Q4. (3pnts) In the context of evapotranspiration modeling, discuss how vegetation type and soil properties might interact to influence AET in the Fraser Experimental Forest. Provide a hypothetical example of how these factors could vary between two sub-watersheds in the area. ANSWER: Q5 (5 pnts) Review the map watershed figure and consider changes in vegetation type and soil properties in high elevation watersheds. How might these variables interact to influence AET in the Fraser Experimental Forest. Provide a hypothetical example of how these factors could vary between two sub-watersheds in the area. Q6 (3 pnts) Let’s assume we do not have eddy covariance tower data for this time period, but you have the provided discharge and precipitation measurements. Which model would you choose to estimate an annual ET sum and why? ANSWER: Q6 (4 pnts) In our modeling script, while our main objective was to estimate evapotranspiration (ET), which specific aspect required the most labor and attention to detail? Reflect on the tasks involved in data preparation, cleaning, and formatting. How did these preliminary steps impact the overall modeling process? 5.2 Snowmelt Models 5.2.1 Learning Module 8 5.2.1.1 Background: Understanding snowmelt runoff is crucial for managing water resources and assessing flood risks, as it plays a significant role in the hydrologic cycle. Annual runoff and peak flow are influenced by snowmelt, rainfall, or a combination of both. In regions with a snowmelt-driven hydrologic cycle, such as the Rocky Mountains, snowpack acts as a natural reservoir, storing water during the winter months and releasing it gradually during the spring and early summer, thereby playing a vital role in maintaining water availability for various uses downstream. By examining how snowmelt interacts with other factors like precipitation, land cover, and temperature, we can better anticipate water supply fluctuations and design effective flood management strategies. Learning objectives: In this module, our primary focus will be modeling snowmelt as runoff, enabling us to predict when it will impact streamflow timing. We will consider some factors that may influence runoff timing. However, the term ‘snowmelt modeling’ is a field in itself and can represent a lifetime worth of work. There are many uses for snowmelt modeling (e.g., climate science and avalanche forecasting). If you are interested in exploring more on this subject, there is an excellent Snow Hydrology: Focus on Modeling series offered by CUAHSI’s Virtual University on YouTube. Helpful terms: The most common way to measure the water content of the snowpack is by the Snow Water Equivalent or SWE. The SWE is the water depth resulting from melting a unit column of the snowpack. 5.2.1.2 Model Approaches Similar to the model development structure we discussed in the last module, snowmelt models are generally classified into three different types of abalation algorithms Empirical and Data-Driven Models: These models use historical data and statistical techniques to predict runoff based on the relationship between snow characteristics (like snow area) and runoff. They use past patterns to make predictions about the future. The emergence of data-driven models has benefited from the growth of massive data and the rapid increase in computational power. These models simulate the changes in snowmelt runoff using machine learning algorithms to select appropriate parameters (e.g., daily rainfall, temperature, solar radiation, snow area, and snow water equivalent) from different data sources. Conceptual Models: These models simplify the snowmelt process by establishing a simple, rule-based relationship between snowmelt and temperature. These models use a basic formula based on temperature to estimate how much snow will melt. Physical Models: The physical snowmelt models calculate snowmelt based on the energy balance of snow cover. If all the heat fluxes toward the snowpack are considered positive and those away are considered negative, the sum of these fluxes is equal to the change in heat content of the snowpack for a given time period. Fluxes considered may be net solar radiation (solar incoming minus albedo), thermal radiation, sensible heat transfer of air (e.g., when air is a different temperature than snowpack), latent heat of vaporization from condensation or evaporation/sublimation, heat conducted from the ground, advected heat from precipitation examples: layered snow thermal model (SNTHERM) and physical snowpack model (SNOWPACK), Many effective models may incorporate elements from some or all of these modeling approaches. 5.2.1.3 Spatial complexity We may also identify models based on the model architecture or spatial complexity. The architecture can be designed based on assumptions about the physical processes that may affect the snowmelt to runoff volume and timing. Homogenous basin modeling: You may also hear these types of models referred to as ‘black box’ models. Black-box models do not provide a detailed description of the underlying hydrological processes. Instead, they are typically expressed as empirical models that rely on statistical relationships between input and output variables. While these models can predict specific outcomes effectively, they may not be ideal for understanding the physical mechanisms that drive hydrological processes. In terms of snow cover, this is a simplistic case model where we assume: the snow is consistent from top to bottom of the snow column and across the watershed melt appears at the top of the snowpack water immediately flows out the bottom This type of modeling may work well if the snowpack is isothermal, if we are interested in runoff over large timesteps, or if we are modeling annual water budgets in lumped models. Vertical layered modeling: Depending on the desired application of the model, snowmelt may be modeled in multiple layers in the snow column (air-snow surface to ground). Climate models, for example, may estimate phase changes or heat flux and consider the snowpack in 5 or more layers. Avalanche forecasters may need to consider grain evolution, density, water content, and more over hundreds of layers! Hydrologists may also choose variable layers, but many will choose single- or two-layer models for basin-wide studies, as simple models can be effective when estimating basin runoff. Here is a study by Dutra et al. (2012) that looked at the effect of the number of snow layers, liquid water representation, snow density, and snow albedo parameterizations within their tested models. Table 1 and figures 1-3 will be sufficient to understand the effects of changes to these parameters on modeled runoff and SWE. In this case, the three-layer model performed best when predicting the timing of the SWE and runoff, but density improved more by changing other parameters rather than layers (Figure 1). Lateral spatial heterogeneity: The spatial extent of the snow cover determines the area contributing to runoff at any given time during the melt period. The more snow there is, the more water there will be when it melts. Therefore, snow cover tells us which areas will contribute water to rivers and streams as the snow melts. In areas with a lot of accumulated snow, the amount of snow covering the ground gradually decreases as the weather warms up. This melting process can take several months. How quickly the snow disappears depends on the landscape. For example, in mountainous ecosystems, factors like elevation, slope aspect, slope gradient, and forest structure affect how the snow can accumulate, evaporate or sublimate and how quickly the snow melts. For mountain areas, similar patterns of depletion occur from year to year and can be related to the snow water equivalent (SWE) at a site, accumulated ablation, accumulated degree-days, or to runoff from the watershed using depletion curves from historical data. Here is an example of snow depletion curves developed using statistical modeling and remotely sensed data. The use of remotely sensed data can be incredibly helpful to providing estimates in remote areas with infrequent measurements. Observing depletion patterns may not be effective in ecosystems where patterns are more variable (e.g., prairies). However, stratified sampling with snow surveys, snow telemetry networks (SNOTEL) or continuous precipitation measurements can be used with techniques like cluster analyses or interpolation, to determine variables that influence SWE and estimate SWE depth or runoff over heterogeneous systems. You can further explore all readings linked in the above section. These readings may assist in developing the workflow for your term project, though they are optional for completing this assignment. However, it is recommended that you review the figures to grasp the concepts and retain them for future reference if necessary. 5.2.1.4 Model choices: My snow is different from your snow When determining the architecture of your snow model, your model choices will reflect the application of your model and the processes you are trying to represent. Recall that parsimony and simplicity often make for the most effective models. So, how do we know if we have a good model? Here are a few things we can check to validate our model choices: Model Variability: A good model should produce consistent results when given the same inputs and conditions. Variability between model runs should be minimal if the watershed or environment is not changing. Versatility: Check the model under a range of scenarios different from the conditions under which it was developed. The model should apply to similar systems or scenarios beyond the initial scope of development Sensitivity Analysis: We reviewed this a bit in the Monte Carlo module. How do changes in model inputs impact outputs? A good model will show reasonable sensitivity changes in input parameters, with outputs responding as expected. Validation with empirical data: Comparison with real-world data checks whether the model accurately represents the actual system Applicability and simplicity: A good model should provide valuable insights or aid in decision-making processes relevant to the problem it addresses. It strikes a balance between complexity and simplicity, avoiding unnecessary intricacies that can lead to overfitting or computational inefficiency while sufficiently capturing the system’s complexities. 5.2.2 Labwork (20 pnts) 5.2.2.1 Download the repo for this lab HERE In this module, we will simulate snowmelt in a montane watershed in central Colorado with a simple temperature model. The Fool Creek watershed is located in the Fraser Experimental Forest (FEF), 137 km west of Denver, Colorado. The FEF contains several headwater streams (Strahler order 1-3) within the Colorado Headwaters watershed which supplies much of the water to the populated Colorado Front Range. This Forest has been the site of many studies to evaluate the effects of land cover change on watershed hydrology. The Fool Creek is a small, forested headwater stream, with an approximate watershed area of 2.75 km^2. The watershed elevation ranges from approximately 2,930m (9,600ft) at the USFS monitoring station to 3,475m (11,400ft) at the highest point. There is a SNOTEL station located at 3,400m. Fool Creek delineated watershed Q1: Using this map, what differences can you see between these two sites or what variation do you see in the watershed that may affect snow accumulation and melt? (2 points)? ANSWER: Let’s look at some data for Fool Creek: This script collects SNOTEL input data using snotelr. The SNOTEL automated collection site in Fool Creek supplies SWE data that can simplify our runoff modeling. Lets explore the sites available through snotelr and select the site of interest. 5.2.2.2 Import data: We will generate a conceptual runoff model using the date, daily mean temperature, SWE depth in mm, and daily precipitation (mm). We will use select() to keep the desired columns only. Then we will use mutate() to add a water year, and group_by() to add a cumulative precipitation column for each water year. We will also download flow data collected by USFS at the Fool Creek outlet at 2,930m. Let’s look at the data for the SNOTEL station at 3400m in elevation. 5.2.2.3 Calculate liquid input to the ground by analyzing the daily changes in Snow Water Equivalent (SWE) and precipitation. This script incorporates temperature conditions to determine when to add changes to liquid input. Q2: In plain language, explain each step of the 5 if-else statements above (~5 brief sentences) (3pts). ANSWER: Let’s visualize the timing disparities between precipitation and melt input in relation to discharge. Q3. What processes could account for the differences between cumulative input, calculated from SWE data, and cumulative precipitation, both measured at the same SNOTEL station? (1 or 2 processes) What could explain the discrepancy if the cumulative input, is found to be less than the cumulative precipitation measured at the same SNOTEL station? (at least 2 possible processes) (4 points) (2-3 sentences) ANSWER: 5.2.2.4 Modeling SWE Let’s assume that we have a precipitation gage at 3400m in Fool Creek, but without SWE data, how could we model SWE using temperature? The model below is a variation of the degree-day method. Here we are running a similar script to the input calculations above, using temperature as the primary determinant. When the temperature falls below a certain threshold and precipitation occurs, an equivalent amount is added to our SWE accumulation. We’ll conduct a simulation with 100 Monte Carlo runs and analyze the optimal outcome. The parameters we are testing are pack threshold, a degree-day factor, and the threshold temperature for determining rain from snow. This simple model seems to simulate SWE well if assessed with the NSE. Let’s model SWE at the Fool Creek outlet, where we do not have daily SWE data. First we’ll bring in precipitation data, measured at a USFS maintained meteorological station located near the Fool outlet: We also need temperature data. This is also collected at the USFS Lower Fool meteorological station. Even though this meteorological station is very close to the SNOTEL station, lets compare the values between the upper watershed and the lower Now let’s estimate SWE for the Lower Fool Creek. Again, we’ll just simulate a single year. Run another set of simulations and compare the values of the best performing parameters across sites. Since we don’t have SWE measurement for Lower Fool Creek, let’s see how the simulated values compare to acutal Upper Fool SNOTEL SWE measurements. Q4: Based on the differences in precipitation and temperature totals between the two sites, are SWE simulation results as you expected? (1 sentence)(2 pts) ANSWER: Q5: How does the estimated threshold temperature distinguishing rain from snow change with elevation? Is the relationship between your parameter estimations as expected? How does this influence your confidence in the model? (2 pts) ANSWER Q6: Compare the precipitation and snow accumulation patterns between two sites. Discuss at least 3 potential natural and/or anthropogenic mechanisms driving the observed variations in precipitation and snow accumulation. Consider the factors you observed in question 1 (3 points)(2-3 sentences). ANSWER: Q7: Let’s say you want to model runoff for the Fool Creek catchment identified above. To do that, you want to estimate the timing and volume of snowmelt for this watershed. Refer to at least 2 model types reviewed in the 06_model_classifications lecture or terms in the lecture above and qualitatively describe a model that you could use to capture the spatial heterogeneity of snow accumulation in this watershed (4 points) (2-4 sentences). ANSWER: "],["spatial-models-and-gis-integration.html", "Chapter 6 Spatial Models and GIS Integration 6.1 Gridded and tabular data retrieval 6.2 DEM processing", " Chapter 6 Spatial Models and GIS Integration 6.1 Gridded and tabular data retrieval 15 pnts 6.1.1 Learning Module 9 6.1.1.1 Objectives In this module you will: -Retrieve flow data using the dataretrieval package -Learn to retrieve and manage digital elevation models -Retrieve and import remotely sensed data for the watershed extent 6.1.1.2 Background This final section moves toward spatially explicit modeling, integrating everything we have learned so far with geospatial data. A refresher in GIS concepts: This module assumes familiarity with basic Geographic Information System (GIS) such as coordinate systems (latitude and lognitude, UTM). Some terms to recall: Gridded data refers to spatial data in raster format. If you have performed any GIS work, you are likely using gridded data. Rasters organize geographical areas into a grid where each regularly spaced cell represents a specific location and contains some attribute representing that location. For example, digital elevation models (DEM) are a form of gridded data. Imagine that we are working in a watershed and we can drop a grid or square celled net over the entire catchment. We can then model the elevation data of that catchment by assigning each cell in the net a mean elevation value for the area represented by the cell. Similarly, we can assign any value we wish to model to the cells. If we were to drop another net and assign a maximum tree height to every cell, we would be adding another ‘layer’ to our model. The spatial resolution of gridded data refers to the length and width of each cell represents. If we want to observe gridded data over time, we can generate a raster stack, where each raster layer represents a point in time. With a raster stack, we can increase the temporal resolution of our gridded data. Vector data describes spatial features using points, lines, and polygons. These spatial features are represented a geometrical objects with associated attributes, like lines to represent linear features such as rivers, or points to represent single locations. We will work with a .shp file in this module that is a collection of points representing the boundary of a watershed. Tabular data refers to data organized in a row-column format. In this class, each of our .csv datasets are a form of tabular data. 6.1.1.3 A refresher in remote sensing: Remote sensing is the science of obtaining information about objects or areas from a distance, typically from aircraft or satellites. It involves the collection and interpretation of data gathered by sensors that detect electromagnetic radiation reflected or emitted from the Earth’s surface and atmosphere. As ecologists, many of us utilize remote sensing to observe and analyze the Earth’s surface and its dynamics, providing valuable insights into environmental processes, land use and land cover changes, natural disasters, and more. Some helpful terms to recall: spatial resolution: Remotely sensed data can vary in its spatial resolution, which refers to the scale of the smallest unit, or level of visual detail. temporal resolution: When used to describe remotely sensed data, we are often talking about how frequently a remote sensing system revisits the same area on the Earth surface. Systems with high temporal resolution revisit the same location frequently, allowing for monitoring rapid changes in phenomena, where a lower resolution may limit a system’s ability to capture short-term events. We often find that data collection requires a balance of spatial and temporal resolution to select the most appropriate data for the coverage and detail that ecological work requires. 6.1.1.4 Ethical conduct When utilizing publicly available datasets, it is important for us as users to: -Be aware of any copyright or licensing agreements. There may be restrictions or usage terms imposed by data providers. Read provided agreements or terms to be sure we are in compliance with these when using datasets in our research. This may also include requirements for registration or subscription established by providers or agencies like USGS or NASA. -Cite and acknowledge the contributions of data providers and researchers whose datasets we utilize. This also facilitates transparency and reproducibility in our research. - Assess the quality and accuracy of geospatial data before using it for decision making purposes. It is important that we consider spatial resolution, temporal resolution and metadata documentation to critically evaluate the reliability of the datasets we use. -Promote Open Data Principles: Try to explore and use publicly available datasets that support transparency and collaboration within our scientific and academic communities. 6.1.2 Repo link The repo for this module can be found here 6.1.3 Labwork (20 pnts) In this module we will import streamflow (tablular) data, using USGS’s DataRetrieval package, then visualize vector data to delineate our area of interest and learn how to find spatial gridded data for the area. 6.1.3.1 Setup: When importing packages and setting parameters, think about how the local working environment can vary among users and computers. A function that examines and installs packages before importing them enhances the script’s ability to be easily used by others. 6.1.3.2 Selecting a watershed: Here, we are using USGS’ dataRetrieval package to search and filter hydrologic datasets. There are many ways data can be searched; more can be found in this dataRetrieval vingette. In this example, we will fetch USGS site numbers for Colorado watersheds with streamflow (discharge) data for a specified time period. Q1 (2 pts). If you were creating this script for your work team’s shared collaboration, or for yourself to use again a year from now, what might be the disadvantages of embedding search parameters directly into the function parameters? ANSWER: Q2 (3 pts). What is a potential use for whatNWISsites? ANSWER: EXTRA (1 pt): Why might the above script return an error if dpylr:: is not placed before select()? ANSWER: drain_area_va is a column value returned by readNWISsite. Other variables can be found here Q3 (1 pt). In a sentence, what does this dataframe (all_site_data) tell you? ANSWER: Note that while your research interests may not require streamflow data, it can serve as a valuable resource for a wide array of data types. For the remainder of this assignment, we’ll focus on a single watershed in central Colorado near the Experimental Forest that we explored in the Snowmelt module. There are several dataRetrieval functions could we utilize to see what data is available for a particular site number. Check dataRetrieval docs to learn package functions or tools. Q4 (4 pts). Find the longest running dataset in DataAvailable (i.e. either by count or by start and end date). Use https://help.waterdata.usgs.gov to look up the meaning of some of column headers and the codes associated with them. a. What kind of data does this long running dataset hold? b. What column did you use to learn that? c. What does ‘dv’ mean in data_type_cd? ANSWER: 6.1.3.3 Flow data retrieval: Let’s rename our columns. For this you will need to determine the units of the date Q5 (2 pts). Based on the hydrograph plot, what type of precipitation events appear to influence the observed intra-annual variations in flow rates? ANSWER: 6.1.3.4 Watershed boundary vector data: A boundary shapefile has been provided to save time and improve repeatability. Watershed boundaries for gauging stations can be found at USGS StreamStats by zooming into the water map and clicking ‘Delineate’ near your desired gage or point. Once your watershed is delineated, you can click on ‘Download Basin’ and select from a choice of file types. This can be a great to to delineate river basins if all you require is a shapefile. For smaller streams, other analyses are available. We will cover those in the next module. Let’s import a pre-created shapefile: Let’s transform this to a projected CRS (flat surface) rather than a geographic (spherical surface) CRS. A projected CRS will depict the data on a flat surface which is required for distance or area measurements. A more detailed explanation can be found at the University of Colorado’s Earth Lab if needed. 6.1.3.5 Visualization - watershed boundary i.e., sanity check 6.1.3.6 Digital Elevation Models (DEM) A DEM has been provided for consistency among users. However, if you are unfamiliar with the process of retrieving a DEM here are three methods that our team commonly uses: USDA’s Natural Resources Conservation Science (NRCS) website. Here you can order (free) DEMs by state, county or bounding box. It takes just minutes for them to be emailed to you. If you have a USGS account (fast and free) you can also find DEM data at https://earthexplorer.usgs.gov. See this video if interested. This is the method we used to provide this DEM and we will use this resource again to retrieve Landsat data. GIS or hydrological software like QGIS and SAGA offer tools for downloading DEMs directly into your project for analysis. This can be optimal when working over large geographic extents to streamline the data acquisition process. Both QGIS and SAGA are free and easy to learn if you have experience with ESRI products. Let’s check out our DEM: If the watershed and raster don’t overlap as you expect, be sure the projection and extent is the same for all data files plotted. 6.1.3.7 Other remotely sensed data: Now that we have site data spatial data, let’s cover a couple of techniques or resources you can use to access satellite data for your site. In the event that you only need a vegetative map layer from a specific point in time, it is not too difficult to download actual Landsat imagery from usgs.gov. In the case of Cabin Creek let’s look at a single summer image from 2020. Navigate to https://ers.cr.usgs.gov/, and if you don’t already have an account, you can create one quickly or sign in if you already have one. Once signed in, go to https://earthexplorer.usgs.gov you should see a surface map with tabs on the left. Start with the ‘Search Criteria’ tab. For this assignment the shape file we have is too large to upload, so it is easiest to scroll down and choose ‘polygon’ or ‘circle’ to define your area. Under the circle tab you can use Center Latitude 39.9861 Center Longitude -105.719 and a radius of about 3.5 kilometers to capture our study area. Let’s say we want an NDVI map layer from summer of 2020. In ‘date range’, select dates for the summer of 2020. In this high elevation area of Colorado, the greenest times of the year without snow cover are likely June and July with senescence beginning sometime in late July. First search from June 1 to July 15 2020. Then click on ‘Data Sets’ either at the bottom or top of the left-hand side menus. From the data sets we will select Landsat Collection 2 level 2, Landsat 8 - 9 OLI TIRS C2L2. We can skip ‘Additional Criteria’ for now and go to ‘Results’. You should receive a set of about 11 images. For each image you can check out the metadata. That is the ‘pen and notepad’ icon in the middle of image specific menu. Here you can look at data like cloud cover. For these images, the values you see are a cloud coverage percent. Cloud coverage measurements and the codes to filter for certain cloud cover thresholds may change across missions. If you are retrieving Landsat data for time series using code, it will be important to explore the different quality control parameters and what they represent. You don’t need to download anything for this assignment, we’ve provided it for you. However to gain familiarity with this process, click on the ‘download’ icon for July 7, 2020 and click ‘select files’ on the Reflectance Bands option. Q6 (1 pt): What 2 bands should we download to calculate NDVI? Answer: Q7 (3 pt): Find 2 other indices that can be used as indicators of vegetative health. What might each of these indices indicate. What are the bands used for each index? Check out other earthexplorer resources while you are at the site. You will likely find resources here for your MS projects, even if it is just for making maps of your study area for presentations or paper. 6.1.3.7.1 Landsat If you want to learn about Landsat missions or utilize Landsat data, USGS offers all of the reference information you need. The sensors on these missions can be a fantastic resource if characterizing land or water surface over large temporal scales. Landsat data provides (30m) spatial resolution so can capture spatial heterogeneity, even for smaller study areas. If calculating vegetation indices over different time periods, it is important to know which mission you are retrieving data from and check the band assignments for that mission. For example, on the Landsat 6 instrument, Band 4 is near-infrared, whereas Band 4 on Landsat 9’s instrument is red. 6.1.3.7.2 Moderate Resolution Imaging Spectroradiometer (MODIS): is a sensor aboard NASA’s Terra and Aqua satellites which orbit Earth and collect data on a daily basis. MODIS data provides moderate spatial resolution (250m to 1km) and high temporal resolution. Therefore, MODIS data can be an optimal way to monitor or characterize rapid changes to Earth’s surface like land cover changes resulting from wildfire. Data can be accessed and downloaded directly through NASA’s Earthdata tool. Let’s import our raster data. Q8 (2 pt) What does ‘SR’ in the .tif names tell us about the data? ANSWER: Q9 (2 pts) What is the difference between ‘mask’ and ‘crop’ when filtering large files to fit the area of interest? ANSWER: 6.1.3.8 Earth surface changes over time If considering changes over time, comparing individual rasters from different time periods directly may not always provide meaningful insights because the absolute values of vegetation indices can vary due to factors such as differences in solar angle, atmospheric conditions, and land cover changes. Instead, it’s often more informative to analyze trends in vegetation over time. 6.1.3.8.1 LandTrendr One fun way to do this is with Oregon State University’s LandTendr. With a Google account and access to Earth Engine, you can easily interact with the UI Applications in section 8. You can jump to the pixel timeseries here. This will allow you to view changes in vegetation indices over a specified time period within a single Landsat pixel. 6.1.3.8.2 Google Earth Engine Another effective way to access evaluate large spatial datasets is through Google Earth Engine (GEE). GEE is a cloud-based platform developed by Google for large scale environmental data analysis. It provides a massive archive of satellite imagery and other geospatial datasets, as well as computational tools to view and analyze them. GEE uses Google’s cloud to provide processing of data so you don’t need powerful computing ability to run a potentially computationally expensive spatial analysis. 6.1.3.8.2.1 GEE Access If spatial data is something you may need for your MS project, we recommend exploring this resource Sign Up: Go to the Google Earth Engine website (https://earthengine.google.com/) and sign in with your Google account. If you don’t have a Google account, you can sign up for access by filling out a request form. Request Access: If you’re signing up for the first time, you’ll need to request access to Google Earth Engine. Provide information about your affiliation, research interests, and how you plan to use Earth Engine (learn and access data!). Approval: After submitting your request, you’ll need to wait for approval from the Google Earth Engine team. Approval times may vary, but you should receive an email notification once your request has been approved. Access Earth Engine: Once approved, you can access Google Earth Engine through the Code Editor interface in your web browser at code.earthengine.google.com to start exploring the available datasets, running analyses, and visualizing your results. We have provided a basic script for retrieving and viewing NDVI data. This is a code snapshot, so you can manually copy the code from the snapshot and paste it into you own Google Earth Engine Code editor. You are welcome to save this to your own GEE files and try changing the area of interest, vegetation indices, time periods or exploring other data. Colorado State University also has a great tutorial here that will start you from scratch. Note that JavaScript is the default language for the code editor interface in your web browser. If you are familiar with Python, GEE provides a Python API that can allow you to use Python syntax. If neither of these languages are familiar, don’t give up yet! You can still use this tool with your R coding background and trial and error. We recommend finding other scripts that suit your need (there are a lot of shared resources online as well as in our data collection assignment) and adapting them to your area of interest. Once you understand coding concepts, like those in R, other languages can be picked up easily. You will find that you are learning JavaScript before you realize it. 6.2 DEM processing 6.2.1 Learning Module 10 6.2.1.1 Objective: Users will explore basic hydrological tools through the process of DEM preprocessing and defining stream networks for a watershed in the Fraser Experimental Forest in Colorado using Whitebox Tools for R. This exercise will also demonstrate methods for writing functions and efficiently handling multiple files simultaneously, including importing, processing, and exporting data within the context of Whitebox Tools for R. Hydrological analysis preprocessing involves the use of digital elevation model (DEM) raster data to establish a watershed model and a simulation of surface hydrological processes. These steps enable us to quantify key parameters such as flow accumulation, stream network characteristics, and hydrological connectivity, which are essential for studying the dynamics of water movement within a landscape. Overall, preprocessing is the set of foundational steps in hydrological modeling and analysis. Whitebox Tools is an advanced geospatial data analysis platform that can be used to perform common geographical information systems (GIS) analysis operations. This platform was developed with the Center for Hydrogeomatics in Guelph University so it is focused on hydrological analysis. With just a DEM, it allows us to produce a multitude of outputs that can be used for future analysis (Lindsay, 2016) doi:10.1016/j.cageo.2016.07.003). While we are demonstrating its use in R, these tools are also available in QGIS and Python platforms. 6.2.2 THE LINK TO THE REPO IS HERE 6.2.3 Labwork (20 pts) 6.2.3.1 Installing libraries We are going to try installing the whitebox R package from CRAN as it should be the simplest method. However, if this does not work for you, you can install the development version from GitHub by putting this inside a code chunk: if (!require(“remotes”)) install.packages(‘remotes’) remotes::install_github(“opengeos/whiteboxR”, build = FALSE) More information on installation can be found at: https://cran.r-project.org/web/packages/whitebox/readme/README.html Helpful whitebox documentation can be found at https://jblindsay.github.io/wbt_book/preface.html. Essentially, we will be using input rasters via filepath and output filepaths as arguments for various whitebox functions. The script is designed to perform functions on all rasters in a given folder at once. When writing scripts, developers typically follow a standard workflow: 1. Import required libraries 2. Generate functions useful throughout the script 3. Establish working directories or paths to other directories if needed 4. Import data 5. Data Cleaning and Preprocessing - this may involve handling missing values, removing outliers, converting to preferred units. etc. 6. Exploratory Data Analysis - it is beneficial to explore data visually to help understand the characteristics of the data. 7. Apply functions, or models or other analytical techniques 8. Evaluate results - If modeling, this may involve comparing model predictions with observed data or conducting sensitivity analysis 9. Visualize and report - plots, maps and tables can be effective ways to communicate findings. While you might find slight variations among collaborators, following this general workflow ensures that our scripts are structured in a way that facilitates easy sharing and reproducibility of results. 6.2.3.1.1 Generate functions Since we have imported the required libraries, let’s generate some functions. Q1.(2 pnt) What does date: “26 February, 2025” in the header do? ANSWER: Q2.(3 pnt) What does ‘recursive = TRUE’ do? What would the ‘recursive = FALSE’ return? ANSWER: EXTRA (1 pnt) : Rewrite this function to generate ‘splitbase’ with fewer lines. For example, what happens when you replace ‘basename’ in ‘splitbase &lt;- strsplit(basename,’_‘)[[1]][1]’ with ‘splitpath[[1]][2]’? Q3. (3 pnt) What is the point of writing a function? Why do you think it is advantageous to write functions early in your script? ANSWER: 6.2.3.1.2 Establish new directories(folders) in our working directory. We will use these to store the outputs of the Whitebox functions. Check your working directory, you should see the new folders there. 6.2.3.2 Resample DEMs Here we will start with LiDAR data with 0.5m resolution. While this resolution has useful applications, a high resolution DEM can make hydrological models very computationally expensive with little or no improvement to the output. If you have the option of high resolution data in your work, you can test model outputs at different resolutions to determine what is the most efficient and effective resolution for your work. Here, we will resample our LiDAR data to a 10m resolution. Q4. (3 pnt) Did we use the function extractsitenames in the above chunk? How? What did it do? ANSWER: Let’s quickly check our work by importing a resampled DEM and checking the resolution. Note: This can also be done without importing the raster to the workspace by installing the library gdalUtils. Q5.(2 pnt) What is the resolution of the resampled DEM? Where and how could we change the resolution to 30m if desired? ANSWER: 6.2.3.3 Filling and breaching When performing hydrological analysis on a DEM, the DEM usually needs to be pre-processed by ‘filling’ or ‘breaching’ any depressions or sinks to create a hydraulically connected and filled DEM. There are several depression or pit filling options available in whitebox. Breach depressions can be a better option that just pit filling according to whitebox documentation, however, some users argue that this can smooth too much, resulting in an altered watershed delineation. It is prudent to investigate different DEM pre-processing methods and their resulting DEM. You can fill depressions directly, breach depressions and then fill them, applying breach or fill single cell pit before breach/fill depressions, and use the one that generates the most reasonable watershed delineation results. Here we are going to make extra sure our flowpaths are uninhibited by first filling in single cell pits, and then breaching any larger depressions. Q6 (2 pnt) What is breach1? How is lapply using breach1? ANSWER: 6.2.3.4 Flow direction and accumulation rasters Q7 (2 pnt) Check out the WhiteboxTools User Manual. What does a d8pointer raster tell us and what might we use it for? ANSWER: Let’s visualize some of our work so far: Q8. (3 pnt) What are the units for the values that you see in each of these legends? It may be helpful to check out the Manual again. ANSWER: 6.2.3.5 Streams Define streams using the flow accumulation raster. Use of the wbt_extract_streams function will return a raster with stream cells indicated only. Sometimes we would prefer to see the stream within the watershed boundary. Here we are using the extent of the flow accumulation raster to generate a raster with ‘0’ to indicate watershed cells, along with stream cells indicated by the streams.tif. This is a demonstration for one watershed. 6.2.3.6 Final thoughts: There are many more hydrological preprocessing and analysis tools available through Whitebox for R. If you are interested in watershed delineation in R, there is a tutorial here that is fairly easy to follow. However, if you find that you use these tools frequently and do not use R much in other work, you may also consider these options for hydrological analysis: 1. SAGA SAGA tools offer a versatile suite of geospatial processing capabilities accessible through both QGIS and ArcGIS plugins as well their standalone GUI interface. Often I find the GUI easiest for preprocessing, then I will import SAGA’s output rasters to QGIS for formatting or map making, or into model scripts. SAGA has a robust online support community, so it can be a valuable resource for hydrological work. 2. Similarly, Whitebox GAT tools can be used as plugin to QGIS and ArcGIS, providing Whitebox functionality directly with in a GIS environment. When using these tools, the order of operations is similar to our work above: fill the DEM, generate a flow direction and flow accumulation raster, identify channels, delineate watersheds, then you can move forward according to the specificity of your project. Ultimately, the choice of workflow is yours, but I suggest documenting your process as you proceed, including links or file paths to projects and scripts within a written or diagrammed workflow (or workflow software). It’s also important to carefully consider the organization and storage of your projects and files. For instance, files generated by a GIS project should be readily accessible to any associated scripts. Returning to a preprocessing step can be (sometimes painfully) challenging if there’s no clear way to trace back your workflow and regenerate a crucial layer. Take another look at the model selection process from Bevin (above), and think about how you can make your workflow easier to recall and repeat. A little planning upfront can save so much time later. (Seriously, so. much. time.) To document and track iterations efficiently, consider using a tool that fits your workflow: - Bookdown in R - this is useful if your work involves R scripts or Markdown. This is the tool we used to develop the tutorial ‘book’ for this course and I frequently use it to develop and share my own research scripts, figures and workflows. - Obsidian or Notion - Better options if R scripts aren’t a major part of your work, allowing for flexible organization and easy cross-referencing. - You can even make a slide in Power Point with a work flow chart, inserting links, file paths and written steps into the chart as you go. As you work on your term projects over the next couple of weeks, keep these tips in mind. If you haven’t started the thesis course yet (LRES 575), you’ll likely want to revisit the workflow you develop now and continue refining it for your thesis. Set yourself up for success by making it easy to track, recall and build on your work. Think about how you can make this process smoother for your future self. Your future self will thank you. Good Luck! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
